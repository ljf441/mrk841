{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install astropy\n",
    "!pip install tabulate\n",
    "!pip install emcee\n",
    "!pip install corner\n",
    "!pip install lmfit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------- Import packages --------------\n",
    "import joblib\n",
    "joblib.Parallel(n_jobs=12)\n",
    "\n",
    "# Enable inline plotting in notebook\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from numba import njit\n",
    "import astropy\n",
    "\n",
    "from matplotlib.colors import TwoSlopeNorm\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import arx\n",
    "import time\n",
    "import math\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from functools import partial\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "import sys\n",
    "import argparse\n",
    "import scipy\n",
    "import PYCCF as myccf\n",
    "from scipy import stats \n",
    "np.set_printoptions(threshold=sys.maxsize)\n",
    "from matplotlib import gridspec\n",
    "from scipy import interpolate\n",
    "from astropy import units as u\n",
    "import matplotlib.ticker as mtick\n",
    "import matplotlib.ticker as ticker\n",
    "import emcee\n",
    "from IPython.display import display, Math\n",
    "import scipy as sp\n",
    "from matplotlib.legend_handler import HandlerTuple\n",
    "from datetime import datetime\n",
    "from astropy.io import fits\n",
    "from astropy.constants import b_wien\n",
    "import os\n",
    "import seaborn as sns\n",
    "\n",
    "# from PyROA, available at https://github.com/Alymantara/PyROA\n",
    "# Reference: Donnan et al. 2021 (https://ui.adsabs.harvard.edu/abs/2021arXiv210712318D/abstract)\n",
    "import PyROA\n",
    "import Utils\n",
    "\n",
    "import xspec\n",
    "\n",
    "# from RELAGN, available at https://github.com/scotthgn/RELAGN\n",
    "# Reference: Hagen & Done (2023b) (https://ui.adsabs.harvard.edu/abs/2023arXiv230401253H/abstract)\n",
    "from relagn import relagn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rc('xtick', labelsize=15) \n",
    "plt.rc('ytick', labelsize=15)\n",
    "plt.rc('axes', titlesize=20)     # fontsize of the axes title\n",
    "plt.rc('axes', labelsize=20)\n",
    "plt.rc('legend', fontsize=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intercalibration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function library\n",
    "Reference: Vielute R. (in. prep.) Functions are included in their entirety to reflect how they have been used throughout the project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ===================================== Function Library ============================================\n",
    "def Clean(Star_File):\n",
    "    \"\"\" Drop duplicate values, existing zp and zp_err columns, and strange errors (= 99.0).\n",
    "    \n",
    "    Input:\n",
    "    Star_File - Original data file (pd dataframe).\n",
    "    \n",
    "    Output:\n",
    "    File2 - Cleaned up data file (pd dataframe).\n",
    "    \"\"\"\n",
    "    \n",
    "    File2 = Star_File\n",
    "    File2 = File2.drop_duplicates(subset=['id_apass', 'Filter', 'MJD', 'telid', 'airmass', 'seeing'], keep='first', inplace=False, ignore_index=False)\n",
    "    File2 = File2.drop(File2[(File2['err_aper'] > 20)].index)\n",
    "    return(File2)\n",
    "    \n",
    "def Locate_Star_ID(Star_File, Star_ID):\n",
    "    \"\"\" Locate star(s) in the file by ID(s).\n",
    "    \n",
    "    Input:\n",
    "    Star_File - Star data (pd dataframe).\n",
    "    Star_ID - Star ID(s) (float or list).\n",
    "    \n",
    "    Output:\n",
    "    Stars - pd dataframe containing selected stars by ID.\n",
    "    \"\"\"\n",
    "    if type(Star_ID) == list:\n",
    "        mask = Star_File['id_apass'].isin(Star_ID)\n",
    "        Stars = Star_File[mask]\n",
    "    else:\n",
    "        Stars = Star_File.loc[Star_File['id_apass'] == Star_ID]\n",
    "    return(Stars)\n",
    "\n",
    "def Locate_Star_Filter(Star_File, Fltr):\n",
    "    \"\"\" Locate star(s) in the file by filter(s).\n",
    "    \n",
    "    Input:\n",
    "    Star_File - Star data (pd dataframe).\n",
    "    Fltr - Filter(s) (string or list).\n",
    "    \n",
    "    Output:\n",
    "    Stars - pd dataframe containing selected stars by filter.\n",
    "    \"\"\"\n",
    "    if type(Fltr) == list:\n",
    "        mask = Star_File['Filter'].isin(Fltr)\n",
    "        Stars = Star_File[mask]\n",
    "    else:\n",
    "        Stars = Star_File.loc[Star_File['Filter'] == Fltr] \n",
    "    return(Stars)\n",
    "\n",
    "def Locate_Star_Scope(Star_File, Telescope):\n",
    "    \"\"\" Locate star(s) in the file by telescope(s).\n",
    "    \n",
    "    Input:\n",
    "    Star_File - Star data (pd dataframe).\n",
    "    Telescope - Telescope(s) (string or list).\n",
    "    \n",
    "    Output:\n",
    "    Stars - pd dataframe containing selected stars by telescope.\n",
    "    \"\"\"\n",
    "    if type(Telescope) == list:\n",
    "        mask = Star_File['telid'].isin(Telescope)\n",
    "        Stars = Star_File[mask]\n",
    "    else:\n",
    "        Stars = Star_File.loc[Star_File['telid'] == Telescope]\n",
    "    return(Stars)\n",
    "\n",
    "def Locate_Star_Epoch(Star_File, Epoch):\n",
    "    \"\"\" Locate star(s) in the file by Epoch(s).\n",
    "    \n",
    "    Input:\n",
    "    Star_File - Star data (pd dataframe).\n",
    "    Epoch - MJD(s) (string or list).\n",
    "    \n",
    "    Output:\n",
    "    Stars - pd dataframe containing selected stars by epoch.\n",
    "    \"\"\"\n",
    "    if type(Epoch) == list:\n",
    "        mask = Star_File['MJD'].isin(Epoch)\n",
    "        Stars = Star_File[mask]\n",
    "    else:\n",
    "        Stars = Star_File.loc[Star_File['MJD'] == Epoch]\n",
    "    return(Stars)\n",
    "\n",
    "def Generic_Optimal(Data, Errors):\n",
    "    \"\"\" Compute inverse variance weighted average.\n",
    "    \n",
    "    Input:\n",
    "    Data - Data to average (list/array).\n",
    "    Errors - Errors of data to average (list/array).\n",
    "    \n",
    "    Output:\n",
    "    avg - Optimal average (float).\n",
    "    err - standard deviation of average (float).\n",
    "    \"\"\"\n",
    "    \n",
    "    #errors handled by returning nan values\n",
    "    #ensure data are in arrays\n",
    "    var = np.array(Errors)**2\n",
    "    Data = np.array(Data)\n",
    "    \n",
    "    if len(var) != 0:\n",
    "        try:\n",
    "            w = 1/var\n",
    "            avg = np.nansum(w*Data) / np.nansum(w)\n",
    "            var = 1/np.nansum(w)\n",
    "        except:\n",
    "            avg = np.nan\n",
    "            var = np.nan\n",
    "    else:\n",
    "        avg = np.nan\n",
    "        var = np.nan\n",
    "    \n",
    "    if var**0.5 == np.inf:\n",
    "        var == np.nan\n",
    "        avg == np.nan\n",
    "    \n",
    "    err = var**0.5\n",
    "    return(avg, err)\n",
    "\n",
    "def Brightest(Star_File, Filter, AGN_ID = AGN_ID):\n",
    "    \"\"\" IDs of the brghtest stars in the data for a specific filter, identified using optimal average. \n",
    "    \n",
    "    Input:\n",
    "    Star_File - Star data (pd dataframe).\n",
    "    Filter - Filter (string).\n",
    "    AGN_ID - ID of AGN (int).\n",
    "    \n",
    "    Output:\n",
    "    Brightest_IDs - IDs of stars sorted from brightest to dimmest (list).\n",
    "    \"\"\"\n",
    "    \n",
    "    #Select specified filter data\n",
    "    Star_Data0 = Locate_Star_Filter(Star_File, Filter)\n",
    "    \n",
    "    #Make sure AGN isn't included\n",
    "    Star_IDs = [k for k in pd.unique(Star_Data0['id_apass']) if k!= AGN_ID]\n",
    "\n",
    "    #Compute optimal average of instrumental magnitudes\n",
    "    MAGS = []\n",
    "    IDS = []\n",
    "    for ID in Star_IDs:\n",
    "        Star_Data = Locate_Star_ID(Star_Data0, ID)\n",
    "        mag = Star_Data['mag_aper'].values\n",
    "        mag_err = Star_Data['err_aper'].values\n",
    "        mag_mean = Generic_Optimal(mag, mag_err)[0]\n",
    "        MAGS.append(mag_mean)\n",
    "        IDS.append(ID)\n",
    "    \n",
    "    #Sort from brightest to dimmest and return IDs\n",
    "    MAGS_ref = MAGS\n",
    "    MAGS = sorted(MAGS)\n",
    "    Brightest_indices = [MAGS_ref.index(i) for i in MAGS]\n",
    "    Brightest_IDs = np.array(IDS)[np.array(Brightest_indices)]\n",
    "    return Brightest_IDs\n",
    "\n",
    "def root_mean_squared_deviation(data, mean):\n",
    "    \"\"\"Compute rms of dataset.\n",
    "    Input:\n",
    "    data - dataset to compute rms for (array/list).\n",
    "    mean - mean of dataset (float).\n",
    "    \n",
    "    Output:\n",
    "    rms - root mean squared deviation (float).\n",
    "    \"\"\"\n",
    "    \n",
    "    rms = (sum((data-mean)**2) / len(data))**0.5\n",
    "    return(rms)\n",
    "\n",
    "def Brightest_Reduced(Star_File, Filter, AGN_ID = AGN_ID, frac = 0.5):\n",
    "    \"\"\" IDs of the brightest stars in the data for a specific filter, identified using optimal average. \n",
    "        Only include stars that have more than specified number of data points per star.\n",
    "    \n",
    "    Input:\n",
    "    Star_File - Star data (pd dataframe).\n",
    "    Filter - Filter (string).\n",
    "    AGN_ID - ID of AGN (int).\n",
    "    frac - (Optional) Fraction of max number of epochs to keep (float).\n",
    "    \n",
    "    Output:\n",
    "    Brightest_IDs - IDs of stars sorted from brightest to dimmest (list).\n",
    "    \"\"\"\n",
    "    \n",
    "    #Select specified filter data\n",
    "    Star_Data0 = Locate_Star_Filter(Star_File, Filter)\n",
    "    \n",
    "    #Make sure AGN isn't included\n",
    "    Star_IDs = [k for k in pd.unique(Star_Data0['id_apass']) if k!= AGN_ID]\n",
    "    \n",
    "    Max_length = 0.0  \n",
    "    #Get max number of epochs amongst all stars\n",
    "    for ID in Star_IDs:\n",
    "        Star_Data = Locate_Star_ID(Star_Data0, ID)\n",
    "        if len(Star_Data['MJD'].values) > Max_length:\n",
    "            Max_length = len(Star_Data['MJD'].values)\n",
    "    \n",
    "    #Select all stars with more than specified number of datapoints\n",
    "    keep_dp = frac * Max_length\n",
    "    Reduced_IDs = []\n",
    "    for ID in Star_IDs:     \n",
    "        Star_Data = Locate_Star_ID(Star_Data0, ID)\n",
    "        if len(Star_Data['MJD'].values) > keep_dp:\n",
    "            Reduced_IDs.append(ID)\n",
    "    \n",
    "    #Compute optimal average of instrumental magnitudes\n",
    "    MAGS = []\n",
    "    IDS = []\n",
    "    for ID in Reduced_IDs:\n",
    "        Star_Data = Locate_Star_ID(Star_Data0, ID)\n",
    "        mag = Star_Data['mag_aper'].values\n",
    "        mag_err = Star_Data['err_aper'].values\n",
    "        mag_mean = Generic_Optimal(mag, mag_err)[0]\n",
    "        MAGS.append(mag_mean)\n",
    "        IDS.append(ID)\n",
    "    \n",
    "    #Sort from brightest to dimmest and return IDs\n",
    "    MAGS_ref = MAGS\n",
    "    MAGS = sorted(MAGS)\n",
    "    Brightest_indices = [MAGS_ref.index(i) for i in MAGS]\n",
    "    Brightest_IDs = list(np.array(IDS)[np.array(Brightest_indices)])\n",
    "    return Brightest_IDs\n",
    "\n",
    "def Epoch_Dist(Star_File, Filter, AGN_ID = AGN_ID, frac = 0.0):\n",
    "    \"\"\" Display distribution of stars by number of epochs per star.\n",
    "        Include how many stars are available with at least a certain fraction of the max number of epochs.\n",
    "        Also show the instrumental magnitude of stars as a function of the number of epochs.\n",
    "        \n",
    "    Input:\n",
    "    Star_File - Star data (pd dataframe).\n",
    "    Filter - Filter (string).\n",
    "    frac - (optional) print number of stars available with at least this fraction of the max number of epochs.\n",
    "    \"\"\"\n",
    "    \n",
    "    #Select specified filter data\n",
    "    Star_Data0 = Locate_Star_Filter(Star_File, Filter)\n",
    "    \n",
    "    #Make sure AGN isn't included\n",
    "    Star_IDs = [k for k in pd.unique(Star_Data0['id_apass']) if k!= AGN_ID]\n",
    "    #Count number of epochs for each star and mean instrumental mags\n",
    "    Lengths = []\n",
    "    Mags = []\n",
    "    for ID in Star_IDs:\n",
    "        Star_Data = Locate_Star_ID(Star_Data0, ID)\n",
    "        Lengths.append(len(Star_Data['MJD'].values))\n",
    "        Mags.append(Generic_Optimal(Star_Data.mag_aper.values, Star_Data.err_aper.values)[0])\n",
    "    Lengths = np.array(Lengths)\n",
    "    \n",
    "    # --------------- Plot data ----------------\n",
    "    fig, (ax, axb) = plt.subplots(1,2, gridspec_kw={'width_ratios': [3, 2]}, figsize = (16, 6))\n",
    "    fig.subplots_adjust(wspace = 0.25)\n",
    "    nbins = 50\n",
    "    hist0 = ax.hist(Lengths, bins = nbins, weights=np.ones(len(Lengths)) / len(Lengths))\n",
    "    ax.cla()\n",
    "    hist = ax.hist(Lengths, bins = nbins, color = 'gray', alpha = 0.7, zorder = 10, edgecolor = None, weights=np.ones(len(Lengths)) / max(hist0[0]) / len(Lengths))\n",
    "\n",
    "    #CDF\n",
    "    X0 = sorted(list(np.arange(min(Lengths), max(Lengths) + (max(Lengths) - min(Lengths)) / nbins, (max(Lengths) - min(Lengths)) / nbins))*2)[:-1]\n",
    "    N = [0.]\n",
    "    for i in hist0[0]:\n",
    "        N.append(i+N[-1])\n",
    "    N = sorted(N*2)[1:]\n",
    "    X = sorted(X0)\n",
    "    ax.plot(X, N, color = 'black', zorder = 20, lw = 1.0)\n",
    "    \n",
    "    #Number of Stars Available\n",
    "    labs = []\n",
    "    X2 = []\n",
    "    N2 = []\n",
    "    for j in np.arange(1.0, -0.1, -0.01):\n",
    "        j = round(j, 3)\n",
    "        X2.append(j*max(Lengths))\n",
    "        N2.append(len([k for k in Lengths if k > j*max(Lengths)]) / len(Lengths))\n",
    "        labs.append(str(len([k for k in Lengths if k > j*max(Lengths)])))\n",
    "    N2 = sorted(N2*2)[1:]\n",
    "    X2 = sorted(X2*2)[1:] ; X2 = sorted(X2, reverse = True)\n",
    "    ax.plot(X2, N2, color = 'Teal', zorder = 20, lw = 1.0)\n",
    "    \n",
    "    # ------- Formatting -------\n",
    "    ax.set_title(str(Filter) + ' Filter')    \n",
    "    ax.set_xlabel('Number of Epochs')\n",
    "    ax.set_ylabel('# of Stars Available with at least M datapoints \\n M = Frac x max # of Epochs', color = 'Teal')\n",
    "    ax.set_xlim(0, max(Lengths))\n",
    "    ax.set_xticks(np.arange(0, max(Lengths), 100))\n",
    "    ax.set_ylim(0, 1.01)\n",
    "    ax.set_yticks(np.arange(0.0, 1.1, 0.1))\n",
    "    ax.set_yticklabels([round(i) for i in np.arange(0, len(Lengths)+len(Lengths)/11, 0.1*len(Lengths))], color = 'Teal')\n",
    "    ax.tick_params(axis = 'both', which = 'major', direction = 'in', length = 6)\n",
    "    ax.tick_params(axis = 'both', which = 'minor', direction = 'in', length = 4)\n",
    "    ax.minorticks_on()\n",
    "    \n",
    "    ax2 = ax.twinx()\n",
    "    ax2.plot(X, N, alpha = 0)\n",
    "    ax2.set_ylim(0, 1.01)\n",
    "    ax2.set_yticks(np.arange(0, 1.1, 0.1))\n",
    "    ax2.set_yticklabels([round(j, 2) for j in np.arange(0, 1.1, 0.1)], color = 'black')\n",
    "    ax2.set_ylabel('CDF', color = 'black')\n",
    "    ax2.tick_params(axis = 'both', which = 'major', direction = 'in', length = 6)\n",
    "    ax2.tick_params(axis = 'both', which = 'minor', direction = 'in', length = 4)\n",
    "    ax2.minorticks_on()\n",
    "    \n",
    "    ax3 = ax.twiny()\n",
    "    ax3.vlines(np.arange(0,1.05,0.1), 0, 1,color = 'teal', lw = 0.8, ls = '--', alpha = 0.2, zorder = 0)\n",
    "    ax3.vlines(np.arange(0,1.05,0.05), 0, 1,color = 'teal', lw = 0.5, ls = '--', alpha = 0.2, zorder = 0)\n",
    "    ax3.hlines(np.arange(0,1.05,0.1), 0, 1,color = 'teal', lw = 0.8, ls = '--', alpha = 0.2, zorder = 0)\n",
    "    ax3.hlines(np.arange(0,1.05,0.05), 0, 1,color = 'teal', lw = 0.5, ls = '--', alpha = 0.2, zorder = 0)\n",
    "    ax3.set_xticks(np.arange(0, 1.1, 0.1))\n",
    "    ax3.set_xticklabels([round(j, 2) for j in np.arange(0, 1.1, 0.1)], color = 'Teal')\n",
    "    ax3.plot(X, N, alpha = 0)\n",
    "    ax3.set_xlim(0, 1.01)\n",
    "    ax3.tick_params(axis = 'both', which = 'major', direction = 'in', length = 6)\n",
    "    ax3.tick_params(axis = 'both', which = 'minor', direction = 'in', length = 4)\n",
    "    ax3.minorticks_on()\n",
    "    ax3.set_xlabel('Frac', color = 'Teal')\n",
    "        \n",
    "    axb.scatter(Lengths, Mags, color = 'orange')\n",
    "    axb.set_xlabel('Number of Epochs')\n",
    "    axb.set_ylabel('Mean Instrumental Mag')\n",
    "    axb.invert_yaxis()\n",
    "    axb.tick_params(axis = 'both', which = 'major', direction = 'in', length = 6)\n",
    "    axb.tick_params(axis = 'both', which = 'minor', direction = 'in', length = 4)\n",
    "    axb.minorticks_on()\n",
    "    \n",
    "    if frac == 0.0:\n",
    "        print('============================== ',str(Filter), 'Filter  ==============================')\n",
    "        print('Total number of stars available: ', len(Lengths))\n",
    "        print('Max number of epochs amongst all stars:', max(Lengths))\n",
    "    else:\n",
    "        print('============================== ',str(Filter), 'Filter  ==============================')\n",
    "        print('Total number of stars available: ', len(Lengths))\n",
    "        print('Max number of epochs amongst all stars:', max(Lengths))\n",
    "        print('Number of stars available with more than ' + str(frac)+' of the max number of epochs: '+str(len([k for k in Lengths if k > frac*max(Lengths)])) )\n",
    "\n",
    "def Detect_Var(Star_File, Filter, High = [], Label = False, Log = False, HighLabel = True, TEL = TEL, AGN_ID = AGN_ID):\n",
    "    \"\"\"Display rms vs mag plot to identify variable stars. *work in progress - do a fitting/binning*\n",
    "\n",
    "    Input:\n",
    "    Star_File - calibrated star dataframe (pd dataframe).\n",
    "    Filter - Filter (string).\n",
    "    High - (Optional) any stars you want to highlight better on the plot for clarity (list/array)\n",
    "    \"\"\"\n",
    "    \n",
    "    #Set up figure\n",
    "    fig, ax = plt.subplots(1,1)\n",
    "    \n",
    "    #Put data on log plot if wanted\n",
    "    if Log == True:\n",
    "        ax.set_yscale('log')\n",
    "    \n",
    "    ax.set_title(Filter + ' Filter')\n",
    "    #All stars in file\n",
    "    Star_IDs = pd.unique(Star_File['id_apass'])\n",
    "    \n",
    "    for ID in Star_IDs:\n",
    "        #Select data for particular star\n",
    "        dat = Locate_Star_ID(Star_File, ID)\n",
    "        mags = dat['m_star'].values[0]\n",
    "        mags_err = dat['m_star_err'].values[0]\n",
    "        \n",
    "        #If you wanted to plot the extra variance\n",
    "        #rms_star = dat['rms_star'].values[0]\n",
    "        #rms_star_err = dat['rms_star_err'].values[0]\n",
    "        \n",
    "        #Compute rms\n",
    "        rms_star = root_mean_squared_deviation(dat['mag_aper'], dat['m_star'])\n",
    "        \n",
    "        if ID in High:\n",
    "            ax.scatter(mags, rms_star, color = 'deeppink', s = 25, marker = 'x', zorder = 10)\n",
    "        else:\n",
    "            ax.scatter(mags, rms_star, color = 'teal', s = 15)\n",
    "        \n",
    "        #Add star IDs nect to data points if wanted\n",
    "        if Label == True:\n",
    "            \n",
    "            if Log == True:\n",
    "                if ID in High:\n",
    "                    t = ax.text(mags, rms_star, str(ID), rotation = 0, color = 'deeppink', alpha = 1.0, size = 10, zorder = 10, weight = 'bold')\n",
    "                else:\n",
    "                    ax.text(mags, rms_star, str(ID), rotation = 90, color = 'black', alpha = 0.5, size = 8)\n",
    "            else:\n",
    "                if ID in High:\n",
    "                    t = ax.text(mags - 0.05, rms_star + 0.003, str(ID), rotation = 0, color = 'deeppink', alpha = 1.0, size = 10, zorder = 10, weight = 'bold')                        \n",
    "                else:\n",
    "                    ax.text(mags - 0.05, rms_star + 0.003, str(ID), rotation = 90, color = 'black', alpha = 0.5, size = 8)\n",
    "        elif HighLabel == True:\n",
    "            if Log == True:\n",
    "                if ID in High:\n",
    "                    t = ax.text(mags, rms_star, str(ID), rotation = 0, color = 'deeppink', alpha = 1.0, size = 10, zorder = 10, weight = 'bold')\n",
    "            else:\n",
    "                if ID in High:\n",
    "                    t = ax.text(mags - 0.05, rms_star + 0.003, str(ID), rotation = 0, color = 'deeppink', alpha = 1.0, size = 10, zorder = 10, weight = 'bold')                        \n",
    "\n",
    "    ax.set_xlabel('Mean Instrumental Magnitude')\n",
    "    ax.set_ylabel('RMS Star')\n",
    "    ax.tick_params(axis = 'both', which = 'major', direction = 'in', length = 6)\n",
    "    ax.tick_params(axis = 'both', which = 'minor', direction = 'in', length = 4)\n",
    "    ax.minorticks_on()\n",
    "    \n",
    "def Plot_LC(Star_Data, Filter, TEL = TEL, err_th = 0.05, Stars = []):\n",
    "    \"\"\"Plot calibrated star lightcurves and total error distribution.\n",
    "       Remove outliers based on a double outlier detection method and inspect cleaned lightcurves.\n",
    "    \n",
    "    Input:\n",
    "    Star_Data - Calibrated star dataframe (pd dataframe).\n",
    "    Filter - Filter (string).\n",
    "    err_th - (Optional) Error threshold for clipping outliers.\n",
    "    Stars - (Optional) If you don't wish to plot aLL the lightcurves for all the stars in the file, input IDs of stars you do want to see (list/array).\n",
    "    \"\"\"\n",
    "    COLORS = ['navy', 'green', 'blue', 'orange', 'purple', 'yellow', 'skyblue', 'violet', 'darkgreen', 'maroon']\n",
    "    \n",
    "    if len(Stars) > 0:\n",
    "        #Selecting only inputed stars\n",
    "        Star_IDs = Stars\n",
    "    else:\n",
    "        #All stars in dataframe\n",
    "        Star_IDs = pd.unique(Star_Data['id_apass'])\n",
    "    \n",
    "    for ID in Star_IDs:\n",
    "        #set up figure\n",
    "        fig0, ax0 = plt.subplots(5,2, sharex = False, figsize = (13, 15))\n",
    "        fig0.subplots_adjust(wspace=0.1, hspace = 0.4)\n",
    "        r = 0\n",
    "        c = 0\n",
    "        \n",
    "        #older MAD method\n",
    "        #scope_mad = []\n",
    "        \n",
    "        for scope in TEL:\n",
    "            #Select data for particular telescope\n",
    "            dat = Locate_Star_ID(Locate_Star_Scope(Star_Data, scope), ID)\n",
    "            err = dat.err_tot\n",
    "            \n",
    "            if len(dat) > 0:\n",
    "                nbins = int(0.8*len(err))\n",
    "                if nbins == 0:\n",
    "                    nbins = 1\n",
    "                hist = ax0[r, c].hist(err, bins = nbins, alpha = 0.7, density = False, color = COLORS[TEL.index(scope)], edgecolor = 'black')\n",
    "                ax0[r, c].vlines(err_th, 0, max(hist[0]), lw = 1.2, ls = '--', color = 'red', label = 'Outlier Threshold')\n",
    "                \n",
    "                #older MAD method\n",
    "                #MAD = np.sum(abs(err - np.median(err))) / len(err)\n",
    "                #ax0[r, c].axvspan(np.median(err) - MAD, np.median(err) + MAD, alpha=0.1, color='red',label = 'MAD')\n",
    "                #ax0[r, c].axvspan(np.median(err) - 2*MAD, np.median(err) + 2*MAD, alpha=0.1, color='orange',label = '2*MAD')\n",
    "                \n",
    "                ax0[r, c].legend()\n",
    "                ax0[r, c].tick_params(axis = 'both', which = 'major', direction = 'in', length = 6)\n",
    "                ax0[r, c].tick_params(axis = 'both', which = 'minor', direction = 'in', length = 4)\n",
    "                ax0[r, c].minorticks_on()\n",
    "                ax0[r, c].set_xlabel('Total error')\n",
    "            ax0[r, c].set_title('Star ' + str(ID) + '  |  '+ str(scope) +'  |  No. of datapoints: ' + str(len(dat)))\n",
    "\n",
    "            if c < 1:\n",
    "                c = c + 1\n",
    "            else:\n",
    "                r = r + 1\n",
    "                c = 0\n",
    "            #older mad method\n",
    "            #scope_mad.append(MAD)\n",
    "\n",
    "        #Set up figure\n",
    "        fig, ax4 = plt.subplots(2,1, figsize = (10, 6), sharex = True)\n",
    "        fig.subplots_adjust(hspace = 0)\n",
    "        ax4[0].set_ylabel('Mag')\n",
    "        ax4[1].set_ylabel('Mag')\n",
    "        ax4[1].set_xlabel('MJD')\n",
    "\n",
    "        tot_dat = 0.\n",
    "        clipped = 0.\n",
    "        for scope in TEL:\n",
    "            #Select data based on particular telescope\n",
    "            dat = Locate_Star_Scope(Locate_Star_Filter(Locate_Star_ID(Star_Data, ID), Filter), scope)\n",
    "            if len(dat) > 0:\n",
    "                #older mad method\n",
    "                #con1 = (dat['err_tot'] > np.median(dat['err_tot']) + scope_mad[TEL.index(scope)])\n",
    "\n",
    "                #First outlier detection method based on specified threshold\n",
    "                con1 = (dat['err_tot'] > err_th)\n",
    "                #Secondary outlier detection method based on deviation from mean\n",
    "                con2 = (abs(dat['m_star'] - dat['mag_aper']) > 3*dat['err_tot'])\n",
    "                outliers = np.where(np.logical_or(con1,con2))[0]\n",
    "                not_outliers = [j for j in np.arange(0, len(dat), 1) if j not in outliers]\n",
    "\n",
    "                dat2a = dat.iloc[outliers]\n",
    "                dat2b = dat.iloc[not_outliers]\n",
    "                tot_dat = tot_dat + len(dat)\n",
    "                clipped = clipped + len(outliers)\n",
    "\n",
    "                ax4[0].scatter(dat2a['MJD'].values, dat2a['mag_aper'].values, color = 'red', s = 30, marker = 'x', zorder = 10, alpha = 1.0, linewidths=1.0)\n",
    "                label = scope\n",
    "\n",
    "                Data = Locate_Star_Scope(dat, scope)\n",
    "                ax4[1].scatter(dat2b['MJD'].values, dat2b['mag_aper'].values, s = 20, zorder = 10, label = label, color = COLORS[TEL.index(scope)])\n",
    "                ax4[1].errorbar(dat2b['MJD'].values, dat2b['mag_aper'].values, yerr = dat2b['err_tot'].values,ls = 'none', color = COLORS[TEL.index(scope)], lw = 0.5)\n",
    "                ax4[0].errorbar(dat['MJD'].values, dat['mag_aper'].values, yerr = (dat['err_aper'].values**2 + dat['rms_sc'].values**2 + dat['rms_t'].values**2 + dat['rms_star'].values**2)**0.5, ls = 'none', color = COLORS[TEL.index(scope)], lw = 0.5)\n",
    "                ax4[0].scatter(dat['MJD'].values, dat['mag_aper'].values, s = 20, label = label, color = COLORS[TEL.index(scope)])\n",
    "        \n",
    "        Data = Locate_Star_ID(Star_Data, ID)\n",
    "        ax4[0].hlines(Data['m_star'].values[0], min(Data['MJD'].values), max(Data['MJD'].values), lw = 1, ls = '--', label = 'Star: ' + str(ID))\n",
    "        ax4[0].legend(loc='center right', bbox_to_anchor=(1.15, 0.5), fontsize = 'x-small')\n",
    "        ax4[0].invert_yaxis()\n",
    "        ax4[0].tick_params(axis = 'both', which = 'major', direction = 'in', length = 6)\n",
    "        ax4[0].tick_params(axis = 'both', which = 'minor', direction = 'in', length = 4)\n",
    "        ax4[0].minorticks_on()\n",
    "        ax4[0].set_title('Star ' + str(ID) + '  |  Datapoints Clipped: '+str(int(clipped))+' out of '+str(int(tot_dat)))\n",
    "        ax4[1].hlines(Data['m_star'].values[0], min(Data['MJD'].values), max(Data['MJD'].values), lw = 1, ls = '--', label = 'Star: ' + str(ID))\n",
    "        ax4[1].legend(loc='center right', bbox_to_anchor=(1.15, 0.5), fontsize = 'x-small')\n",
    "        ax4[1].invert_yaxis()\n",
    "        ax4[1].tick_params(axis = 'both', which = 'major', direction = 'in', length = 6)\n",
    "        ax4[1].tick_params(axis = 'both', which = 'minor', direction = 'in', length = 4)\n",
    "        ax4[1].minorticks_on()\n",
    "        \n",
    "def AGN_LC(Original_Star_File, Star_Data, Filter, err_th = 0.05, Plot = True, Rem_out = True, AGN_ID = AGN_ID, TEL = TEL, zp = [0.,0.]):\n",
    "    \"\"\" Return Dataframe with calibrated AGN Lightcurve. Choose whether to leave in outliers.\n",
    "        Can also plot the AGN lightcurve.\n",
    "    \n",
    "    Input:\n",
    "    Original_Star_File - Original lco file (pd dataframe).\n",
    "    Star_Data - Calibrated star dataframe (pd dataframe).\n",
    "    Filter - Filter (string).\n",
    "    err_th - (Optional) Error threshold for clipping outliers.\n",
    "    Plot - (Optional) Choose whether to display the error distributions and lightcurves (Bool).\n",
    "    Rem_out = (Optional) Choose whether to remove outliers from returned AGN dataframe (Bool). \n",
    "    \n",
    "    Output:\n",
    "    AGN_DF - dataframe containing calibrated AGN lightcurve.\n",
    "   \"\"\"\n",
    "    \n",
    "    #Set up dataframe\n",
    "    columns = ['Filter','telid', 'MJD', 'mag', 'err', 'err_sys']\n",
    "    AGN_DF = pd.DataFrame(columns=columns)\n",
    "    \n",
    "    AGN_Data = Locate_Star_ID(Locate_Star_Filter(Original_Star_File, Filter), AGN_ID)\n",
    "    for scope in TEL:\n",
    "        \n",
    "        #Select data based on telescope\n",
    "        AGN_Data2 = Locate_Star_Scope(AGN_Data, scope)\n",
    "\n",
    "        for E in pd.unique(AGN_Data2['MJD']):\n",
    "            \n",
    "            #Select data based on Epoch\n",
    "            AGN_Data3 = Locate_Star_Epoch(AGN_Data2, E)\n",
    "            Star_Data2 = Locate_Star_Epoch(Locate_Star_Filter(Locate_Star_Scope(Star_Data, scope), Filter), E).head(1)\n",
    "            \n",
    "            #Assuming the AGN has a datapoint for that epoch...\n",
    "            if len(Star_Data2) != 0:\n",
    "                \n",
    "                #Apply correction parameters\n",
    "                AGN_mag = AGN_Data3['mag_aper'].values - Star_Data2['DMAGT'].values - Star_Data2['DMAGS'].values + zp[0]\n",
    "                AGN_mag_err = (AGN_Data3['err_aper'].values[0]**2 + Star_Data2['rms_sc'].values[0]**2 + Star_Data2['rms_t'].values[0]**2)**0.5\n",
    "                \n",
    "                temp = [{'Filter': Filter, 'telid': scope, 'MJD': E, 'mag': AGN_mag[0], 'err': AGN_mag_err, 'err_sys': AGN_mag_err*0.0+zp[1]}]\n",
    "                temp_DF = pd.DataFrame(temp)\n",
    "                \n",
    "                AGN_DF = pd.concat([AGN_DF, temp_DF], ignore_index=True)\n",
    "                \n",
    "                # AGN_DF = AGN_DF.append({'Filter': Filter, 'telid': scope, 'MJD': E, 'mag': AGN_mag[0], 'err': AGN_mag_err, 'err_sys': AGN_mag_err*0.0+zp[1]}, ignore_index = True)\n",
    "    \n",
    "    if Plot == True:\n",
    "        print('======================================================== ' + Filter + ' Filter' + ' ========================================================')\n",
    "        \n",
    "        COLORS = ['navy', 'green', 'blue', 'orange', 'purple', 'yellow', 'skyblue', 'violet', 'darkgreen', 'maroon']\n",
    "        \n",
    "        #Set up figures\n",
    "        fig0, ax0 = plt.subplots(5,2, sharex = False, figsize = (13, 15))\n",
    "        fig0.subplots_adjust(wspace=0.1, hspace = 0.4)\n",
    "        \n",
    "        fig, ax = plt.subplots(1,1,figsize=(13, 6), sharex = False)\n",
    "        ax.set_title('Uncalibrated AGN Lightcurve')\n",
    "        ax.set_xlabel('MJD')\n",
    "        ax.set_ylabel('Mag')\n",
    "        ax.tick_params(axis = 'both', which = 'major', direction = 'in', length = 6)\n",
    "        ax.tick_params(axis = 'both', which = 'minor', direction = 'in', length = 4)\n",
    "        ax.minorticks_on()\n",
    "        ax.invert_yaxis()\n",
    "\n",
    "        fig2, ax2 = plt.subplots(1,1,figsize=(13, 6))\n",
    "        fig3, ax3 = plt.subplots(1,1,figsize=(13, 6))\n",
    "        ax2.set_title('Calibrated AGN Lightcurve with outliers')\n",
    "        ax2.set_ylabel('Mag')\n",
    "        ax2.tick_params(axis = 'both', which = 'major', direction = 'in', length = 6)\n",
    "        ax2.tick_params(axis = 'both', which = 'minor', direction = 'in', length = 4)\n",
    "        ax2.minorticks_on()\n",
    "        ax2.invert_yaxis()\n",
    "        \n",
    "        clipped = len(AGN_DF.loc[AGN_DF['err'] > err_th])\n",
    "        ax3.set_title('Calibrated AGN Lightcurve without outliers  |  '+ 'Datapoints Clipped: ' +str(int(clipped))+' out of '+str(len(AGN_DF))+'  |  Outlier error threshold: '+ str(round(100*err_th, 1)) + '%')\n",
    "        ax3.set_ylabel('Mag')\n",
    "        ax3.set_xlabel('MJD')\n",
    "        ax3.tick_params(axis = 'both', which = 'major', direction = 'in', length = 6)\n",
    "        ax3.tick_params(axis = 'both', which = 'minor', direction = 'in', length = 4)\n",
    "        ax3.minorticks_on()\n",
    "        ax3.invert_yaxis()\n",
    "        \n",
    "        \n",
    "        r = 0\n",
    "        c = 0\n",
    "        #Plot error distributions\n",
    "        for scope in TEL:\n",
    "            dat = Locate_Star_Scope(AGN_DF, scope)\n",
    "            \n",
    "            if len(dat) > 0:\n",
    "                nbins = int(len(dat) / 2)\n",
    "                if nbins == 0:\n",
    "                    nbins = 1\n",
    "                hist = ax0[r, c].hist(dat['err'].values, bins = nbins, alpha = 0.7, density = False, color = COLORS[TEL.index(scope)], edgecolor = 'black')\n",
    "                ax0[r, c].vlines(err_th, 0, max(hist[0]), lw = 1.2, ls = '--', color = 'red', label = 'Outlier Threshold: '+ str(err_th))\n",
    "\n",
    "                #Old mad selection\n",
    "                #ax0[r, c].vlines(np.median(dat['err'].values), 0, max(hist[0]), lw = 1, ls = '--', color = 'maroon', label = 'Median: '+ str(round(np.median(dat['err'].values), 6)))\n",
    "                #MAD = np.sum(abs(errors[TEL.index(scope)] - np.median(errors[TEL.index(scope)]))) / len(errors[TEL.index(scope)])\n",
    "                #ax0[r, c].axvspan(np.median(dat['err'].values) - MAD, np.median(dat['err'].values) + MAD, alpha=0.1, color='red',label = 'MAD')\n",
    "                #ax0[r, c].axvspan(np.median(dat['err'].values) - 2*MAD, np.median(dat['err'].values) + 2*MAD, alpha=0.1, color='orange',label = '2*MAD')\n",
    "\n",
    "                ax0[r,c].legend()\n",
    "                ax0[r,c].tick_params(axis = 'both', which = 'major', direction = 'in', length = 6)\n",
    "                ax0[r,c].tick_params(axis = 'both', which = 'minor', direction = 'in', length = 4)\n",
    "                ax0[r,c].minorticks_on()\n",
    "                ax0[r,c].set_title(str(scope) + '  |  No. of datapoints: '+ str(len(dat['err'].values)))\n",
    "                ax0[r,c].set_xlabel('Total error')\n",
    "    \n",
    "            if c < 1:\n",
    "                c = c + 1\n",
    "            else:\n",
    "                r = r + 1\n",
    "                c = 0\n",
    "            \n",
    "            #Plot uncalibrated data\n",
    "            ax.scatter(Locate_Star_Scope(AGN_Data, scope)['MJD'], Locate_Star_Scope(AGN_Data, scope)['mag_aper'], s = 5, label = scope, color = COLORS[TEL.index(scope)])\n",
    "            \n",
    "            #Plot calibrated data with errors\n",
    "            ax2.scatter(dat['MJD'], dat['mag'], s = 5, color = COLORS[TEL.index(scope)], label = scope)\n",
    "            ax2.errorbar(dat['MJD'], dat['mag'], yerr = dat['err'], lw = 0.5, color = COLORS[TEL.index(scope)], ls = 'none')\n",
    "            \n",
    "            #Plot calibrated data without errors\n",
    "            dat2 = dat.loc[dat['err'] < err_th]\n",
    "            ax3.scatter(dat2['MJD'], dat2['mag'], s = 5, color = COLORS[TEL.index(scope)], label = scope)\n",
    "            ax3.errorbar(dat2['MJD'], dat2['mag'], yerr = dat2['err'], lw = 0.5, color = COLORS[TEL.index(scope)], ls = 'none')\n",
    "            \n",
    "        ax.legend()\n",
    "        ax2.legend()\n",
    "        ax3.legend()\n",
    "    \n",
    "    if Rem_out == True:\n",
    "        print('Returning dataframe with outliers taken out')\n",
    "        AGN_DF = AGN_DF.loc[AGN_DF['err'] < err_th]\n",
    "        return AGN_DF\n",
    "    else:\n",
    "        print('Returning dataframe with outliers kept in')\n",
    "        return AGN_DF\n",
    "    \n",
    "def AGN_LC2(Original_Star_File, Star_Data, Filter, err_th = 0.05, Plot = True, Rem_out = True, AGN_ID = AGN_ID, TEL = TEL, zp = [0.,0.]):\n",
    "    \"\"\" Return Dataframe with calibrated AGN Lightcurve. Choose whether to leave in outliers.\n",
    "        Can also plot the AGN lightcurve with shared axes.\n",
    "    \n",
    "    Input:\n",
    "    Original_Star_File - Original lco file (pd dataframe).\n",
    "    Star_Data - Calibrated star dataframe (pd dataframe).\n",
    "    Filter - Filter (string).\n",
    "    err_th - (Optional) Error threshold for clipping outliers.\n",
    "    Plot - (Optional) Choose whether to display the error distributions and lightcurves (Bool).\n",
    "    Rem_out = (Optional) Choose whether to remove outliers from returned AGN dataframe (Bool). \n",
    "    \n",
    "    Output:\n",
    "    AGN_DF - dataframe containing calibrated AGN lightcurve.\n",
    "   \"\"\"\n",
    "    \n",
    "    #Set up dataframe\n",
    "    columns = ['Filter','telid', 'MJD', 'mag', 'err', 'err_sys']\n",
    "    AGN_DF = pd.DataFrame(columns=columns)\n",
    "    \n",
    "    AGN_Data = Locate_Star_ID(Locate_Star_Filter(Original_Star_File, Filter), AGN_ID)\n",
    "    for scope in TEL:\n",
    "        \n",
    "        #Select data based on telescope\n",
    "        AGN_Data2 = Locate_Star_Scope(AGN_Data, scope)\n",
    "\n",
    "        for E in pd.unique(AGN_Data2['MJD']):\n",
    "            \n",
    "            #Select data based on Epoch\n",
    "            AGN_Data3 = Locate_Star_Epoch(AGN_Data2, E)\n",
    "            Star_Data2 = Locate_Star_Epoch(Locate_Star_Filter(Locate_Star_Scope(Star_Data, scope), Filter), E).head(1)\n",
    "            \n",
    "            #Assuming the AGN has a datapoint for that epoch...\n",
    "            if len(Star_Data2) != 0:\n",
    "                \n",
    "                #Apply correction parameters\n",
    "                AGN_mag = AGN_Data3['mag_aper'].values - Star_Data2['DMAGT'].values - Star_Data2['DMAGS'].values + zp[0]\n",
    "                AGN_mag_err = (AGN_Data3['err_aper'].values[0]**2 + Star_Data2['rms_sc'].values[0]**2 + Star_Data2['rms_t'].values[0]**2)**0.5\n",
    "                \n",
    "                temp = [{'Filter': Filter, 'telid': scope, 'MJD': E, 'mag': AGN_mag[0], 'err': AGN_mag_err, 'err_sys': AGN_mag_err*0.0+zp[1]}]\n",
    "                temp_DF = pd.DataFrame(temp)\n",
    "                \n",
    "                AGN_DF = pd.concat([AGN_DF, temp_DF], ignore_index=True)\n",
    "                \n",
    "                # AGN_DF = AGN_DF.append({'Filter': Filter, 'telid': scope, 'MJD': E, 'mag': AGN_mag[0], 'err': AGN_mag_err, 'err_sys': AGN_mag_err*0.0+zp[1]}, ignore_index = True)\n",
    "    \n",
    "    if Plot == True:\n",
    "        print('======================================================== ' + Filter + ' Filter' + ' ========================================================')\n",
    "        \n",
    "        COLORS = ['navy', 'green', 'blue', 'orange', 'purple', 'yellow', 'skyblue', 'violet', 'darkgreen', 'maroon']\n",
    "        \n",
    "        #Set up figures        \n",
    "        fig, ax = plt.subplots(2, figsize=(13,12), sharex=True)\n",
    "        plt.subplots_adjust(wspace=0, hspace=0)\n",
    "        ax[0].tick_params(axis = 'y', which = 'major', direction = 'out', length = 6)\n",
    "        ax[0].tick_params(axis = 'y', which = 'minor', direction = 'out', length = 4)\n",
    "        ax[1].tick_params(axis = 'y', which = 'major', direction = 'out', length = 6)\n",
    "        ax[1].tick_params(axis = 'y', which = 'minor', direction = 'out', length = 4)\n",
    "        ax[1].tick_params(axis = 'x', which = 'major', direction = 'out', length = 6)\n",
    "        ax[1].tick_params(axis = 'x', which = 'minor', direction = 'out', length = 4)\n",
    "        ax[0].set_ylabel('Uncalibrated Mag')\n",
    "        ax[0].set_xlabel('MJD')\n",
    "        ax[1].set_ylabel('Calibrated Mag')\n",
    "        ax[1].set_xlabel('MJD')        \n",
    "        ax[0].minorticks_on()\n",
    "        ax[0].invert_yaxis()\n",
    "        ax[1].minorticks_on()\n",
    "        ax[1].invert_yaxis()\n",
    "        \n",
    "        \n",
    "        r = 0\n",
    "        c = 0\n",
    "        #Plot error distributions\n",
    "        for scope in TEL:\n",
    "            dat = Locate_Star_Scope(AGN_DF, scope)\n",
    "            \n",
    "            \n",
    "    \n",
    "            if c < 1:\n",
    "                c = c + 1\n",
    "            else:\n",
    "                r = r + 1\n",
    "                c = 0\n",
    "            \n",
    "            #Plot uncalibrated data\n",
    "            ax[0].scatter(Locate_Star_Scope(AGN_Data, scope)['MJD'], Locate_Star_Scope(AGN_Data, scope)['mag_aper'], s = 5, label = scope, color = COLORS[TEL.index(scope)])\n",
    "            \n",
    "            #Plot calibrated data without errors\n",
    "            dat2 = dat.loc[dat['err'] < err_th]\n",
    "            ax[1].scatter(dat2['MJD'], dat2['mag'], s = 5, color = COLORS[TEL.index(scope)], label = scope)\n",
    "            ax[1].errorbar(dat2['MJD'], dat2['mag'], yerr = dat2['err'], lw = 0.5, color = COLORS[TEL.index(scope)], ls = 'none')\n",
    "            \n",
    "        ax[0].legend(ncol=3, bbox_to_anchor=(0.8, -1.2), fontsize=20)\n",
    "        \n",
    "    plt.savefig(fr\"{Filter}_agnlc\", dpi=300, bbox_inches='tight')\n",
    "    \n",
    "    if Rem_out == True:\n",
    "        print('Returning dataframe with outliers taken out')\n",
    "        AGN_DF = AGN_DF.loc[AGN_DF['err'] < err_th]\n",
    "        return AGN_DF\n",
    "    else:\n",
    "        print('Returning dataframe with outliers kept in')\n",
    "        return AGN_DF\n",
    "    \n",
    "def Plot_by_year(AGN_dataframe, year):\n",
    "    \"\"\" Plot particular year of AGN data for closer visual inspection.\n",
    "        *Note: MJD boundaries may need to be adjusted here for individual AGNs.\n",
    "    \n",
    "    Input:\n",
    "    AGN_dataframe - dataframe containing calibrated AGN data from AGN_LC function (pd dataframe).\n",
    "    year - year of data to plot (int)\n",
    "    \"\"\"\n",
    "    \n",
    "    COLORS = ['navy', 'orange', 'yellow', 'green', 'blue', 'indigo', 'violet', 'pink', 'brown', 'gray']\n",
    "    \n",
    "    #Ref dates may need to be changed depending on specific AGN monitoring\n",
    "    if year == 1:\n",
    "        ref_date1 = 0\n",
    "        ref_date2 = 57754\n",
    "    elif year == 2:\n",
    "        ref_date1 = 57754\n",
    "        ref_date2 = 58119\n",
    "    elif year == 3:\n",
    "        ref_date1 = 58119\n",
    "        ref_date2 = 10**(10)\n",
    "\n",
    "    fig, ax = plt.subplots(len(FILTERS),1, figsize = (15, 10), sharex = True)\n",
    "    fig.subplots_adjust(wspace=0.1, hspace = 0.0)\n",
    "    \n",
    "    ax[0].set_title('Year ' + str(year), size = 15)\n",
    "    ax[-1].set_xlabel('MJD')\n",
    "    for Filter in FILTERS:\n",
    "        dat0 = Locate_Star_Filter(AGN_dataframe, Filter)\n",
    "        dat = dat0.loc[(dat0['MJD'] > ref_date1) & (dat0['MJD'] < ref_date2)]\n",
    "    \n",
    "        ax[FILTERS.index(Filter)].scatter(dat.MJD, dat.mag, s = 10, color = 'Teal', zorder = 0, label = Filter)\n",
    "        ax[FILTERS.index(Filter)].errorbar(dat.MJD, dat.mag, yerr = dat.err, ls = 'none', color = 'Teal', zorder = 0)\n",
    "        ax[FILTERS.index(Filter)].invert_yaxis()\n",
    "        ax[FILTERS.index(Filter)].legend()\n",
    "        ax[FILTERS.index(Filter)].set_ylim(min(dat.mag) - 0.05, max(dat.mag) + 0.05)\n",
    "        ax[FILTERS.index(Filter)].vlines(np.arange(min(dat['MJD'].values), max(dat['MJD'].values) + 10, 10), min(dat['mag'].values) - 0.3, max(dat['mag'].values) + 0.3, color = 'black', ls = '--', lw = 0.5, alpha = 0.8)\n",
    "        ax[FILTERS.index(Filter)].tick_params(axis = 'both', which = 'major', direction = 'in', length = 6)\n",
    "        ax[FILTERS.index(Filter)].tick_params(axis = 'both', which = 'minor', direction = 'in', length = 4)\n",
    "        ax[FILTERS.index(Filter)].minorticks_on()\n",
    "        ax[FILTERS.index(Filter)].set_ylabel('mag')\n",
    "        ax[FILTERS.index(Filter)].invert_yaxis()\n",
    "\n",
    "def Check_RMS_MAG(Star_File, Filter = FILTERS[0]):\n",
    "    \"\"\"Compute the mean magnitude & rms of initial star data, calibrated using zeropoints, and plot rms vs mag to \n",
    "       check AGN ID.\n",
    "    Inputs:\n",
    "    Star_File - Original file with zeropoints (pd dataframe).\n",
    "    Filter - (Optional) Filter (str).\n",
    "    \"\"\"\n",
    "    \n",
    "    #Set up figure\n",
    "    fig, ax = plt.subplots(1,1, figsize = (5, 5))\n",
    "    #Select data based on filter\n",
    "    dat = Locate_Star_Filter(Star_File, Filter)\n",
    "    Star_IDs = pd.unique(dat['id_apass'])\n",
    "    \n",
    "    for ID in Star_IDs:\n",
    "        #Select data based on star ID\n",
    "        dat2 = Locate_Star_ID(dat, ID)\n",
    "        mag = dat2.mag_aper.values + dat2.zp.values\n",
    "        \n",
    "        #Compute mean magnitude\n",
    "        mean = np.mean(mag)\n",
    "        #Compute rms\n",
    "        rms = root_mean_squared_deviation(mag, mean)\n",
    "        ax.scatter(mean - 0.2, rms, color = 'lightcoral', edgecolor = 'black', linewidth = 1,  s = 25)\n",
    "        ax.text(mean, rms, str(ID), rotation = 90, size = 10, alpha = 0.7)\n",
    "    ax.set_yscale('log')\n",
    "    ax.set_xlabel('mag', size = 15)\n",
    "    ax.set_ylabel('rms', size = 15)\n",
    "    ax.tick_params(axis = 'both', which = 'major', direction = 'in', length = 6)\n",
    "    ax.tick_params(axis = 'both', which = 'minor', direction = 'in', length = 4)\n",
    "    ax.minorticks_on()\n",
    "    \n",
    "def Check_LC(Star_File, ID, Filter = FILTERS[0]):\n",
    "    \"\"\"Plot lightcurve of chosen ID, using original data with zeropoints, to help identify the AGN.\n",
    "    Inputs:\n",
    "    Star_File - Original file with zeropoints (pd dataframe).\n",
    "    ID - ID of potential AGN (float)\n",
    "    Filter - (Optional) Filter (str).\n",
    "    \"\"\"\n",
    "    #Set up figure\n",
    "    fig, ax = plt.subplots(1,1, figsize = (12, 4))\n",
    "    \n",
    "    #Select data based on ID and Filter\n",
    "    dat = Locate_Star_ID(Locate_Star_Filter(Star_File, Filter), ID)\n",
    "    mag = dat.mag_aper.values + dat.zp.values\n",
    "    MJD = dat.MJD\n",
    "    ax.set_title('Star ID: '+ str(ID))\n",
    "    ax.scatter(MJD, mag, s = 15, color = 'lightcoral', edgecolor = 'black', linewidth = 1)\n",
    "    ax.set_xlabel('MJD', size = 15)\n",
    "    ax.set_ylabel('Mag', size = 15)\n",
    "    ax.tick_params(axis = 'both', which = 'major', direction = 'in', length = 6)\n",
    "    ax.tick_params(axis = 'both', which = 'minor', direction = 'in', length = 4)\n",
    "    ax.minorticks_on()\n",
    "    ax.invert_yaxis()\n",
    "\n",
    "def Convergence_Plot(Traces, mode, parameter, parameter_err = None, lim = 20, zoom = 0):\n",
    "    \"\"\" Plots of traces of intercalibration algorithm to visualy check convergence.\n",
    "    \n",
    "    Input:\n",
    "    Traces - Traces output from Corr() (list).\n",
    "    mode - 'double' or 'single' (str): \n",
    "            double: Plots the trace of a parameter AND its error as a shell around the plot.\n",
    "            single: Plots any ONE parameter from the list above. Can be used to check the convergence of the errors themselves for example.\n",
    "    parameter - parameter to check convergence for (str).\n",
    "    parameter_err - (Optional) if mode = 'double', parameter error (str).\n",
    "    lim - (Optional) number of plots to show for epoch & star_mag related parameters as there can be a lot of them (int).\n",
    "    zoom - (Optional) zoom into convergence tail to check finer fluctuations (int).\n",
    "    \"\"\"\n",
    "    \n",
    "    #Possible parameters to check\n",
    "    names = ['dmagt', 'dmagt_err', 'rms_t', 'rms_t_err', 'DMAGT', 'DMAGT_err', 'dmags', 'dmags_err', 'rms_sc',\n",
    "             'rms_sc_err', 'm_star', 'm_star_err', 'rms_star', 'rms_star_err', 'DMAGS', 'DMAGS_err']\n",
    "    scope_pars = ['dmags', 'dmags_err', 'rms_sc','rms_sc_err', 'DMAGS', 'DMAGS_err']\n",
    "    time_pars = ['dmagt', 'dmagt_err', 'rms_t','rms_t_err', 'DMAGT', 'DMAGT_err']\n",
    "    star_pars = ['m_star', 'm_star_err', 'rms_star', 'rms_star_err']\n",
    "    iterations = len(Traces[-1])\n",
    "    \n",
    "    if mode == 'single':\n",
    "        par = Traces[names.index(parameter)]\n",
    "            \n",
    "        if parameter in scope_pars:\n",
    "            COLORS = ['red', 'green', 'blue', 'purple', 'black', 'gray', 'violet', 'teal', 'orange', 'yellow']\n",
    "\n",
    "            #Set up plot\n",
    "            fig,ax = plt.subplots(2,5, sharex = True, figsize = (20, 10))\n",
    "            fig.subplots_adjust(wspace=0.3, hspace = 0)\n",
    "\n",
    "            #Iteration number\n",
    "            IT = np.arange(1, iterations + 1, 1)[zoom:]\n",
    "            r = 0\n",
    "            c = 0\n",
    "            for j in range(len(TEL)):\n",
    "                D = []\n",
    "                for i in range(len(par)):\n",
    "                    D.append(par[i][j])\n",
    "                D = D[zoom:]\n",
    "\n",
    "                ax[r, c].plot(IT, D, label = 'Scope: ' + str(TEL[j]), color = COLORS[j], lw = 0.8)\n",
    "                ax[r, c].legend()\n",
    "\n",
    "                if r == 1:\n",
    "                    ax[r, c].set_xlabel('Iteration')\n",
    "                if c == 0:\n",
    "                    ax[r,c].set_ylabel(parameter)\n",
    "\n",
    "                if c < 4:\n",
    "                    c = c + 1\n",
    "                else:\n",
    "                    c = 0\n",
    "                    r = r + 1\n",
    "        \n",
    "        elif parameter in time_pars:\n",
    "            SCOPE = 0\n",
    "            IT = np.arange(1, iterations + 1, 1)[zoom:]\n",
    "            r = 0\n",
    "            c = 0            \n",
    "            if lim < 4:\n",
    "                lim = 4\n",
    "            fig, ax = plt.subplots(math.ceil(lim / 4), 4, sharex = True, figsize = (30,20))\n",
    "            fig.subplots_adjust(wspace=0.2, hspace = 0)\n",
    "            for i in range(lim):\n",
    "                D = []\n",
    "                for j in range(len(par)):\n",
    "                    D.append(par[j][SCOPE][i])\n",
    "                D = D[zoom:]\n",
    "                ax[r, c].plot(IT, D, label = 'MJD: ' + str(i), lw = 0.8)\n",
    "                ax[r, c].legend()\n",
    "                ax[r, c].ticklabel_format(useOffset=False)\n",
    "\n",
    "                if r > math.ceil(lim / 4) - 4:\n",
    "                    ax[r, c].set_xlabel('Iteration')\n",
    "                if c == 0:\n",
    "                    ax[r,c].set_ylabel(parameter)\n",
    "\n",
    "                if c < 3:\n",
    "                    c = c + 1\n",
    "                else:\n",
    "                    c = 0\n",
    "                    r = r + 1\n",
    "\n",
    "        elif parameter in star_pars:\n",
    "            IT = np.arange(1, iterations + 1, 1)[zoom:]\n",
    "            r = 0\n",
    "            c = 0\n",
    "\n",
    "            if lim < 5:\n",
    "                lim = 5\n",
    "\n",
    "            fig,ax = plt.subplots(math.ceil(lim / 5), 5, sharex = True, sharey = False, figsize = (20, 10))\n",
    "            fig.subplots_adjust(wspace=0.4, hspace = 0)\n",
    "            for j in range(lim):\n",
    "                D = []\n",
    "                for i in range(len(par)):\n",
    "                    D.append(par[i][j])\n",
    "                D = D[zoom:]\n",
    "                ax[r, c].plot(IT, D, lw = 0.8)\n",
    "                ax[r, c].ticklabel_format(useOffset=False)\n",
    "\n",
    "                if r > math.ceil(lim / 5) - 5:\n",
    "                    ax[r, c].set_xlabel('Iteration')\n",
    "                if c == 0:\n",
    "                    ax[r,c].set_ylabel(parameter)\n",
    "\n",
    "                if c < 4:\n",
    "                    c = c + 1\n",
    "                else:\n",
    "                    c = 0\n",
    "                    r = r + 1         \n",
    "    \n",
    "    elif mode == 'double':\n",
    "        \n",
    "        if parameter_err == None:\n",
    "            raise ValueError(\"With 'double' mode, par and par_err must be specified.\")\n",
    "        \n",
    "        else:\n",
    "            par = Traces[names.index(parameter)]\n",
    "            par_err = Traces[names.index(parameter_err)]\n",
    "            \n",
    "            if parameter in scope_pars:\n",
    "                COLORS = ['red', 'green', 'blue', 'purple', 'black', 'gray', 'violet', 'teal', 'orange', 'yellow']\n",
    "\n",
    "                #Set up plot\n",
    "                fig,ax = plt.subplots(2,5, sharex = True, figsize = (20, 10))\n",
    "                fig.subplots_adjust(wspace=0.3, hspace = 0)\n",
    "\n",
    "                #Iteration number\n",
    "                IT = np.arange(1, iterations + 1, 1)[zoom:]\n",
    "                r = 0\n",
    "                c = 0\n",
    "                for j in range(len(TEL)):\n",
    "                    D = []\n",
    "                    D_err = []\n",
    "                    for i in range(len(par)):\n",
    "                        D.append(par[i][j])\n",
    "                        D_err.append(par_err[i][j])\n",
    "                    D = D[zoom:]\n",
    "                    D_err = D_err[zoom:]\n",
    "                    \n",
    "                    ax[r, c].plot(IT, D, label = 'Scope: ' + str(TEL[j]), color = COLORS[j], lw = 0.8)\n",
    "                    ax[r, c].legend()\n",
    "\n",
    "                    ax[r, c].plot(IT, np.array(D) + np.array(D_err), color = COLORS[j], lw = 0.8, ls = '--')\n",
    "                    ax[r, c].plot(IT, np.array(D) - np.array(D_err), color = COLORS[j], lw = 0.8, ls = '--')\n",
    "                    ax[r, c].fill_between(IT, np.array(D) - np.array(D_err), np.array(D) + np.array(D_err), color = COLORS[j], alpha = 0.3)\n",
    "\n",
    "                    if r == 1:\n",
    "                        ax[r, c].set_xlabel('Iteration')\n",
    "                    if c == 0:\n",
    "                        ax[r,c].set_ylabel(parameter)\n",
    "\n",
    "                    if c < 4:\n",
    "                        c = c + 1\n",
    "                    else:\n",
    "                        c = 0\n",
    "                        r = r + 1\n",
    "            elif parameter in time_pars:\n",
    "                SCOPE = 0\n",
    "                IT = np.arange(1, iterations + 1, 1)[zoom:]\n",
    "                r = 0\n",
    "                c = 0            \n",
    "                if lim < 4:\n",
    "                    lim = 4\n",
    "                fig, ax = plt.subplots(math.ceil(lim / 4), 4, sharex = True, figsize = (30,20))\n",
    "                fig.subplots_adjust(wspace=0.2, hspace = 0)\n",
    "                for i in range(lim):\n",
    "                    D = []\n",
    "                    D_err = []\n",
    "                    for j in range(len(par)):\n",
    "                        D.append(par[j][SCOPE][i])\n",
    "                        D_err.append(par_err[j][SCOPE][i])\n",
    "                    D = D[zoom:]\n",
    "                    D_err = D_err[zoom:]\n",
    "                    ax[r, c].plot(IT, D, label = 'MJD: ' + str(i), lw = 0.8)\n",
    "                    ax[r, c].legend()\n",
    "                    ax[r, c].ticklabel_format(useOffset=False)\n",
    "\n",
    "                    ax[r, c].plot(IT, np.array(D) + np.array(D_err), lw = 0.8, ls = '--', color = 'blue')\n",
    "                    ax[r, c].plot(IT, np.array(D) - np.array(D_err), lw = 0.8, ls = '--', color = 'blue')\n",
    "                    ax[r, c].fill_between(IT, np.array(D) - np.array(D_err), np.array(D) + np.array(D_err), alpha = 0.3)\n",
    "\n",
    "                    if r > math.ceil(lim / 4) - 4:\n",
    "                        ax[r, c].set_xlabel('Iteration')\n",
    "                    if c == 0:\n",
    "                        ax[r,c].set_ylabel(parameter)\n",
    "\n",
    "                    if c < 3:\n",
    "                        c = c + 1\n",
    "                    else:\n",
    "                        c = 0\n",
    "                        r = r + 1\n",
    "\n",
    "            elif parameter in star_pars:\n",
    "                IT = np.arange(1, iterations + 1, 1)[zoom:]\n",
    "                r = 0\n",
    "                c = 0\n",
    "\n",
    "                if lim < 5:\n",
    "                    lim = 5\n",
    "\n",
    "                fig,ax = plt.subplots(math.ceil(lim / 5), 5, sharex = True, sharey = False, figsize = (20, 10))\n",
    "                fig.subplots_adjust(wspace=0.4, hspace = 0)\n",
    "                for j in range(lim):\n",
    "                    D = []\n",
    "                    D_err = []\n",
    "                    for i in range(len(par)):\n",
    "                        D.append(par[i][j])\n",
    "                        D_err.append(par_err[i][j])\n",
    "                    D = D[zoom:]\n",
    "                    D_err = D_err[zoom:]\n",
    "                    ax[r, c].plot(IT, D, lw = 0.8)\n",
    "                    ax[r, c].ticklabel_format(useOffset=False)\n",
    "\n",
    "                    ax[r, c].plot(IT, np.array(D) + np.array(D_err), lw = 0.8, ls = '--')\n",
    "                    ax[r, c].plot(IT, np.array(D) - np.array(D_err), lw = 0.8, ls = '--')\n",
    "                    ax[r, c].fill_between(IT, np.array(D) - np.array(D_err), np.array(D) + np.array(D_err), alpha = 0.3)\n",
    "\n",
    "                    if r > math.ceil(lim / 5) - 5:\n",
    "                        ax[r, c].set_xlabel('Iteration')\n",
    "                    if c == 0:\n",
    "                        ax[r,c].set_ylabel(parameter)\n",
    "\n",
    "                    if c < 4:\n",
    "                        c = c + 1\n",
    "                    else:\n",
    "                        c = 0\n",
    "                        r = r + 1\n",
    "    else:\n",
    "        raise ValueError(str(mode) + \" is not a valid mode input. Please select either 'single' or 'double'.\")\n",
    "\n",
    "def Corr(Star_File, Filter, MAX_LOOPS = 100, bad_IDs = [], safe = 0.5, frac = 0.3, TEL = TEL, AGN_ID = AGN_ID, Star_Lim = None):\n",
    "    \"\"\"Algorithm to compute telescope correction parameters, star magnitudes, and extra variance as well as their errors.\n",
    "    \n",
    "    Input:\n",
    "    Star_File - Uncalibrated Star data (pd dataframe).\n",
    "    Filter - Filter (String).\n",
    "    MAX_LOOPS - (Optional) Max number of loops if convergence isn't reached (int).\n",
    "    bad_IDs - (Optional) Star IDs to omit in the calibration, primarily meant for variable stars (list/array).\n",
    "    safe - (Optional) Safety step size to avoid overstepping (float).\n",
    "    frac - (Optional) Fraction of stars with number of datapoints above max number of epochs (float).\n",
    "    TEL - Telescope list (list/array)\n",
    "    AGN_ID - AGN ID (float)\n",
    "    \n",
    "    Output:\n",
    "    df - dataframe containing final correction parameters, extra variances, their errors and their corrected magnitudes (pd dataframe)\n",
    "    TRACES - Traces of the correction parameters (arrays)\n",
    "    \"\"\"\n",
    "    # ======================= Select stars to use in calibration ========================\n",
    "    \n",
    "    start_time = time.time()\n",
    "\n",
    "    #Select stars based on fraction of datapoints\n",
    "    Star_IDs = Brightest_Reduced(Star_File, Filter, frac = frac)\n",
    "    \n",
    "    #Further select stars based on variability\n",
    "    if len(bad_IDs) > 0:\n",
    "        Star_IDs = [i for i in Star_IDs if i not in bad_IDs]\n",
    "    \n",
    "    #Max number of stars to use\n",
    "    if Star_Lim != None and len(Star_IDs) > Star_Lim:\n",
    "        Star_IDs = Star_IDs[:Star_Lim]\n",
    "    \n",
    "    #Total number of stars used in calibration\n",
    "    Nstars = len(Star_IDs)\n",
    "    print('No. of Stars: '+ str(Nstars))\n",
    "    \n",
    "    #Select star data based on filter and star IDs\n",
    "    Star_Data = Locate_Star_Filter(Locate_Star_ID(Star_File, Star_IDs), Filter)\n",
    "    \n",
    "    # ====================== Dataframes to return at the end ============================\n",
    "    \n",
    "    #Length of correction parameter dataframe\n",
    "    df_length = len(Star_Data['id_apass'].values)\n",
    "    \n",
    "    #'scope_mean' and 'scope_mean_err' are temporary and will not be returned in the end, just used to speed up computation\n",
    "    df = pd.DataFrame({'id_apass': Star_Data['id_apass'].values, 'Filter': Star_Data['Filter'].values,\n",
    "                       'MJD': Star_Data['MJD'].values, 'telid': Star_Data['telid'].values, \n",
    "                       'scope_mean': df_length*[0.], 'scope_mean_err': df_length*[0.],\n",
    "                       'mag_aper': Star_Data['mag_aper'].values, 'err_aper': Star_Data['err_aper'].values,\n",
    "                       'dmagt': df_length*[0.], 'dmagt_err': df_length*[0.], 'rms_t': df_length*[0.],\n",
    "                       'rms_t_err': df_length*[0.], 'dmags': df_length*[0.], 'dmags_err': df_length*[0.],\n",
    "                       'rms_sc':df_length*[0.], 'rms_sc_err': df_length*[0.], 'm_star': df_length*[0.],\n",
    "                       'm_star_err': df_length*[0.], 'rms_star': df_length*[0.], 'rms_star_err': df_length*[0.],\n",
    "                       'err_tot': Star_Data['err_aper'].values, 'DMAGT': df_length*[0.], 'DMAGS': df_length*[0.],\n",
    "                       'DMAGT_err': df_length*[0.], 'DMAGS_err': df_length*[0.],\n",
    "                       'airmass': Star_Data['airmass'].values, 'seeing': Star_Data['seeing'].values}) \n",
    "    \n",
    "    df.set_index(Star_Data.index.values)\n",
    "    \n",
    "    # ==================================== Traces ====================================\n",
    "    \n",
    "    epochs = []\n",
    "    dmagts = [] ; dmagts_err = []\n",
    "    rms_ts = [] ; rms_ts_err = []\n",
    "    dmagss = [] ; dmagss_err = []\n",
    "    rms_scs = [] ; rms_scs_err = []\n",
    "    mstars = [] ; mstars_err = []\n",
    "    rms_stars = [] ; rms_stars_err = []\n",
    "    DMAGTS = [] ; DMAGTS_err = []\n",
    "    DMAGSS = [] ; DMAGSS_err = []\n",
    "    \n",
    "    # ============================== Intercalibration =================================\n",
    "    \n",
    "    star_mag_correction = 0.\n",
    "    loop = 0\n",
    "    \n",
    "    while loop < MAX_LOOPS:\n",
    "        # ---------------------------------  1.dmagt  -------------------------------\n",
    "        if __name__ == \"__main__\":\n",
    "            df = process_dataframe_parallel2(df, loop, safe, Star_IDs)\n",
    " \n",
    "        # ---------------------------------- 2.dmags --------------------------------\n",
    "        #Compute temporary mean of star using optimal average\n",
    "        for ID in Star_IDs:\n",
    "            \n",
    "            #Select data for particular telescope\n",
    "            Star_Data_S = Locate_Star_ID(df, ID)\n",
    "            Mag = Star_Data_S['mag_aper'].values\n",
    "            Err = Star_Data_S['err_tot'].values\n",
    "            Mean, Mean_err = Generic_Optimal(Mag, Err)\n",
    "            \n",
    "            #Update dataframe\n",
    "            df['m_star'] = np.where((df['id_apass'] == ID), Mean, df['m_star'])\n",
    "            df['m_star_err'] = np.where((df['id_apass'] == ID), Mean_err, df['m_star_err'])\n",
    "        \n",
    "        if __name__ == \"__main__\":\n",
    "            df = process_dataframe_parallel3(df, loop, safe, Star_IDs)\n",
    "            \n",
    "        \n",
    "        dat = df\n",
    "        initial_DMAGS = (dat.DMAGS + dat.dmags).values\n",
    "        DMAGS_mean, DMAGS_mean_err = Generic_Optimal(initial_DMAGS, dat['dmags_err'].values)\n",
    "        Corr1 = dat.dmags - DMAGS_mean\n",
    "        \n",
    "        #Propagate error in new dmagt accordingly\n",
    "        CorrE = (dat.dmags_err**2 + DMAGS_mean_err**2)**0.5\n",
    "        \n",
    "        #Update dataframe\n",
    "        df.loc[Corr1.index, 'dmags'] = Corr1\n",
    "        df.loc[CorrE.index, 'dmags_err'] = CorrE\n",
    "        df.loc[CorrE.index, 'DMAGS_err'] = CorrE\n",
    "        \n",
    "        #Updated dataframe\n",
    "        dat = df\n",
    "        Corr2 = dat.DMAGS + dat.dmags\n",
    "        \n",
    "        #Update dataframe\n",
    "        df.loc[Corr2.index, 'DMAGS'] = Corr2\n",
    "        \n",
    "        # ---------------------------------- 3.m(*) --------------------------------\n",
    "        \n",
    "        if __name__ == \"__main__\":\n",
    "            df = process_dataframe_parallel4(df, loop, safe)\n",
    "        \n",
    "        first_star_corr = Locate_Star_ID(df, Star_IDs[0])['m_star'].values[0]\n",
    "        df['mag_aper'] = df['mag_aper'].values - first_star_corr\n",
    "        df['m_star'] = df['m_star'].values - first_star_corr\n",
    "\n",
    "        #Keep track of how much the data shifts in total\n",
    "        star_mag_correction = star_mag_correction + first_star_corr\n",
    "        \n",
    "        #Update traces\n",
    "        epochs_temp = []\n",
    "        dmagts_temp = [] ; dmagts_err_temp = []\n",
    "        rms_t_temp = [] ; rms_t_err_temp = []\n",
    "        DMAGTS_temp = [] ; DMAGTS_err_temp = []\n",
    "        dmagss_temp = [] ; dmagss_err_temp = []\n",
    "        DMAGSS_temp = [] ; DMAGSS_err_temp = []\n",
    "        rms_sc_temp = [] ; rms_sc_err_temp = []\n",
    "        \n",
    "        for Telescope in TEL:\n",
    "            Star_Data_T = Locate_Star_Scope(df, Telescope)\n",
    "            dat = Star_Data_T.sort_values('MJD')\n",
    "            \n",
    "            unique_epochs = pd.unique(dat['MJD'])\n",
    "            all_epochs = dat['MJD'].values\n",
    "            unique_epoch_ind = np.array([list(all_epochs).index(e) for e in unique_epochs])\n",
    "            \n",
    "            epochs_temp.append(dat['MJD'].values[unique_epoch_ind])\n",
    "            dmagts_temp.append(dat['dmagt'].values[unique_epoch_ind]) ; dmagts_err_temp.append(dat['dmagt_err'].values[unique_epoch_ind])\n",
    "            rms_t_temp.append(dat['rms_t'].values[unique_epoch_ind]) ; rms_t_err_temp.append(dat['rms_t_err'].values[unique_epoch_ind])\n",
    "            DMAGTS_temp.append(dat['DMAGT'].values[unique_epoch_ind]) ; DMAGTS_err_temp.append(dat['DMAGT_err'].values[unique_epoch_ind])\n",
    "            \n",
    "\n",
    "            dmagss_temp.append(dat['dmags'].values[0]) ; dmagss_err_temp.append(dat['dmags_err'].values[0])\n",
    "            DMAGSS_temp.append(dat['DMAGS'].values[0]) ; DMAGSS_err_temp.append(dat['DMAGS_err'].values[0])\n",
    "            rms_sc_temp.append(dat['rms_sc'].values[0]) ; rms_sc_err_temp.append(dat['rms_sc_err'].values[0])\n",
    "            \n",
    "        epochs.append(np.array(epochs_temp, dtype = object))\n",
    "        dmagts.append(np.array(dmagts_temp, dtype = object)) ; dmagts_err.append(np.array(dmagts_err_temp, dtype = object))\n",
    "        rms_ts.append(np.array(rms_t_temp, dtype = object)) ; rms_ts_err.append(np.array(rms_t_err_temp, dtype = object))\n",
    "        DMAGTS.append(np.array(DMAGTS_temp, dtype = object)) ; DMAGTS_err.append(np.array(DMAGTS_err_temp, dtype = object))\n",
    "        dmagss.append(np.array(dmagss_temp, dtype = object)) ; dmagss_err.append(np.array(dmagss_err_temp, dtype = object))\n",
    "        DMAGSS.append(np.array(DMAGSS_temp, dtype = object)) ; DMAGSS_err.append(np.array(DMAGSS_err_temp, dtype = object))\n",
    "        rms_scs.append(np.array(rms_sc_temp, dtype = object)) ; rms_scs_err.append(np.array(rms_sc_err_temp, dtype = object))\n",
    "        \n",
    "        mstars_temp = [] ; mstars_err_temp = []\n",
    "        rms_stars_temp = [] ; rms_stars_err_temp = []\n",
    "        for ID in Star_IDs:\n",
    "            dat = Locate_Star_ID(df, ID)\n",
    "            mstars_temp.append(dat['m_star'].values[0]) ; mstars_err_temp.append(dat['m_star_err'].values[0])\n",
    "            rms_stars_temp.append(dat['rms_star'].values[0]) ; rms_stars_err_temp.append(dat['rms_star_err'].values[0])\n",
    "        mstars.append(np.array(mstars_temp)) ; mstars_err.append(np.array(mstars_err_temp))\n",
    "        rms_stars.append(np.array(rms_stars_temp)) ; rms_stars_err.append(np.array(rms_stars_err_temp))\n",
    "\n",
    "        TRACES = [dmagts, dmagts_err, rms_ts, rms_ts_err, DMAGTS, DMAGTS_err, dmagss, dmagss_err, rms_scs, rms_scs_err, mstars, mstars_err, rms_stars, rms_stars_err, DMAGSS, DMAGSS_err]\n",
    "\n",
    "        #Check convergence\n",
    "        tiny = 1*10**(-4)\n",
    "        All_par_diff = []\n",
    "        if loop > 0:\n",
    "            for i in np.arange(0, len(TRACES) - 1, 2):\n",
    "                tr = TRACES[i]\n",
    "                tr_err = TRACES[i+1]\n",
    "                try:\n",
    "                    All_par_diff.extend((abs((np.concatenate(tr[-1]).ravel()) - (np.concatenate(tr[-2]).ravel())) / np.concatenate(tr_err[-1]).ravel()))\n",
    "                except:\n",
    "                    All_par_diff.extend((abs(np.array(tr[-1]) - np.array(tr[-2])) / np.array(tr_err[-1])))\n",
    "            #print(max(All_par_diff))\n",
    "            if max(All_par_diff) < tiny:\n",
    "                print('Iteration ' + str(loop + 1) + '/' + str(MAX_LOOPS) + ' Finished')\n",
    "                df['m_star'] = df['m_star'] + star_mag_correction\n",
    "                df['mag_aper'] = df['mag_aper'] + star_mag_correction\n",
    "                print('Total run time:', time.time() - start_time)\n",
    "                break\n",
    "        print('Iteration ' + str(loop + 1) + '/' + str(MAX_LOOPS) + ' Finished')\n",
    "        loop = loop + 1\n",
    "    if loop == MAX_LOOPS:\n",
    "        df['m_star'] = df['m_star'] + star_mag_correction\n",
    "        df['mag_aper'] = df['mag_aper'] + star_mag_correction\n",
    "        print('Total run time:', time.time() - start_time)\n",
    "    return(df, TRACES)\n",
    "\n",
    "def dmagt(object_data, loop, safe, Star_IDs):\n",
    "    object_data = object_data\n",
    "    #Compute avg and sig(avg) magnitude of the telescope for each star using optimal average\n",
    "    for ID in Star_IDs:\n",
    "        #Select data for particular star\n",
    "        Star_Data_S = Locate_Star_ID(object_data, ID)\n",
    "        Mag = Star_Data_S['mag_aper'].values\n",
    "        Err = Star_Data_S['err_tot'].values\n",
    "        Mean_Scope_Mag, Mean_Scope_Mag_Err = Generic_Optimal(Mag, Err)\n",
    "        \n",
    "        #Update dataframe\n",
    "        object_data['scope_mean'] = np.where((object_data['id_apass'] == ID), Mean_Scope_Mag, object_data['scope_mean'])\n",
    "        object_data['scope_mean_err'] = np.where((object_data['id_apass'] == ID), Mean_Scope_Mag_Err, object_data['scope_mean_err'])\n",
    "    \n",
    "    Unique_MJDs = pd.unique(object_data['MJD'])     \n",
    "    Telescope = object_data.telid.values[0]\n",
    "    if __name__ == \"__main__\":\n",
    "        # Process the DataFrame in parallel\n",
    "        result_df = process_dataframe_parallel(object_data, loop, safe, Telescope)\n",
    "\n",
    "    #Ensure average of the dmagt's is zero to avoid degeneracy\n",
    "    Star_Data_T = result_df\n",
    "    initial_DMAGT = Star_Data_T['DMAGT'].values + Star_Data_T['dmagt'].values\n",
    "    DMAGT_mean, DMAGT_mean_err = Generic_Optimal(initial_DMAGT, Star_Data_T['dmagt_err'].values)\n",
    "    Corr1 = Star_Data_T.dmagt - DMAGT_mean\n",
    "    \n",
    "    #Propagate error in new dmagt accordingly\n",
    "    CorrE = (Star_Data_T.dmagt_err**2 + DMAGT_mean_err**2)**0.5\n",
    "    \n",
    "    #Update dataframe\n",
    "    result_df.loc[Corr1.index, 'dmagt'] = Corr1\n",
    "    result_df.loc[CorrE.index, 'dmagt_err'] = CorrE\n",
    "    result_df.loc[CorrE.index, 'DMAGT_err'] = CorrE\n",
    "    \n",
    "    #Updated dataframe\n",
    "    Star_Data_T = result_df\n",
    "    Corr2 = Star_Data_T.dmagt + Star_Data_T.DMAGT\n",
    "    \n",
    "    #Update dataframe\n",
    "    result_df.loc[Corr2.index, 'DMAGT'] = Corr2\n",
    "    \n",
    "    return(result_df)\n",
    "    \n",
    "def dmags(object_data, loop, safe, Star_IDs):\n",
    "    #Select data for particular telescope\n",
    "    object_data = object_data\n",
    "    Shifted_Mags = (object_data.mag_aper - object_data.dmagt - object_data.m_star).values\n",
    "    \n",
    "    #Propagate errors accordingly\n",
    "    Shifted_Mags_Err = ((object_data.err_tot**2 + object_data.dmagt_err**2 + object_data.m_star_err**2)**0.5).values\n",
    "    \n",
    "    #Compute correction parameters for that telescope\n",
    "    dmags, dmags_err, rms_sc, rms_sc_err = arx.arx(Shifted_Mags, Shifted_Mags_Err, 1000)\n",
    "    Telescope = object_data.telid.values[0]\n",
    "    #Take safe size step to prevent overstepping\n",
    "    if loop > 0:\n",
    "        #old_dmags = df.loc[df['telid'] == TEL[i]]['dmags'].values[0]\n",
    "        old_rms_sc = object_data['rms_sc'].values[0]\n",
    "    \n",
    "        #dmags = old_dmags + safe*(dmags - old_dmags)\n",
    "        rms_sc = old_rms_sc + safe*(rms_sc - old_rms_sc)\n",
    "    \n",
    "    #Update dataframe\n",
    "    object_data['dmags'] = np.where((object_data['telid'] == Telescope), dmags, object_data['dmags'])\n",
    "    object_data['dmags_err'] = np.where((object_data['telid'] == Telescope), dmags_err, object_data['dmags_err'])\n",
    "    object_data['rms_sc'] = np.where((object_data['telid'] == Telescope), rms_sc, object_data['rms_sc'])             \n",
    "    object_data['rms_sc_err'] = np.where((object_data['telid'] == Telescope), rms_sc_err, object_data['rms_sc_err'])\n",
    "    object_data['DMAGS_err'] = np.where((object_data['telid'] == Telescope), dmags_err, object_data['DMAGS_err'])\n",
    "    \n",
    "    #Update total error of datapoint with new sc_rms\n",
    "    err_tot = (object_data.err_aper**2 + object_data.rms_star**2 + object_data.rms_t**2 + rms_sc**2)**0.5\n",
    "    \n",
    "    #Update dataframe\n",
    "    object_data.loc[err_tot.index, 'err_tot'] = err_tot\n",
    "    \n",
    "    return object_data\n",
    "\n",
    "def star_mags(object_data, loop, safe):\n",
    "    object_data = object_data\n",
    "    \n",
    "    #Apply correction parameters\n",
    "    Shifted_Mag = object_data['mag_aper'].values - object_data['dmagt'].values - object_data['dmags'].values\n",
    "    \n",
    "    #Propagate errors accordingly\n",
    "    Shifted_Mag_Err = (object_data['err_tot'].values**2 + object_data['dmagt_err'].values**2 + object_data['dmags_err'].values**2)**0.5\n",
    "    \n",
    "    #Compute mean of star\n",
    "    m_star, m_star_err, rms_star, rms_star_err = arx.arx(Shifted_Mag, Shifted_Mag_Err, 1000)\n",
    "    \n",
    "    #Take safe size step to prevent overstepping\n",
    "    if loop > 0:\n",
    "        old_mstar = object_data['m_star'].values[0]\n",
    "        old_rms_star = object_data['rms_star'].values[0]\n",
    "        \n",
    "        m_star = old_mstar + safe*(m_star - old_mstar)\n",
    "        rms_star = old_rms_star + safe*(rms_star - old_rms_star)\n",
    "    \n",
    "    #Update dataframe\n",
    "    object_data['m_star'] = m_star\n",
    "    object_data['m_star_err'] = m_star_err\n",
    "    object_data['rms_star'] = rms_star           \n",
    "    object_data['rms_star_err'] = rms_star_err\n",
    "    \n",
    "    #Update total error of datapoint with new star_rms\n",
    "    err_tot = (object_data.err_aper**2 + object_data.rms_sc**2 + object_data.rms_t**2 + rms_star**2)**0.5\n",
    "    Shift = object_data.mag_aper - object_data.dmagt - object_data.dmags\n",
    "    \n",
    "    #Update dataframe\n",
    "    object_data.loc[err_tot.index, 'err_tot'] = err_tot\n",
    "    object_data.loc[Shift.index, 'mag_aper'] = Shift\n",
    "    \n",
    "    return object_data\n",
    "def process_object_data(object_data, loop, safe, Telescope):\n",
    "    object_data = object_data\n",
    "    #Shift mags by telescope means\n",
    "    Shifted_Mags = (object_data.mag_aper - object_data.scope_mean).values\n",
    "\n",
    "    #Propagate errors accordingly\n",
    "    Shifted_Mags_Err = ((object_data.err_tot**2 + object_data.scope_mean_err**2)**0.5).values\n",
    "\n",
    "    #Compute correction parameters for that MJD\n",
    "    dmagt, dmagt_err, rms_t, rms_t_err = arx.arx(Shifted_Mags, Shifted_Mags_Err, 1000)\n",
    "    \n",
    "    #Take safe size step to prevent overstepping\n",
    "    if loop > 0:\n",
    "        old_dmagt = object_data['dmagt'].values[0]\n",
    "        old_rms_t = object_data['rms_t'].values[0]\n",
    "        \n",
    "        dmagt = old_dmagt + safe*(dmagt - old_dmagt)\n",
    "        rms_t = old_rms_t + safe*(rms_t - old_rms_t)\n",
    "    \n",
    "    E = object_data.MJD.values[0]\n",
    "    #Update dataframe\n",
    "    object_data['dmagt'] = np.where((object_data['MJD'] == E), dmagt, object_data['dmagt'])\n",
    "    object_data['dmagt_err'] = np.where((object_data['MJD'] == E), dmagt_err, object_data['dmagt_err'])\n",
    "    object_data['rms_t'] = np.where((object_data['MJD'] == E), rms_t, object_data['rms_t'])\n",
    "    object_data['rms_t_err'] = np.where((object_data['MJD'] == E), rms_t_err, object_data['rms_t_err'])\n",
    "\n",
    "    #Updated dataframe\n",
    "    Star_Data_T = Locate_Star_Scope(object_data, Telescope)\n",
    "    Star_Data_mjd = Locate_Star_Epoch(Star_Data_T, E)\n",
    "    \n",
    "    #Update total error of datapoint with new t_rms\n",
    "    err_tot = (Star_Data_mjd.err_aper**2 + Star_Data_mjd.rms_sc**2 + Star_Data_mjd.rms_star**2 + rms_t**2)**0.5\n",
    "    \n",
    "    #Update dataframe\n",
    "    object_data.loc[err_tot.index, 'err_tot'] = err_tot\n",
    "    \n",
    "    return object_data\n",
    "\n",
    "def process_dataframe_parallel(df, loop, safe, Telescope, num_workers=4):\n",
    "    # Group the DataFrame by the 'MJD' column\n",
    "    grouped_data = df.groupby('MJD')\n",
    "\n",
    "    # Prepare the input for parallel processing\n",
    "    object_data_list = [object_subset for _, object_subset in grouped_data]\n",
    "\n",
    "    # Use ThreadPoolExecutor to parallelize the processing\n",
    "    with ThreadPoolExecutor(max_workers=num_workers) as executor:\n",
    "        futures = {executor.submit(process_object_data, object_subset, loop, safe, Telescope): object_subset for object_subset in object_data_list}\n",
    "\n",
    "    # Collect the results and concatenate into a single DataFrame\n",
    "    results = [future.result() for future in as_completed(futures)]\n",
    "    result_df = pd.concat(results, ignore_index=True)\n",
    "\n",
    "    result_df = pd.concat(results, ignore_index=True)\n",
    "    return result_df\n",
    "\n",
    "\n",
    "def process_dataframe_parallel2(df, loop, safe, Star_IDs, num_workers=4):\n",
    "    # Group the DataFrame by the 'telid' column\n",
    "    grouped_data = df.groupby('telid')\n",
    "\n",
    "    # Prepare the input for parallel processing\n",
    "    object_data_list = [object_subset for _, object_subset in grouped_data]\n",
    "\n",
    "    # Use ThreadPoolExecutor to parallelize the processing\n",
    "    with ThreadPoolExecutor(max_workers=num_workers) as executor:\n",
    "        futures = {executor.submit(dmagt, object_subset, loop, safe, Star_IDs): object_subset for object_subset in object_data_list}\n",
    "\n",
    "    # Collect the results and concatenate into a single DataFrame\n",
    "    results = [future.result() for future in as_completed(futures)]\n",
    "    result_df = pd.concat(results, ignore_index=True)\n",
    "\n",
    "    return result_df\n",
    "\n",
    "def process_dataframe_parallel3(df, loop, safe, Star_IDs, num_workers=4):\n",
    "    # Group the DataFrame by the 'telid' column\n",
    "    grouped_data = df.groupby('telid')\n",
    "\n",
    "    # Prepare the input for parallel processing\n",
    "    object_data_list = [object_subset for _, object_subset in grouped_data]\n",
    "\n",
    "    # Use ThreadPoolExecutor to parallelize the processing\n",
    "    with ThreadPoolExecutor(max_workers=num_workers) as executor:\n",
    "        futures = {executor.submit(dmags, object_subset, loop, safe, Star_IDs): object_subset for object_subset in object_data_list}\n",
    "\n",
    "    # Collect the results and concatenate into a single DataFrame\n",
    "    results = [future.result() for future in as_completed(futures)]\n",
    "    result_df = pd.concat(results, ignore_index=True)\n",
    "\n",
    "    return result_df\n",
    "\n",
    "def process_dataframe_parallel4(df, loop, safe, num_workers=4):\n",
    "    # Group the DataFrame by the 'telid' column\n",
    "    grouped_data = df.groupby('id_apass')\n",
    "\n",
    "    # Prepare the input for parallel processing\n",
    "    object_data_list = [object_subset for _, object_subset in grouped_data]\n",
    "\n",
    "    # Use ThreadPoolExecutor to parallelize the processing\n",
    "    with ThreadPoolExecutor(max_workers=num_workers) as executor:\n",
    "        futures = {executor.submit(star_mags, object_subset, loop, safe): object_subset for object_subset in object_data_list}\n",
    "\n",
    "    # Collect the results and concatenate into a single DataFrame\n",
    "    results = [future.result() for future in as_completed(futures)]\n",
    "    result_df = pd.concat(results, ignore_index=True)\n",
    "\n",
    "    return result_df\n",
    "\n",
    "#----------------\n",
    "#Absolute photometric calibration functions\n",
    "def Phot_Cal(DF,filt,catalogue='apass_updated.csv', AGN_ID=AGN_ID):\n",
    "    df = {}\n",
    "    apass = pd.read_csv(catalogue)\n",
    "    for i,idx in enumerate(np.unique(DF['id_apass'])[:]):\n",
    "        #print(idx)\n",
    "        df_temp = Locate_Star_ID(DF,idx)\n",
    "        #Apply correction parameters\n",
    "        mags = df_temp['mag_aper'].values \n",
    "        #plt.plot(mags,'ko')\n",
    "        errs = (df_temp['err_aper'].values**2 + df_temp['rms_sc'].values**2 + df_temp['rms_t'].values**2)**0.5\n",
    "        mag,err = Generic_Optimal(mags,errs)\n",
    "\n",
    "        ss = apass['id'] == idx\n",
    "        #print(ss.sum())\n",
    "        df[i] = {\n",
    "                'id_apass':idx,\n",
    "                'mag':df_temp['mag_aper'].values[0],\n",
    "                'err':err,\n",
    "                'mag_lco': apass[filt[:1]+'_lco'].values[ss][0],\n",
    "                'err_lco': apass[filt[:1]+'err_lco'].values[ss][0]\n",
    "                }\n",
    "        #print(mag,df_temp['mag_aper'].values[0],apass[filt[:1]+'_lco'].values[ss])\n",
    "\n",
    "    zps = pd.DataFrame.from_dict(df,\"index\")\n",
    "    \n",
    "    from astropy.stats import sigma_clip\n",
    "\n",
    "    diffs =  zps['mag_lco'].values - zps['mag'].values\n",
    "    diffs_err = np.sqrt(zps['err'].values**2 + zps['err_lco'].values**2)\n",
    "    pp = sigma_clip(diffs, sigma=3)\n",
    "\n",
    "    zp,aa,zp_err,ee = arx.arx(diffs[~pp.mask],diffs_err[~pp.mask])\n",
    "    \n",
    "    return zp, zp_err"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Files and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------- Import star data in pickle format -------\n",
    "lco = pd.read_pickle('lco_latest_stan.pkl')\n",
    "\n",
    "# -------------- Obs info --------------\n",
    "#AGN Name\n",
    "obj_name = 'Mrk_841'\n",
    "\n",
    "#FILTERS = pd.unique(lco['Filter'])\n",
    "FILTERS = ['gp', 'V', 'rp', 'ip', 'zs'] #in order\n",
    "print('FILTERS:', FILTERS)\n",
    "\n",
    "TEL = list(pd.unique(lco['telid']))\n",
    "print('SCOPES:', TEL)\n",
    "\n",
    "AGN_ID = 1017"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------- Clean up data file --------------\n",
    "lco2=Clean(lco)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intercalibration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Intercalibrate\n",
    "DF_gp, TR_gp = Corr(lco2, 'gp', MAX_LOOPS = 100, bad_IDs = [], safe = 0.6, frac = 0.5, TEL = TEL, AGN_ID = AGN_ID, Star_Lim = 100)\n",
    "DF_V, TR_V = Corr(lco2, 'V', MAX_LOOPS = 100, bad_IDs = [], safe = 0.6, frac = 0.5, TEL = TEL, AGN_ID = AGN_ID, Star_Lim = 100)\n",
    "DF_rp, TR_rp = Corr(lco2, 'rp', MAX_LOOPS = 100, bad_IDs = [], safe = 0.6, frac = 0.5, TEL = TEL, AGN_ID = AGN_ID, Star_Lim = 100)\n",
    "DF_ip, TR_ip = Corr(lco2, 'ip', MAX_LOOPS = 100, bad_IDs = [], safe = 0.6, frac = 0.5, TEL = TEL, AGN_ID = AGN_ID, Star_Lim = 100)\n",
    "DF_zs, TR_zs = Corr(lco2, 'zs', MAX_LOOPS = 100, bad_IDs = [], safe = 0.6, frac = 0.5, TEL = TEL, AGN_ID = AGN_ID, Star_Lim = 100)\n",
    "\n",
    "#Combine all into one dataframe\n",
    "All_DF = [DF_gp, DF_V, DF_rp, DF_ip, DF_zs]\n",
    "CALIBRATED_STARS = pd.concat(All_DF, ignore_index = True)\n",
    "#Save as csv file\n",
    "CALIBRATED_STARS.to_csv(obj_name +'_Calibrated_Stars'+'.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load csv file\n",
    "CALIBRATED_STARS = pd.read_csv(obj_name +'_Calibrated_Stars'+'.csv')\n",
    "\n",
    "DF_gp = Locate_Star_Filter(CALIBRATED_STARS, 'gp')\n",
    "DF_V = Locate_Star_Filter(CALIBRATED_STARS, 'V')\n",
    "DF_rp = Locate_Star_Filter(CALIBRATED_STARS, 'rp')\n",
    "DF_ip = Locate_Star_Filter(CALIBRATED_STARS, 'ip')\n",
    "DF_zs = Locate_Star_Filter(CALIBRATED_STARS, 'zs')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zeropoint calibration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ZP_gp = Phot_Cal(DF_gp, 'gp', catalogue='apass_updated.csv')\n",
    "ZP_V = Phot_Cal(DF_V, 'V', catalogue='apass_updated.csv')\n",
    "ZP_rp = Phot_Cal(DF_rp, 'rp', catalogue='apass_updated.csv')\n",
    "ZP_ip = Phot_Cal(DF_ip, 'ip', catalogue='apass_updated.csv')\n",
    "ZP_zs = Phot_Cal(DF_zs, 'zs', catalogue='apass_updated.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AGN light curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AGN_DF_gp = AGN_LC(lco2, DF_gp, 'gp', err_th = 0.03, Plot = True, Rem_out = True, zp=ZP_gp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AGN_DF_V = AGN_LC(lco2, DF_V, 'V', err_th = 0.03, Plot = True, Rem_out = True, zp=ZP_V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AGN_DF_rp = AGN_LC(lco2, DF_rp, 'rp', err_th = 0.03, Plot = True, Rem_out = True, zp=ZP_rp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AGN_DF_ip = AGN_LC(lco2, DF_ip, 'ip', err_th = 0.03, Plot = True, Rem_out = True, zp=ZP_ip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AGN_DF_zs = AGN_LC(lco2, DF_zs, 'zs', err_th = 0.03, Plot = True, Rem_out = True, zp=ZP_zs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Combine all AGN data into one dataframe\n",
    "AGN = pd.concat([AGN_DF_gp, AGN_DF_V, AGN_DF_rp, AGN_DF_ip, AGN_DF_zs], ignore_index=True)\n",
    "#Saving to file\n",
    "AGN.to_csv(obj_name +'_AGN'+'.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating plots\n",
    "for F, dff in zip(FILTERS, [DF_gp, DF_V, DF_rp, DF_ip, DF_zs])\n",
    "    AGN_LC2(lco2, dff, fr'{F}', err_th = 0.03, Plot = True, Rem_out = False, zp=ZP_gp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyCCF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load csv file\n",
    "AGN_STARS = pd.read_csv(obj_name +'_AGN'+'.csv')\n",
    "UVOT = pd.read_csv('Mrk841_UVOT.csv')\n",
    "\n",
    "#Combining files\n",
    "DF_AGN_W2 = UVOT.loc[UVOT['FILTER'] == \"UVW2\"] \n",
    "DF_AGN_W2 = DF_AGN_W2.drop(columns=[\"OBJECT\", \"MJD_ERR\", \"HJD\", \"OBSDATE\", \"OBSTIME\", \"OBSID\", \"EXTENSION\", \"FLUX_AA_5\", \"FLUX_AA_5_ERR_SIG5\"])\n",
    "DF_AGN_W2 = DF_AGN_W2.rename(columns={\"FILTER\":\"Filter\", \"FLUX_HZ_5\":\"flux\", \"FLUX_HZ_5_ERR_SIG7P5\":\"err\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Conversions\n",
    "def Jyconvert(mag):\n",
    "    return 10**(23-0.4*(mag+48.6)) * 1e3\n",
    "def errorconvert(err, flux):\n",
    "    return flux*err*np.log(10)/2.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Converting LCO database units to Swift units\n",
    "def converter(database):\n",
    "    columns = ['MJD', 'Filter', 'flux', 'err']\n",
    "    dataframe = pd.DataFrame(columns=columns)\n",
    "    mjd1, flux1, err1, filter1 =  database['MJD'].to_numpy(), database['mag'].to_numpy(), database['err'].to_numpy(), database['Filter'].to_numpy()\n",
    "    \n",
    "    fluxjy = Jyconvert(flux1)\n",
    "    # fluxjy = flux1\n",
    "    errjy = errorconvert(err1, fluxjy)\n",
    "    # errjy = err1\n",
    "\n",
    "    temp = {'MJD': mjd1, 'Filter': filter1, 'flux': fluxjy, 'err': errjy}\n",
    "    temp_DF = pd.DataFrame(temp)\n",
    "    dataframe = pd.concat([dataframe, temp_DF], ignore_index=True)\n",
    "    return dataframe\n",
    "\n",
    "NEW_STARS = converter(AGN_STARS)\n",
    "ALLSTARS = pd.concat([NEW_STARS, DF_AGN_W2], ignore_index=True)\n",
    "#Saving to file\n",
    "ALLSTARS.to_csv(obj_name +'_ALLSTARS'+'.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Converting to format usable in PyROA\n",
    "def pyroadf2(database):\n",
    "    FILTERS = ['gp', 'V', 'rp', 'ip', 'zs', 'UVW2'] #in order\n",
    "    DF_LIST = []\n",
    "    columns = ['MJD', 'flux', 'err']\n",
    "    for i in range(len(FILTERS)):\n",
    "        dataframe = pd.DataFrame(columns=columns)\n",
    "        \n",
    "        f1 = FILTERS[i]\n",
    "        # print(f1)\n",
    "        lc1 = Locate_Star_Filter(database, str(f1))\n",
    "        # display(datum)\n",
    "        mjd1, flux1, err1 =  lc1['MJD'].to_numpy(), lc1['flux'].to_numpy(), lc1['err'].to_numpy()\n",
    "        # fluxjy = Jyconvert(flux1)\n",
    "        # fluxjy = flux1\n",
    "        # errjy = errorconvert(err1, fluxjy)\n",
    "        # errjy = err1\n",
    "        \n",
    "        temp = {'MJD': mjd1, 'flux': flux1, 'err': err1}\n",
    "        temp_DF = pd.DataFrame(temp)\n",
    "        dataframe = pd.concat([dataframe, temp_DF], ignore_index=True)\n",
    "        \n",
    "        DF_LIST.append(dataframe)\n",
    "    return DF_LIST\n",
    "\n",
    "DF2_LIST = pyroadf2(ALLSTARS)\n",
    "\n",
    "for i in range(len(DF2_LIST)):\n",
    "    FILTERS = ['gp', 'V', 'rp', 'ip', 'zs', 'UVW2'] #in order\n",
    "    df = DF2_LIST[i]\n",
    "    Filter = FILTERS[i]\n",
    "    df.to_csv(fr\"/home/jovyan/PyROAF/{obj_name}_{FILTERS[i]}.dat\", header=None, index=None, sep=\" \")\n",
    "    df.to_csv(fr\"/home/jovyan/PyROAF/{obj_name}_{FILTERS[i]}1.dat\", header=None, index=None, sep=\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using PyCCF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adapted from sample_runcode.py script accessible at http://ascl.net/code/v/1868\n",
    "# Reference: Peterson et al. 1998 (https://arxiv.org/abs/astro-ph/9802103)\n",
    "def pyccfer_flux(database):\n",
    "    \n",
    "    start_time = time.time()\n",
    "    columns = ['Centroid', 'Centroid upper error', 'Centroid lower error', 'Peak', 'Peak upper error', 'Peak lower error']\n",
    "    df = pd.DataFrame(columns=columns)\n",
    "    database = database.sort_values(by=['MJD'])\n",
    "    FILTERS = [\"gp\", \"V\", \"rp\", \"ip\", \"zs\", \"UVW2\"]\n",
    "    \n",
    "    for j in range(2):\n",
    "        \n",
    "        print(j)\n",
    "        \n",
    "        if j == 0:\n",
    "            ref_date1 = 57478\n",
    "            ref_date2 = 57664\n",
    "        \n",
    "        if j == 1:\n",
    "            ref_date1 = 57700\n",
    "            ref_date2 = 10**6\n",
    "        \n",
    "        databased = database.loc[(database['MJD'] > ref_date1) & (database['MJD'] < ref_date2)]\n",
    "        \n",
    "        for i in range(len(FILTERS)):\n",
    "            f1 = FILTERS[0]\n",
    "            f2 = FILTERS[i]\n",
    "\n",
    "            print(fr'Compared filters: {f1} and {f2}')\n",
    "\n",
    "            lc1 = Locate_Star_Filter(databased, str(f1))\n",
    "            lc2 = Locate_Star_Filter(databased, str(f2))\n",
    "\n",
    "            mjd1, flux1, err1 =  lc1['MJD'].to_numpy(), lc1['flux'].to_numpy(), lc1['err'].to_numpy()\n",
    "            mjd2, flux2, err2 =  lc2['MJD'].to_numpy(), lc2['flux'].to_numpy(), lc2['err'].to_numpy()\n",
    "\n",
    "\n",
    "            #########################################\n",
    "            ##Set Interpolation settings, user-specified\n",
    "            #########################################\n",
    "            lag_range = [-20, 20]  #Time lag range to consider in the CCF (days). Must be small enough that there is some overlap between light curves at that shift (i.e., if the light curves span 80 days, these values must be less than 80 days)\n",
    "            interp = 0.5 #Interpolation time step (days). Must be less than the average cadence of the observations, but too small will introduce noise.\n",
    "            nsim = 10000  #Number of Monte Carlo iterations for calculation of uncertainties\n",
    "            mcmode = 0  #Do both FR/RSS sampling (1 = RSS only, 2 = FR only) \n",
    "            sigmode = 0.2  #Choose the threshold for considering a measurement \"significant\". sigmode = 0.2 will consider all CCFs with r_max <= 0.2 as \"failed\". See code for different sigmodes.\n",
    "\n",
    "            ##########################################\n",
    "            #Calculate lag with python CCF program\n",
    "            ##########################################\n",
    "            tlag_peak, status_peak, tlag_centroid, status_centroid, ccf_pack, max_rval, status_rval, pval = myccf.peakcent(mjd1, flux1, mjd2, flux2, lag_range[0], lag_range[1], interp)\n",
    "            tlags_peak, tlags_centroid, nsuccess_peak, nfail_peak, nsuccess_centroid, nfail_centroid, max_rvals, nfail_rvals, pvals = myccf.xcor_mc(mjd1, flux1, abs(err1), mjd2, flux2, abs(err2), lag_range[0], lag_range[1], interp, nsim = nsim, mcmode=mcmode, sigmode = 0.2)\n",
    "\n",
    "            lag = ccf_pack[1]\n",
    "            r = ccf_pack[0]\n",
    "\n",
    "            perclim = 84.1344746    \n",
    "\n",
    "            ###Calculate the best peak and centroid and their uncertainties using the median of the\n",
    "            ##distributions. \n",
    "            centau = stats.scoreatpercentile(tlags_centroid, 50)\n",
    "            centau_uperr = (stats.scoreatpercentile(tlags_centroid, perclim))-centau\n",
    "            centau_loerr = centau-(stats.scoreatpercentile(tlags_centroid, (100.-perclim)))\n",
    "            print('Centroid, error: %10.3f  (+%10.3f -%10.3f)'%(centau, centau_uperr, centau_loerr))\n",
    "\n",
    "            peaktau = stats.scoreatpercentile(tlags_peak, 50)\n",
    "            peaktau_uperr = (stats.scoreatpercentile(tlags_peak, perclim))-centau\n",
    "            peaktau_loerr = centau-(stats.scoreatpercentile(tlags_peak, (100.-perclim)))\n",
    "            print('Peak, errors: %10.3f  (+%10.3f -%10.3f)'%(peaktau, peaktau_uperr, peaktau_loerr))\n",
    "\n",
    "            temp = [{'Centroid':centau, 'Centroid upper error':centau_uperr, 'Centroid lower error':centau_loerr, 'Peak':peaktau, 'Peak upper error':peaktau_uperr, 'Peak lower error':peaktau_loerr, 'CCF':lag, 'Correlation': r, 'Centroid hist': tlags_centroid, 'Peak hist': tlags_peak}]\n",
    "\n",
    "            temp_df = pd.DataFrame(temp)\n",
    "\n",
    "            df = pd.concat([df, temp_df], ignore_index=True)\n",
    "\n",
    "            ##########################################\n",
    "            #Plot the Light curves, CCF, CCCD, and CCPD\n",
    "            ##########################################\n",
    "\n",
    "            fig = plt.figure()\n",
    "            fig.subplots_adjust(hspace=0.2, wspace = 0.1)\n",
    "\n",
    "            #Plot lightcurves\n",
    "            ax1 = fig.add_subplot(3, 1, 1)\n",
    "            ax1.errorbar(mjd1, flux1, yerr = err1, marker = '.', linestyle = ':', color = 'k', label = 'LC 1 (Continuum)')\n",
    "            ax1_2 = fig.add_subplot(3, 1, 2, sharex = ax1)\n",
    "            ax1_2.errorbar(mjd2, flux2, yerr = err2, marker = '.', linestyle = ':', color = 'k', label = 'LC 2 (Emission Line)')\n",
    "\n",
    "            # ax1.text(0.025, 0.825, lc1, fontsize = 15, transform = ax1.transAxes)\n",
    "            # ax1_2.text(0.025, 0.825, lc2, fontsize = 15, transform = ax1_2.transAxes)\n",
    "            ax1.set_ylabel('LC 1 Flux')\n",
    "            ax1_2.set_ylabel('LC 2 Flux')\n",
    "            ax1_2.set_xlabel('MJD')\n",
    "\n",
    "            #Plot CCF Information\n",
    "            xmin, xmax = -99, 99\n",
    "            ax2 = fig.add_subplot(3, 3, 7)\n",
    "            ax2.set_ylabel('CCF r')\n",
    "            ax2.text(0.2, 0.85, 'CCF ', horizontalalignment = 'center', verticalalignment = 'center', transform = ax2.transAxes, fontsize = 16)\n",
    "            ax2.set_xlim(xmin, xmax)\n",
    "            ax2.set_ylim(-1.0, 1.0)\n",
    "            ax2.plot(lag, r, color = 'k')\n",
    "\n",
    "            ax3 = fig.add_subplot(3, 3, 8, sharex = ax2)\n",
    "            ax3.set_xlim(xmin, xmax)\n",
    "            ax3.axes.get_yaxis().set_ticks([])\n",
    "            ax3.set_xlabel('Centroid Lag: %5.1f (+%5.1f -%5.1f) days'%(centau, centau_uperr, centau_loerr), fontsize = 15) \n",
    "            ax3.text(0.2, 0.85, 'CCCD ', horizontalalignment = 'center', verticalalignment = 'center', transform = ax3.transAxes, fontsize = 16)\n",
    "            n, bins, etc = ax3.hist(tlags_centroid, bins = 50, color = 'b')\n",
    "            print(len(tlags_centroid))\n",
    "\n",
    "            ax4 = fig.add_subplot(3, 3, 9, sharex = ax2)\n",
    "            ax4.set_ylabel('N')\n",
    "            ax4.yaxis.tick_right()\n",
    "            ax4.yaxis.set_label_position('right') \n",
    "            #ax4.set_xlabel('Lag (days)')\n",
    "            ax4.set_xlim(xmin, xmax)\n",
    "            ax4.text(0.2, 0.85, 'CCPD ', horizontalalignment = 'center', verticalalignment = 'center', transform = ax4.transAxes, fontsize = 16)\n",
    "            ax4.hist(tlags_peak, bins = bins, color = 'b')\n",
    "\n",
    "            plt.show()\n",
    "        print('Total run time:', time.time() - start_time)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FCCF_DF = pyccfer_flux(ALLSTARS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#File organisation; adding year, waveband error, and waveband wavelength\n",
    "DF_FCCF = FCCF_DF\n",
    "Fwavelength = np.array([4770, 5510, 6231, 7625, 9134, 1894, 4770, 5510, 6231, 7625, 9134, 1894])\n",
    "FFerror = np.array([1262.68/2, 840/2, 1149.52/2, 1238.95/2, 994.39/2, 584.89/2, 1262.68/2, 840/2, 1149.52/2, 1238.95/2, 994.39/2, 584.89/2])\n",
    "Fyear = np.array([2016, 2016, 2016, 2016, 2016, 2016, 2017, 2017, 2017, 2017, 2017, 2017])\n",
    "Fcentroid = DF_FCCF['Centroid']\n",
    "FFILTERS = [\"gp\", \"V\", \"rp\", \"ip\", \"zs\", \"UVW2\"]\n",
    "FFilters = np.tile(FFILTERS, 2)\n",
    "Ftemp = {'Wavelength': Fwavelength, 'Year':Fyear, 'Filter':FFilters, 'Centroid':Fcentroid, 'Filter error':FFerror}\n",
    "Ftemp_DF = pd.DataFrame(Ftemp, index=DF_FCCF.index)\n",
    "DF_FCCF = pd.merge(DF_FCCF, Ftemp_DF, on='Centroid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DF_FCCF.to_pickle(obj_name+'_FPyCCF'+'.pkl')\n",
    "DF_FCCF = pd.read_pickle(obj_name+'_FPyCCF'+'.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyROA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing files, data, making fits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fit was completed six times, each with a different Delta and saved to different folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj_name = 'Mrk_841'\n",
    "FILTERS = ['UVW2','gp1', 'gp', 'V', 'rp', 'ip', 'zs']\n",
    "datadir = \"/home/jovyan/PyROAF/\"\n",
    "filters = FILTERS\n",
    "init_tau = [-3, 0, 0.7, 3.0, 3.5, 4.5]\n",
    "priors = [[0.5, 2.0],[0.5, 2.0], [-50.0, 50.0], [0.5, 10.0], [0.0, 10.0]]\n",
    "fit = PyROA.Fit(datadir, obj_name, filters, priors, add_var=True, init_tau = init_tau, Nsamples=70000, Nburnin=15000, use_backend=True, delay_ref='gp1') \n",
    "fit = PyROA.Fit(datadir, obj_name, filters, priors, add_var=True,Nsamples=30000, Nburnin=0,use_backend=True, resume_progress=True, delay_ref='gp1')\n",
    "outputdir = \"/home/jovyan/PyROAF/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj_name = 'Mrk_841'\n",
    "FILTERS = ['UVW2','gp1', 'gp', 'V', 'rp', 'ip', 'zs']\n",
    "datadir = \"/home/jovyan/PyROA2/\"\n",
    "filters = FILTERS\n",
    "init_tau = [-3, 0, 0.7, 3.0, 3.5, 4.5]\n",
    "priors = [[0.5, 2.0],[0.5, 2.0], [-50.0, 50.0], [1.99, 2.01], [0.0, 10.0]]\n",
    "fit = PyROA.Fit(datadir, obj_name, filters, priors, add_var=True, init_tau = init_tau, Nsamples=70000, Nburnin=15000, use_backend=True, delay_ref='gp1') \n",
    "fit = PyROA.Fit(datadir, obj_name, filters, priors, add_var=True,Nsamples=30000, Nburnin=0,use_backend=True, resume_progress=True, delay_ref='gp1')\n",
    "outputdir = \"/home/jovyan/PyROA2/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj_name = 'Mrk_841'\n",
    "FILTERS = ['UVW2','gp1', 'gp', 'V', 'rp', 'ip', 'zs']\n",
    "datadir = \"/home/jovyan/PyROA5/\"\n",
    "filters = FILTERS\n",
    "init_tau = [-3, 0, 0.7, 3.0, 3.5, 4.5]\n",
    "priors = [[0.5, 2.0],[0.5, 2.0], [-50.0, 50.0], [4.99, 5.01], [0.0, 10.0]]\n",
    "fit = PyROA.Fit(datadir, obj_name, filters, priors, add_var=True, init_tau = init_tau, Nsamples=70000, Nburnin=15000, use_backend=True, delay_ref='gp1') \n",
    "fit = PyROA.Fit(datadir, obj_name, filters, priors, add_var=True,Nsamples=30000, Nburnin=0,use_backend=True, resume_progress=True, delay_ref='gp1')\n",
    "outputdir = \"/home/jovyan/PyROA5/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj_name = 'Mrk_841'\n",
    "FILTERS = ['UVW2','gp1', 'gp', 'V', 'rp', 'ip', 'zs']\n",
    "datadir = \"/home/jovyan/PyROA10/\"\n",
    "filters = FILTERS\n",
    "init_tau = [-3, 0, 0.7, 3.0, 3.5, 4.5]\n",
    "priors = [[0.5, 2.0],[0.5, 2.0], [-50.0, 50.0], [9.99, 10.01], [0.0, 10.0]]\n",
    "fit = PyROA.Fit(datadir, obj_name, filters, priors, add_var=True, init_tau = init_tau, Nsamples=70000, Nburnin=15000, use_backend=True, delay_ref='gp1') \n",
    "fit = PyROA.Fit(datadir, obj_name, filters, priors, add_var=True,Nsamples=30000, Nburnin=0,use_backend=True, resume_progress=True, delay_ref='gp1')\n",
    "outputdir = \"/home/jovyan/PyROA10/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj_name = 'Mrk_841'\n",
    "FILTERS = ['UVW2','gp1', 'gp', 'V', 'rp', 'ip', 'zs']\n",
    "datadir = \"/home/jovyan/PyROAD20/\"\n",
    "filters = FILTERS\n",
    "init_tau = [-3, 0, 0.7, 3.0, 3.5, 4.5]\n",
    "priors = [[0.5, 2.0],[0.5, 2.0], [-50.0, 50.0], [19.99, 20.01], [0.0, 10.0]]\n",
    "fit = PyROA.Fit(datadir, obj_name, filters, priors, add_var=True, init_tau = init_tau, Nsamples=70000, Nburnin=15000, use_backend=True, delay_ref='gp1') \n",
    "fit = PyROA.Fit(datadir, obj_name, filters, priors, add_var=True,Nsamples=30000, Nburnin=0,use_backend=True, resume_progress=True, delay_ref='gp1')\n",
    "outputdir = \"/home/jovyan/PyROAD20/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj_name = 'Mrk_841'\n",
    "FILTERS = ['UVW2','gp1', 'gp', 'V', 'rp', 'ip', 'zs']\n",
    "datadir = \"/home/jovyan/PyROA40/\"\n",
    "filters = FILTERS\n",
    "init_tau = [-3, 0, 0.7, 3.0, 3.5, 4.5]\n",
    "priors = [[0.5, 2.0],[0.5, 2.0], [-50.0, 50.0], [39.99, 40.01], [0.0, 10.0]]\n",
    "fit = PyROA.Fit(datadir, obj_name, filters, priors, add_var=True, init_tau = init_tau, Nsamples=70000, Nburnin=15000, use_backend=True, delay_ref='gp1') \n",
    "fit = PyROA.Fit(datadir, obj_name, filters, priors, add_var=True,Nsamples=30000, Nburnin=0,use_backend=True, resume_progress=True, delay_ref='gp1')\n",
    "outputdir = \"/home/jovyan/PyROA40/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtaining times of Swift XRT observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names3 = ['time','time_pos','time_neg','count', 'count_pos', 'count_neg']\n",
    "curvedf = pd.read_table('curve_plot.qdp',skiprows=3,names=names3, delimiter='\\t')\n",
    "\n",
    "time = curvedf['time'].values\n",
    "t0 = 189304741.8006\n",
    "time = time+t0\n",
    "\n",
    "def convert_mjd(time):\n",
    "    return 51910.00015488 + time/86400\n",
    "\n",
    "mjd = convert_mjd(time)\n",
    "\n",
    "swiftxrt = pd.DataFrame({'time':mjd, 'count':curvedf['count'].values})\n",
    "swiftxrt.to_csv('swiftxrt.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Swift UVOT light curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "UVOT = pd.read_csv('Mrk841_UVOT.csv')\n",
    "FILTERS = ['UVW2', 'UVM2', 'UVW1', 'U', 'B', 'V']\n",
    "filtername=FILTERS\n",
    "def uvot_curve(dataframe):\n",
    "    \n",
    "    def helper(l):\n",
    "        sort_order = {k:v for k,v in zip(l, range(len(l)))}\n",
    "        return lambda s: s.map(lambda x: sort_order[x])\n",
    "\n",
    "    dataframed = dataframe.sort_values('FILTER', key=helper(FILTERS))\n",
    "    dataframed = dataframed.sort_values(by=['MJD'])\n",
    "    \n",
    "    fig, axs = plt.subplots(len(FILTERS), figsize=(15,12), sharex='col')#, layout='constrained')\n",
    "    axs[0].set_title(fr\"Mrk 841 light curves\")\n",
    "    fig.subplots_adjust(wspace=0, hspace=0)\n",
    "    xmin, xmax = -11, 11\n",
    "\n",
    "    colors = [\"lightseagreen\", \"darkgreen\", \"#8eab12\", \"darkorange\", \"crimson\", \"indigo\", \"lightseagreen\", \"darkgreen\", \"greenyellow\", \"darkorange\", \"crimson\", \"indigo\"]\n",
    "\n",
    "    fig.supylabel(r\"$F_\\nu$ (erg/$\\mathrm{cm}^2$/s/Hz)\", fontsize=20, position=(0.05,0.5))\n",
    "    for i in range(len(FILTERS)):\n",
    "        f1 = FILTERS[i]\n",
    "        lc1 = dataframed.loc[(dataframed['FILTER'] == str(f1))]\n",
    "        mjd1, mjderr1, flux1, err1 =  lc1['MJD'].to_numpy(), lc1['MJD_ERR'].to_numpy(), lc1['FLUX_HZ_5'].to_numpy(), lc1['FLUX_HZ_5_ERR_SIG7P5'].to_numpy()\n",
    "\n",
    "        f2=filtername[i]\n",
    "\n",
    "        axs[i].errorbar(mjd1, flux1, yerr = err1, xerr=mjderr1, marker = '.', linestyle = ':', color = colors[i])\n",
    "        axs[i].set_ylabel(fr\"{f2}\")\n",
    "        axs[i].invert_yaxis()\n",
    "        \n",
    "    axs[-1].set_xlabel(\"Modified Julian Date\") \n",
    "    axs[-1].set_xlabel(\"Lag (days)\")\n",
    "    plt.savefig(fr\"UVOT_curve.png\", dpi=300)\n",
    "uvot_curve(UVOT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyCCF light curves and lags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing data\n",
    "ALLSTARS = pd.read_csv(obj_name +'_ALLSTARS'+'.csv')\n",
    "swiftxrt = pd.read_csv('swiftxrt.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (368577995.py, line 108)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"C:\\Users\\18jus\\AppData\\Local\\Temp\\ipykernel_11504\\368577995.py\"\u001b[1;36m, line \u001b[1;32m108\u001b[0m\n\u001b[1;33m    plt.savefig(fr\"totalspec{year}.png\", dpi=300)b\u001b[0m\n\u001b[1;37m                                                 ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "filtername=[r\"UVW2\", r\"$g'$\", r\"$V$\", r\"$r'$\", r\"$i'$\", r\"$z_s$\"]\n",
    "\n",
    "def totalspec2(dataframe, database, time):\n",
    "    database = database.sort_values(by=['MJD'])\n",
    "   \n",
    "    for j in range(2):\n",
    "        if j == 0:\n",
    "            dataframed = dataframe.loc[(dataframe['Year'] == 2016)]\n",
    "            year = 2016\n",
    "            \n",
    "            ref_date1 = 57495\n",
    "            ref_date2 = 57641\n",
    "            \n",
    "            \n",
    "        else:\n",
    "            dataframed = dataframe.loc[(dataframe['Year'] == 2017)]\n",
    "            year = 2017\n",
    "            \n",
    "            ref_date1 = 57751\n",
    "            ref_date2 = 57988\n",
    "        \n",
    "        databased = database.loc[(database['MJD'] > ref_date1) & (database['MJD'] < ref_date2)]\n",
    "        \n",
    "        \n",
    "        def helper(l):\n",
    "            sort_order = {k:v for k,v in zip(l, range(len(l)))}\n",
    "            return lambda s: s.map(lambda x: sort_order[x])\n",
    "        \n",
    "        databased = databased.sort_values('Filter', key=helper(FILTERS))\n",
    "        dataframed = dataframed.sort_values('Filter', key=helper(FILTERS))\n",
    "        \n",
    "        databased = databased.sort_values(by=['MJD'])\n",
    "        \n",
    "        databased = databased.reset_index(drop=True)\n",
    "        dataframed = dataframed.reset_index(drop=True)\n",
    "        \n",
    "        fig, axs = plt.subplots(len(FILTERS), 2, figsize=(15,12), gridspec_kw={'width_ratios': [3, 1]}, sharex='col')#, layout='constrained')\n",
    "        axs[0,0].set_title(fr\"Mrk 841 light curves\")\n",
    "        axs[0,1].set_title(\"CCF\")\n",
    "        \n",
    "        fig.subplots_adjust(wspace=0, hspace=0)\n",
    "        \n",
    "        xmin, xmax = -11, 11\n",
    "        \n",
    "        colors = [\"lightseagreen\", \"darkgreen\", \"#8eab12\", \"darkorange\", \"crimson\", \"indigo\", \"lightseagreen\", \"darkgreen\", \"greenyellow\", \"darkorange\", \"crimson\", \"indigo\"]\n",
    "    \n",
    "        fig.supylabel(fr\"$F_\\nu$ (mJy)\", fontsize=20, position=(0.05,0.5))\n",
    "        for i in range(len(FILTERS)):\n",
    "            f1 = FILTERS[i]\n",
    "            # print(f1)\n",
    "            lc1 = Locate_Star_Filter(databased, str(f1))\n",
    "            # display(lc1)\n",
    "            datum = dataframed.loc[(dataframed['Filter'] == str(f1))]\n",
    "            # display(datum)\n",
    "            mjd1, flux1, err1 =  lc1['MJD'].to_numpy(), lc1['flux'].to_numpy(), lc1['err'].to_numpy()\n",
    "            \n",
    "            f2=filtername[i]\n",
    "            \n",
    "            nbins = 15\n",
    "            lag = datum['CCF'][i]\n",
    "            r = datum['Correlation'][i]\n",
    "            tlags_centroid = datum['Centroid hist'][i]\n",
    "                \n",
    "                \n",
    "            axs[i,0].errorbar(mjd1, flux1, yerr = err1, marker = '.', linestyle = ':', color = colors[i])\n",
    "            axs[i,0].set_ylabel(fr\"{f2}\")\n",
    "            axs[i,0].invert_yaxis()\n",
    "            axs[i,0].vlines(time['time'], ymin=-20, ymax=20, zorder=0, color = 'lightgrey')\n",
    "            axs[i,0].set_xlim(min(mjd1)*0.9999, max(mjd1)*1.0001)\n",
    "            axs[i,0].set_ylim(min(flux1)*0.9, max(flux1)*1.1)\n",
    "           \n",
    "            axs[i,1].plot(lag, r, color='k')\n",
    "            axs[i,1].set_xlim(xmin, xmax)\n",
    "            axs[i, 1].set_ylim(0,1)\n",
    "            axs[i,1].set_yticks([])\n",
    "            ax2 = axs[i,1].twinx()\n",
    "            ax2.hist(tlags_centroid, bins = nbins, color = colors[i])\n",
    "            ax2.axvline(x = np.percentile(tlags_centroid, [16, 50, 84])[1], color=\"black\")\n",
    "            ax2.axvline(x = np.percentile(tlags_centroid, [16, 50, 84])[0], color=\"black\", ls=\"--\")\n",
    "            ax2.axvline(x = np.percentile(tlags_centroid, [16, 50, 84])[2], color=\"black\", ls=\"--\")\n",
    "            ax2.set_yticks([])\n",
    "            ax3 = axs[i,1].twinx()\n",
    "            ax3.set_yticks([])\n",
    "            ax3.axvline(0, color=\"dodgerblue\")\n",
    "            \n",
    "            if i == 0:\n",
    "                ticker = [0,0.25, 0.5, 0.75, 1]\n",
    "                labels = [\"0.00, 1\", \"0.25\", \"0.50\", \"0.75\", \"1.00\"]\n",
    "                axs[i,1].yaxis.tick_right()\n",
    "                axs[i,1].set_yticks(ticker)\n",
    "                axs[i,1].set_yticklabels(labels)\n",
    "            elif i == 5:\n",
    "                ticker = [0,0.25, 0.5, 0.75]\n",
    "                labels = [\"0.00\", \"0.25\", \"0.50\", \"0.75\"]\n",
    "                axs[i,1].yaxis.tick_right()\n",
    "                axs[i,1].set_yticks(ticker)\n",
    "                axs[i,1].set_yticklabels(labels)\n",
    "            else:\n",
    "                ticker = [0,0.25, 0.5, 0.75]\n",
    "                labels = [\"0.00, 1\", \"0.25\", \"0.50\", \"0.75\"]\n",
    "                axs[i,1].yaxis.tick_right()\n",
    "                axs[i,1].set_yticks(ticker)\n",
    "                axs[i,1].set_yticklabels(labels)\n",
    "            \n",
    "            \n",
    "        axs[-1,0].set_xlabel(\"Modified Julian Date\") \n",
    "        axs[-1,1].set_xlabel(\"Lag (days)\")\n",
    "        plt.savefig(fr\"totalspec{year}.png\", dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "totalspec2(DF_FCCF, ALLSTARS, swiftxrt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyROA light curves and lags, flux-flux, and SED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#only fits from Delta=2 were chosen to be plotted\n",
    "\n",
    "waves = [1894, 4770, 5510, 6231, 7625, 9134]\n",
    "redshift = 0.0364\n",
    "band_colors=['#0652DD','#1289A7','#006266','#A3CB38','orange','#EE5A24','brown']\n",
    "delay_ref = 'gp1'\n",
    "outputdir = \"/home/jovyan/PyROA2/\"\n",
    "datadir = \"/home/jovyan/PyROA2/\"\n",
    "gal_ref = 'UVW2'\n",
    "burnin=150000\n",
    "# following functions are adapted from Utils.py accessible at https://github.com/Alymantara/PyROA\n",
    "# Reference: Donnan et al. 2021 (https://ui.adsabs.harvard.edu/abs/2021arXiv210712318D/abstract)\n",
    "\n",
    "import scipy.interpolate as interpolate\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import median_abs_deviation as mad\n",
    "FILTERS = [\"UVW2\", \"gp1\", \"gp\", \"V\", \"rp\", \"ip\", \"zs\"]\n",
    "filters = FILTERS\n",
    "waves = [1894, 4770, 5510, 6231, 7625, 9134]#[3580,4392,4770,5468,6215,7545,8700]\n",
    "redshift = 0.0364\n",
    "band_colors=colors = [\"lightseagreen\", \"darkgreen\", \"darkgreen\",\"#8eab12\", \"darkorange\", \"crimson\", \"indigo\", \"brown\"]\n",
    "delay_ref = 'gp1'\n",
    "outputdir = \"/home/jovyan/PyROA2/\"\n",
    "datadir = \"/home/jovyan/PyROA2/\"\n",
    "gal_ref = 'UVW2'\n",
    "obj_name = 'Mrk_841'\n",
    "swiftxrt = pd.read_csv('swiftxrt.csv')\n",
    "burnin=150000\n",
    "\n",
    "def Lightcurves(objName, filters, delay_ref, \n",
    "                lc_file=\"Lightcurve_models.obj\",\n",
    "                samples_file='samples_flat.obj',\n",
    "                slow_comp_file='Slow_Comps.obj',\n",
    "                outputdir = './', datadir='./',\n",
    "                burnin=0, band_colors = None,\n",
    "                limits=None, grid=False, grid_step=5.0,\n",
    "                show_delay_ref=False, ylab = None,\n",
    "                filter_labels = None, savefig=True, figname=None,\n",
    "                include_slow_comp=False,slow_comp_delta=30.0, time=None\n",
    "                ):\n",
    "\n",
    "    if outputdir[-1] != '/': outputdir += '/'\n",
    "\n",
    "    if ylab ==None: ylab = r\"F$_{\\nu}$\"+\"\\nmJy\"\n",
    "    if filter_labels == None: filter_labels = filters\n",
    "\n",
    "    ss = np.where(np.array(filters) == delay_ref)[0][0]\n",
    "    file = open(outputdir+samples_file,'rb')\n",
    "    samples_flat = pickle.load(file)\n",
    "    samples_flat = samples_flat[burnin:,:]\n",
    "    file = open(outputdir+lc_file,'rb')\n",
    "    models = pickle.load(file)\n",
    "\n",
    "    if include_slow_comp:\n",
    "        file = open(outputdir+slow_comp_file,'rb')\n",
    "        slow_comps = pickle.load(file)\n",
    "\n",
    "\n",
    "    #Split samples into chunks, 4 per lightcurve i.e A, B, tau, sig\n",
    "    chunk_size=4\n",
    "    transpose_samples = np.transpose(samples_flat)\n",
    "    #Insert zero where tau_0 would be \n",
    "    transpose_samples = np.insert(transpose_samples, [ss*4+2], np.array([0.0]*len(transpose_samples[1])), axis=0)\n",
    "    samples_chunks = [transpose_samples[i:i + chunk_size] for i in range(0, len(transpose_samples), chunk_size)] \n",
    "\n",
    "\n",
    "\n",
    "    fig = plt.figure(figsize=(20,len(filters)*3.5))\n",
    "    fig.tight_layout()\n",
    "    corro = 1\n",
    "    if show_delay_ref: corro = 0\n",
    "    gs = fig.add_gridspec(len(filters)-corro, 2, hspace=0, wspace=0, width_ratios=[5, 1])\n",
    "    axs= gs.subplots(sharex='col')\n",
    "\n",
    "    if band_colors == None:\n",
    "        band_colors = ['k']*len(filters)\n",
    "\n",
    "    #Loop over lightcurves\n",
    "\n",
    "    data=[]\n",
    "    ko = 0\n",
    "\n",
    "    if limits !=None:\n",
    "        xmin=limits[0]#59337\n",
    "        xmax=limits[1]#59621\n",
    "\n",
    "    for i in range(len(filters)):\n",
    "        #Read in data\n",
    "        file = datadir + objName+\"_\" + str(filters[i]) + \".dat\"\n",
    "        data.append(np.loadtxt(file))\n",
    "        mjd = data[i][:,0]\n",
    "        flux = data[i][:,1]\n",
    "        err = data[i][:,2]    \n",
    "\n",
    "        if (i == 0) & (limits == None):\n",
    "            xmin = np.nanmin(mjd)-10\n",
    "            xmax = np.nanmax(mjd)+10\n",
    "        #Add extra variance\n",
    "        B = np.percentile(samples_chunks[i][1], 50)\n",
    "        sig = np.percentile(samples_chunks[i][3], 50)\n",
    "        err = np.sqrt(err**2 + sig**2)\n",
    "        fig.supylabel(fr\"$F_\\nu$ (mJy)\", position=(50, 50))\n",
    "        \n",
    "        if ((filters[i] != delay_ref) ):\n",
    "\n",
    "            gs00 = gridspec.GridSpecFromSubplotSpec(6, 1, subplot_spec=axs[i-ko][0],hspace=0)\n",
    "            ax1 = fig.add_subplot(gs00[:, :])\n",
    "            ax1.set_ylim(np.median(flux)-4.8*mad(flux),np.median(flux)+4.8*mad(flux))\n",
    "            if i < len(filters)-1:\n",
    "                ax1.set_xticklabels([])\n",
    "            else:\n",
    "                ax1.set_xlabel(\"MJD\")\n",
    "            axs[i-ko][0].set_yticklabels([])\n",
    "            #Plot Data\n",
    "            ax1.errorbar(mjd, flux , yerr=err, ls='none', marker=\".\", color=band_colors[i], ms=2)\n",
    "            ax1.vlines(time['time'], ymin=-20, ymax=20, zorder=0, color = 'lightgrey')\n",
    "            #Plot Model\n",
    "            t, m, errs = models[i]\n",
    "            new_m = np.interp(mjd,t, m)\n",
    "            \n",
    "            if grid:\n",
    "                for hh in np.arange(59330,xmax,grid_step):\n",
    "                    ax1.axvline(x=hh,ls='--',color='grey',alpha=0.4)\n",
    "                \n",
    "            ax1.set_xlim(xmin,xmax)\n",
    "            \n",
    "            ax1.plot(t,m, color=\"black\", lw=3)\n",
    "\n",
    "            if (include_slow_comp == True):\n",
    "                slow_comp = slow_comps[i]\n",
    "                \n",
    "                ax1.plot(mjd, slow_comp(mjd)+B, linestyle=\"dashed\", color=\"black\")  \n",
    "            filto = filter_labels[i]\n",
    "            ax1.text(0.1,0.2,filto, color=band_colors[i], fontsize=19, transform=ax1.transAxes)\n",
    "            ax1.fill_between(t, m+errs, m-errs, alpha=0.5, color=\"black\")\n",
    "           \n",
    "            #Plot Time delay posterior distributions\n",
    "            tau_samples = samples_chunks[i][2],\n",
    "            axs[i-ko][1].hist(tau_samples, color=band_colors[i], bins=50,histtype='stepfilled')\n",
    "            axs[i-ko][1].axvline(x = np.percentile(tau_samples, [16, 50, 84])[1], color=\"black\")\n",
    "            axs[i-ko][1].axvline(x = np.percentile(tau_samples, [16, 50, 84])[0] , color=\"black\", ls=\"--\")\n",
    "            axs[i-ko][1].axvline(x = np.percentile(tau_samples, [16, 50, 84])[2], color=\"black\",ls=\"--\")\n",
    "            axs[i-ko][1].axvline(x = 0, color=\"dodgerblue\",ls=\"-\")    \n",
    "            axs[i-ko][1].set_xlabel(\"Lag (days)\")\n",
    "            axs[i-ko][1].set_yticklabels([])\n",
    "            axs[i-ko][1].axes.get_yaxis().set_visible(True)\n",
    "            axs[i-ko][0].set_xticklabels([])\n",
    "            \n",
    "            axs[0][0].set_title(\"Mrk 841 light curves\")\n",
    "            axs[0][1].set_title(\"CCF\")\n",
    "\n",
    "\n",
    "        if (filters[i] == delay_ref):\n",
    "            \n",
    "            if ((show_delay_ref == True)):\n",
    "                gs00 = gridspec.GridSpecFromSubplotSpec(6, 1, subplot_spec=axs[i-ko][0],hspace=0)\n",
    "                ax1 = fig.add_subplot(gs00[:-2, :])\n",
    "                ax2 = fig.add_subplot(gs00[-2:, :])\n",
    "                ax1.set_ylim(np.median(flux)-4.8*mad(flux),np.median(flux)+4.8*mad(flux))\n",
    "                if i < len(filters)-1:\n",
    "                    \n",
    "                    ax1.set_xticklabels([])\n",
    "                else:\n",
    "                    ax1.set_xlabel(\"MJD\")\n",
    "                ax1.set_xticklabels([])\n",
    "                axs[i-ko][0].set_yticklabels([])\n",
    "                #Plot Data\n",
    "                ax1.errorbar(mjd, flux , yerr=err, ls='none', marker=\".\", color=band_colors[i], ms=2)\n",
    "                #Plot Model\n",
    "                t, m, errs = models[i]\n",
    "                new_m = np.interp(mjd,t, m)\n",
    "                \n",
    "                if grid:\n",
    "                    for hh in np.arange(59330,xmax,5):\n",
    "                        ax1.axvline(x=hh,ls='--',color='grey',alpha=0.4)\n",
    "                    \n",
    "                ax2.set_xlim(xmin,xmax)\n",
    "                ax1.set_xlim(xmin,xmax)\n",
    "                \n",
    "                ax1.plot(t,m, color=\"black\", lw=3)\n",
    "                filto = filter_labels[i]\n",
    "                ax1.text(0.1,0.2,filto, color=band_colors[i], fontsize=19, transform=ax1.transAxes)\n",
    "                ax1.fill_between(t, m+errs, m-errs, alpha=0.5, color=\"black\")\n",
    "                ax1.set_ylabel(ylab)\n",
    "                ax2.set_ylabel(r\"$\\chi$\")\n",
    "                #print('   --> Skipping')\n",
    "                ko =0\n",
    "            else:\n",
    "                ko=1\n",
    "                            \n",
    "    for ax in axs.flat:\n",
    "        ax.label_outer()    \n",
    "\n",
    "    \n",
    "    plt.savefig(\"pyroa_lightcurve.png\", dpi=300)\n",
    "\n",
    "def FluxFlux(objName, filters, delay_ref, gal_ref,wavelengths,\n",
    "            lc_file=\"Lightcurve_models.obj\",\n",
    "            samples_file='samples_flat.obj',\n",
    "            xt_file='X_t.obj',\n",
    "            outputdir = './', datadir='./',\n",
    "            burnin=0, band_colors = None,\n",
    "            input_units='mJy',output_units='mJy',\n",
    "            redshift=0.0, ebv=0.0,\n",
    "            limits=None, ylab = None,\n",
    "            savefig=True, figname=None,\n",
    "            model=None):\n",
    "    \n",
    "    def unred(wave, flux, ebv, R_V=3.1, LMC2=False, AVGLMC=False):\n",
    "\n",
    "        x = 10000./ wave # Convert to inverse microns\n",
    "        curve = x*0.\n",
    "\n",
    "        # Set some standard values:\n",
    "        x0 = 4.596\n",
    "        gamma =  0.99\n",
    "        c3 =  3.23\n",
    "        c4 =  0.41\n",
    "        c2 = -0.824 + 4.717/R_V\n",
    "        c1 =  2.030 - 3.007*c2\n",
    "\n",
    "        if LMC2:\n",
    "            x0\t=  4.626\n",
    "            gamma =  1.05\n",
    "            c4   =  0.42\n",
    "            c3\t=  1.92\n",
    "            c2\t= 1.31\n",
    "            c1\t=  -2.16\n",
    "        elif AVGLMC:\n",
    "            x0 = 4.596\n",
    "            gamma = 0.91\n",
    "            c4   =  0.64\n",
    "            c3\t=  2.73\n",
    "            c2\t= 1.11\n",
    "            c1\t=  -1.28\n",
    "\n",
    "        # Compute UV portion of A(lambda)/E(B-V) curve using FM fitting function and\n",
    "        # R-dependent coefficients\n",
    "        xcutuv = np.array([10000.0/2700.0])\n",
    "        xspluv = 10000.0/np.array([2700.0,2600.0])\n",
    "\n",
    "        iuv = np.where(x >= xcutuv)[0]\n",
    "        N_UV = len(iuv)\n",
    "        iopir = np.where(x < xcutuv)[0]\n",
    "        Nopir = len(iopir)\n",
    "        if (N_UV > 0): xuv = np.concatenate((xspluv,x[iuv]))\n",
    "        else:  xuv = xspluv\n",
    "\n",
    "        yuv = c1  + c2*xuv\n",
    "        yuv = yuv + c3*xuv**2/((xuv**2-x0**2)**2 +(xuv*gamma)**2)\n",
    "        yuv = yuv + c4*(0.5392*(np.maximum(xuv,5.9)-5.9)**2+0.05644*(np.maximum(xuv,5.9)-5.9)**3)\n",
    "        yuv = yuv + R_V\n",
    "        yspluv  = yuv[0:2]  # save spline points\n",
    "\n",
    "        if (N_UV > 0): curve[iuv] = yuv[2::] # remove spline points\n",
    "\n",
    "        # Compute optical portion of A(lambda)/E(B-V) curve\n",
    "        # using cubic spline anchored in UV, optical, and IR\n",
    "        xsplopir = np.concatenate(([0],10000.0/np.array([26500.0,12200.0,6000.0,5470.0,4670.0,4110.0])))\n",
    "        ysplir   = np.array([0.0,0.26469,0.82925])*R_V/3.1\n",
    "        ysplop   = np.array((np.polyval([-4.22809e-01, 1.00270, 2.13572e-04][::-1],R_V ),\n",
    "                np.polyval([-5.13540e-02, 1.00216, -7.35778e-05][::-1],R_V ),\n",
    "                np.polyval([ 7.00127e-01, 1.00184, -3.32598e-05][::-1],R_V ),\n",
    "                np.polyval([ 1.19456, 1.01707, -5.46959e-03, 7.97809e-04, -4.45636e-05][::-1],R_V ) ))\n",
    "        ysplopir = np.concatenate((ysplir,ysplop))\n",
    "\n",
    "        if (Nopir > 0):\n",
    "          tck = interpolate.splrep(np.concatenate((xsplopir,xspluv)),np.concatenate((ysplopir,yspluv)),s=0)\n",
    "          curve[iopir] = interpolate.splev(x[iopir], tck)\n",
    "\n",
    "        #Now apply extinction correction to input flux vector\n",
    "        curve *= ebv\n",
    "\n",
    "        return flux * 10.**(0.4*curve)\n",
    "\n",
    "    plt.rcParams.update({\n",
    "        \"font.family\": \"Sans\",  \n",
    "        \"font.serif\": [\"DejaVu\"],\n",
    "    \"figure.figsize\":[40,30],\n",
    "    \"font.size\": 19})  \n",
    "\n",
    "    if outputdir[-1] != '/': outputdir += '/'\n",
    "    if ylab ==None: ylab = r\"F$_{\\nu}$\"+\" (mJy)\"\n",
    "    #if filter_labels == None: filter_labels = filters\n",
    "\n",
    "    if input_units == 'mJy': funits = 1*u.mJy\n",
    "    if input_units == 'Jy': funits = 1*u.Jy\n",
    "    if input_units == 'fnu': funits = 1*u.erg/u.s/(u.cm**2)/u.Hz\n",
    "    if input_units == 'flam': funits = 1*u.erg/u.s/(u.cm**2)/u.Angstrom\n",
    "    if output_units == 'mJy': \n",
    "        ylab = r\"F$_{\\nu}$\"+\" (mJy)\"\n",
    "    if output_units == 'Jy': \n",
    "        ylab = r\"F$_{\\nu}$\"+\" (mJy)\"\n",
    "    if output_units == 'fnu': \n",
    "        ylab = r\"F$_{\\nu}$\"+r\" / erg s$^{-1}$ cm$^{-2}$ ${\\rm Hz}^{-1}$\"\n",
    "    if output_units == 'flam': \n",
    "        ylab = r\"F$_{\\lambda}$\"+r\" / $\\times10^{-15}$ erg s$^{-1}$ cm$^{-2}$ ${\\rm \\AA}^{-1}$\"\n",
    "\n",
    "    ss = np.where(np.array(filters) == delay_ref)[0][0]\n",
    "    file = open(outputdir+samples_file,'rb')\n",
    "    samples_flat = pickle.load(file)\n",
    "    samples_flat = samples_flat[burnin:,:]\n",
    "    file = open(outputdir+lc_file,'rb')\n",
    "    models = pickle.load(file)\n",
    "    \n",
    "    file = open(outputdir+xt_file,'rb')\n",
    "    norm_lc = pickle.load(file)\n",
    "    wave = np.array(wavelengths)\n",
    "\n",
    "    #Split samples into chunks, 4 per lightcurve i.e A, B, tau, sig\n",
    "    chunk_size=4\n",
    "    transpose_samples = np.transpose(samples_flat)\n",
    "    #Insert zero where tau_0 would be \n",
    "    transpose_samples = np.insert(transpose_samples, [ss*4+2], np.array([0.0]*len(transpose_samples[1])), axis=0)\n",
    "    samples_chunks = [transpose_samples[i:i + chunk_size] for i in range(0, len(transpose_samples), chunk_size)]\n",
    "    \n",
    "\n",
    "    gal_spectrum,gal_spectrum_err,fnu_f,fnu_b,slope,slope_err = [],[],[],[],[],[]\n",
    "    fnu_f_err,fnu_b_err = [], []\n",
    "    \n",
    "    fig = plt.figure(figsize=(10,7))\n",
    "    xx = np.linspace(-15,5,300)\n",
    "    max_flux = 0.0\n",
    "    \n",
    "    kk = 0\n",
    "    fac_flux = np.ones(len(wavelengths))\n",
    "    for i in range(len(filters)):\n",
    "        if ((filters[i] != delay_ref) ):\n",
    "            \n",
    "            file = datadir + objName+\"_\" + str(filters[i]) + \".dat\"\n",
    "            data = np.loadtxt(file)\n",
    "            snu_mcmc = samples_chunks[i][0]\n",
    "            cnu_mcmc = samples_chunks[i][1]            \n",
    "            sig = np.percentile(samples_chunks[i][3], 50)\n",
    "\n",
    "            mc_pl = np.zeros((200,xx.size))\n",
    "\n",
    "            for lo in range(200):\n",
    "                jj = int(np.random.uniform(0,snu_mcmc.size))\n",
    "                mc_pl[lo] = cnu_mcmc[jj] + xx * snu_mcmc[jj]\n",
    "            \n",
    "            if filters[i] == gal_ref: \n",
    "                x_gal_mcmc = -cnu_mcmc/snu_mcmc\n",
    "                x_gal = np.median(x_gal_mcmc)\n",
    "                x_gal_error = np.std(-cnu_mcmc/snu_mcmc)\n",
    "                \n",
    "            gal_spectrum_mcmc = np.median(cnu_mcmc) +  (x_gal_mcmc+x_gal_mcmc.std()) * np.median(snu_mcmc)\n",
    "            \n",
    "            gal_spectrum.append(gal_spectrum_mcmc.mean())\n",
    "            gal_spectrum_err.append(gal_spectrum_mcmc.std())\n",
    "            \n",
    "            fnu_f_mcmc = snu_mcmc * (np.min(norm_lc[1]) - x_gal_mcmc)\n",
    "            fnu_b_mcmc = snu_mcmc * (np.max(norm_lc[1]) - x_gal_mcmc)\n",
    "    \n",
    "            fnu_f.append(fnu_f_mcmc.mean())\n",
    "            fnu_f_err.append(fnu_f_mcmc.std())\n",
    "\n",
    "            fnu_b.append(fnu_b_mcmc.mean())\n",
    "            fnu_b_err.append(fnu_b_mcmc.std())\n",
    "\n",
    "            slope.append(np.median(snu_mcmc))\n",
    "            slope_err.append(np.std(snu_mcmc))\n",
    "\n",
    "            lin_fit = np.median(snu_mcmc) * xx + np.median(cnu_mcmc)\n",
    "            \n",
    "            \n",
    "            if wavelengths != None:       \n",
    "\n",
    "                         \n",
    "                if (input_units != 'flam') and (output_units !='flam'):\n",
    "                    wave = wavelengths[i+kk] * u.Angstrom\n",
    "                    dd = funits\n",
    "                    #print(input_units,output_units)\n",
    "                    if output_units != 'fnu':\n",
    "                        fac_flux[i+kk] = dd.cgs.to(output_units).value\n",
    "                    else:\n",
    "                        fac_flux[i+kk] = dd.cgs.to('erg s^-1 cm^-2 Hz^-1').value\n",
    "\n",
    "                if (input_units != 'flam') and (output_units =='flam'):\n",
    "                    wave = wavelengths[i+kk] * u.Angstrom\n",
    "                    dd = funits/(wave**2)*ct.c\n",
    "\n",
    "                    fac_flux[i+kk] = dd.cgs.to('erg s^-1 cm^-2 Angstrom^-1').value/1e-15\n",
    "\n",
    "                if (input_units == 'flam') and (output_units !='flam'):\n",
    "                    wave = wavelengths[i+kk] * u.Angstrom\n",
    "                    dd = funits/ct.c*(wave**2)\n",
    "                    if output_units != 'fnu':\n",
    "                        fac_flux[i+kk] = dd.cgs.to(output_units).value\n",
    "                    else:\n",
    "                        fac_flux[i+kk] = dd.cgs.to('erg s^-1 cm^-2 Hz^-1').value\n",
    "                        \n",
    "                        \n",
    "            plt.fill_between(xx,(mc_pl.mean(axis=0)+mc_pl.std(axis=0))*fac_flux[i+kk],\n",
    "                        (mc_pl.mean(axis=0)-mc_pl.std(axis=0))*fac_flux[i+kk],\n",
    "                        color=band_colors[i],\n",
    "                        alpha=0.3)\n",
    "            interp_xt = np.interp(data[:,0],norm_lc[0],norm_lc[1])\n",
    "            plt.errorbar(interp_xt,data[:,1]*fac_flux[i+kk],\n",
    "                        yerr=np.sqrt(data[:,2]**2+sig**2)*fac_flux[i+kk],\n",
    "                        color=band_colors[i],\n",
    "                        ls='None',alpha=0.8)\n",
    "            plt.plot(xx,lin_fit*fac_flux[i+kk],color=band_colors[i],lw=3)\n",
    "            max_flux = np.max([max_flux,np.max(data[:,1]*fac_flux[i+kk])])\n",
    "\n",
    "        else:\n",
    "            kk = -1\n",
    "    fnu_f = np.array(fnu_f)\n",
    "    fnu_f_err = np.array(fnu_f_err)\n",
    "    fnu_b = np.array(fnu_b)\n",
    "    fnu_b_err = np.array(fnu_b_err)\n",
    "    slope = np.array(slope)\n",
    "    slope_err = np.array(slope_err)\n",
    "    gal_spectrum = np.array(gal_spectrum)\n",
    "    gal_spectrum_err = np.array(gal_spectrum_err)\n",
    "    \n",
    "    plt.axvline(x=np.median(x_gal_mcmc+x_gal_mcmc.std()),color='r',\n",
    "                linestyle='-.',label=r'$X_{\\rm G}$')\n",
    "    plt.axvline(x=np.min(norm_lc[1]),color='k',\n",
    "                linestyle='--',label=r'$X_{\\rm F}$')\n",
    "    plt.axvline(x=np.max(norm_lc[1]),color='grey',\n",
    "                linestyle='--',label=r'$X_{\\rm B}$')\n",
    "\n",
    "    lg = plt.legend(ncol=4)\n",
    "    plt.xlim(x_gal-1,3)\n",
    "    #print()\n",
    "    plt.ylim(-0.04*fac_flux[-1],max_flux*1.2)\n",
    "    if limits != None: plt.ylim(-0.04,limits[1])\n",
    "    \n",
    "    plt.xlabel(r'$X_0 (t)$, Normalised driving light curve flux')\n",
    "    plt.ylabel(ylab)\n",
    "    plt.tight_layout()\n",
    "    if savefig:\n",
    "        plt.savefig(\"pyroafluxflux.png\", dpi=300)\n",
    "    if wavelengths != None:\n",
    "        wave = np.array(wavelengths)\n",
    "        fig = plt.figure(figsize=(10,7))\n",
    "        ax = fig.add_subplot(111)\n",
    "        xxx = np.arange(2000,9300)\n",
    "\n",
    "        # AGN variability range\n",
    "        plt.fill_between(wave/(1+redshift),(np.array(unred(wave,fnu_b,ebv)))*fac_flux,\n",
    "                        (np.array(unred(wave,fnu_f,ebv)))*fac_flux\n",
    "                         ,color='k',alpha=0.1,label='AGN variability')\n",
    "        ### F_bright - F_faint\n",
    "        plt.errorbar(wave/(1+redshift),(np.array(unred(wave,fnu_b,ebv)) - \\\n",
    "                                    np.array(unred(wave,fnu_f,ebv)))*fac_flux,\n",
    "                 yerr=np.sqrt((np.array(fnu_f_err))**2 + (np.array(fnu_b_err))**2)*fac_flux,\n",
    "                 marker='.',linestyle='-',color='k',\n",
    "                 label=r'AGN high-low',ms=15)\n",
    "        \n",
    "        ### average spectrum\n",
    "        plt.errorbar(wave/(1+redshift),(np.array(unred(wave,fnu_b,ebv)) + np.array(unred(wave,fnu_f,ebv)))*fac_flux/2,\n",
    "                 yerr=np.sqrt((np.array(fnu_f_err))**2 + (np.array(fnu_b_err))**2)*fac_flux/2,\n",
    "                 marker='.',linestyle='-',color='b',\n",
    "                 label=r'AGN average',ms=15)\n",
    "\n",
    "        ### AGN RMS\n",
    "        plt.errorbar(wave/(1+redshift),np.array(unred(wave,slope,ebv))*fac_flux,\n",
    "             yerr=0,marker='o',linestyle='--',color='grey',label='AGN RMS')\n",
    "        \n",
    "        ### Galaxy spectrum\n",
    "        plt.errorbar(wave/(1+redshift),unred(wave,gal_spectrum,ebv)*fac_flux,\n",
    "                 yerr=gal_spectrum_err*fac_flux,\n",
    "                 marker='s',color='r',label='Galaxy contribution',linestyle='-.')\n",
    "        \n",
    "        a=12\n",
    "        x = np.linspace(1000,10000)\n",
    "        y = a*x**-(1/3)\n",
    "        ax.plot(x, y, label=r\"$\\nu^{1/3}$\", color=\"green\")\n",
    "    \n",
    "        #print(fac_flux)\n",
    "        plt.xscale('log')\n",
    "        plt.yscale('log')\n",
    "        plt.xlim(np.min(wave/(1+redshift))-100,np.max(wave/(1+redshift))+100)\n",
    "        #print(np.min(np.array(unred(wave,slope,ebv)))*0.7,max_flux*1.2)\n",
    "        plt.ylim(np.min(np.array(unred(wave,slope,ebv)))*0.7*fac_flux[-1],max_flux*1.2)\n",
    "        if limits != None: plt.ylim(limits[0],limits[1])\n",
    "        lg = plt.legend(ncol=2, loc='lower right')\n",
    "        if redshift > 0:\n",
    "            plt.xlabel(r'Rest Wavelength ($\\mathrm{\\AA}$)')\n",
    "        else:\n",
    "            plt.xlabel(r'Observed Wavelength / $\\mathrm{\\AA}$')\n",
    "        plt.ylabel(ylab)\n",
    "\n",
    "        ax.yaxis.set_major_formatter(mtick.FormatStrFormatter('%.2f'))\n",
    "        ax.xaxis.set_major_formatter(mtick.FormatStrFormatter('%.0f'))\n",
    "        ax.xaxis.set_minor_formatter(mtick.FormatStrFormatter('%.0f'))\n",
    "        ax.xaxis.set_minor_locator(ticker.MultipleLocator(2000))\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        if savefig:\n",
    "            plt.savefig(\"pyroased.png\", dpi=300)\n",
    "    else:\n",
    "        print(' [PyROA] No wavelength list. Skipping SED plot.')\n",
    "        # Create output file from flux-flux analysis\n",
    "    #print(wave)\n",
    "    d = {'wave': wave,\n",
    "         'mean': np.array((np.array(unred(wave,fnu_b,ebv)) + np.array(unred(wave,fnu_f,ebv)))*fac_flux/2),\n",
    "         'mean_err': np.array(np.sqrt((np.array(fnu_f_err))**2 + (np.array(fnu_b_err))**2)*fac_flux/2),\n",
    "         'faint':np.array(unred(wave,fnu_f,ebv))*fac_flux,\n",
    "         'faint_err':np.sqrt((np.array(fnu_f_err)))}\n",
    "    df = pd.DataFrame(data=d)\n",
    "    df.to_csv(objName+'_MEANSPEC.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Lightcurves(obj_name,filters,delay_ref,\n",
    "                  datadir=datadir, outputdir=outputdir,\n",
    "                  burnin=burnin, band_colors=band_colors,\n",
    "                  grid=True, show_delay_ref=False, time=swiftxrt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FluxFlux(obj_name,filters,delay_ref,gal_ref,waves,\n",
    "                datadir=datadir,outputdir=outputdir,\n",
    "                burnin=burnin,\n",
    "                band_colors=band_colors,\n",
    "                ebv=0.032,redshift=redshift,\n",
    "                limits=[.1,12])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Disc-only delay spectrum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Eddington luminosity function\n",
    "def lumin(M = (10**8)*1.989e30):\n",
    "    G = 6.6743e-11 #Nm^2/kg^2\n",
    "    # M = (10**8.05)*1.989e30 #kg\n",
    "    c = 299792458 #m/s\n",
    "    mp = 1.67262192*1e-27\n",
    "    sigmaTh = 6.6524587321e-29 #m^2\n",
    "    Ledd = 4*np.pi*G*M*c*mp/sigmaTh\n",
    "    return Ledd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILTERS = [\"UVW2\", \"gp1\", \"gp\", \"V\", \"rp\", \"ip\", \"zs\"]\n",
    "filters = FILTERS\n",
    "waves = [1894, 4770, 5510, 6231, 7625, 9134]\n",
    "redshift = 0.0364\n",
    "band_colors=['#0652DD','#1289A7','#006266','#A3CB38','orange','#EE5A24','brown']\n",
    "delay_ref = 'gp1'\n",
    "outputdir = \"/home/jovyan/PyROA2\"\n",
    "burnin=150000\n",
    "\n",
    "# following functions are adapted from Utils.py accessible at https://github.com/Alymantara/PyROA\n",
    "# Reference: Donnan et al. 2021 (https://ui.adsabs.harvard.edu/abs/2021arXiv210712318D/abstract)\n",
    "def DiscLagSpectrum(filters,delay_ref,wavelengths,\n",
    "                burnin=0,samples_file='samples_flat.obj',\n",
    "                outputdirs = './',\n",
    "                band_colors = None,\n",
    "                redshift=0.0,\n",
    "                savefig=True,figname=None,\n",
    "                dataframe=None, resume=False):\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10,8))\n",
    "    outputdir=outputdirs\n",
    "    color='k'\n",
    "    marker='o'\n",
    "    if outputdir[-1] != '/': outputdir += '/'\n",
    "    file = open(outputdir+samples_file,'rb')\n",
    "    samples = pickle.load(file)[burnin:]\n",
    "\n",
    "    ss = np.where(np.array(filters) == delay_ref)[0][0]\n",
    "    #print(ss)\n",
    "    labels = []\n",
    "    for i in range(len(filters)):\n",
    "        for j in [\"A\", \"B\",r\"$\\tau$\", r\"$\\sigma$\"]:\n",
    "            labels.append(j+r'$_{'+filters[i]+r'}$')\n",
    "    labels.append(r'$\\Delta$')\n",
    "    all_labels = labels.copy()\n",
    "    del labels[ss*4+2]\n",
    "\n",
    "    # To get ONLY lags\n",
    "    shifter = 2\n",
    "\n",
    "    list_only = []\n",
    "    mm = 0\n",
    "    ndim = len(filters)\n",
    "    for i in range(ndim):\n",
    "        if i != ss:\n",
    "            list_only.append(i*4+shifter+mm)\n",
    "        if i == ss:\n",
    "            mm = -1\n",
    "    # Get the \n",
    "    lag,lag_m,lag_p = np.zeros(ndim-1),np.zeros(ndim-1),np.zeros(ndim-1)\n",
    "    for j,i in enumerate(list_only):\n",
    "        #print(i)\n",
    "        q50 = np.percentile(samples[:,i],50)\n",
    "        q84 = np.percentile(samples[:,i],84)\n",
    "        q16 = np.percentile(samples[:,i],16)\n",
    "        lag[j] = q50\n",
    "        lag_m[j] = q50-q16\n",
    "        lag_p[j] = q84-q50\n",
    "\n",
    "    # ax = fig.add_subplot(111)\n",
    "\n",
    "    plt.axhline(y=0,ls='--',alpha=0.5,color='k')\n",
    "\n",
    "    if band_colors == None: band_colors = 'k'*7\n",
    "\n",
    "    mm = 0\n",
    "    for i in range(lag.size):        \n",
    "        plt.errorbar(wavelengths[i]/(1+redshift),lag[i]/(1+redshift),\n",
    "                    yerr=lag_m[i],marker=marker,\n",
    "                    color=color, ms=15)\n",
    "\n",
    "    if redshift > 0:\n",
    "        plt.xlabel(r'Rest Wavelength / $\\mathrm{\\AA}$', fontsize=14)\n",
    "        plt.ylabel(r'$\\tau_{\\rm rest}$ / day', fontsize=14)\n",
    "    else:\n",
    "        plt.xlabel(r'Observed Wavelength / $\\mathrm{\\AA}$')\n",
    "        plt.ylabel(r'$\\tau$ / day')\n",
    "        \n",
    "    delta = round(np.mean([np.median(samples[:, -1]), np.mean(samples[:, -1]), sp.stats.mode(samples[:, -1])[0]]))\n",
    "\n",
    "    ax.lines[-1].set_label(fr'lags, $\\Delta$={delta}')\n",
    "        \n",
    "    lam0 = dataframe.loc[(dataframe['Filter'] == 'gp')]['Wavelength'].values[0]/(1+redshift)\n",
    "    \n",
    "    fitwave = np.array(wavelengths)/(1+redshift)\n",
    "    fitlag = np.array(lag)/(1+redshift)\n",
    "    fiterr = np.array(lag_m)\n",
    "    \n",
    "    ax.axvline(lam0, color='k', linestyle=\"--\", alpha=0.5, zorder=0)\n",
    "    \n",
    "    def alpha(theta, FluxWeight=True, lam0=lam0):\n",
    "        # print(lam0)\n",
    "        logmdotedd = theta[0]\n",
    "        c = 299792458 #m/s\n",
    "        k = 1.3806452e-23 #J/K\n",
    "        lam0 = lam0 * 1e-10 #m\n",
    "        h = 6.62607015e-34 #J/Hz\n",
    "        G = 6.6743e-11 #Nm^2/kg^2\n",
    "        M = (10**8)*1.989e30 #kg\n",
    "        sigma = 5.670374419e-8 #W/m^2K^4\n",
    "        eta = 0.1\n",
    "        kappa = 1\n",
    "        mdotedd = 10**logmdotedd #LBol/LEdd\n",
    "\n",
    "        if FluxWeight == True:\n",
    "            X = 2.49\n",
    "        else:\n",
    "            X=4.97\n",
    "\n",
    "        Ledd = lumin(M)\n",
    "        alpha = (1/c)*(X*k*lam0/(h*c) )**(4/3) * ( (G*M/(8*np.pi*sigma)) * (Ledd/(eta*c**2)) * (3+kappa)*mdotedd )**(1/3)\n",
    "        return alpha/(60*60*24)\n",
    "    \n",
    "    def timelag(theta, lam=fitwave, FluxWeight=True, lam0=lam0):\n",
    "        # A, x, beta, y0, logmdotedd = theta\n",
    "        logmdotedd = theta[0]\n",
    "        a = alpha(theta, FluxWeight=FluxWeight)\n",
    "        return a*((lam/lam0)**(4/3) - 1)\n",
    "    \n",
    "    def model(params, lam=fitwave, lam0=lam0):\n",
    "        tau0=params[0]\n",
    "        return tau0*((lam/lam0)**(4/3) - 1)\n",
    "    \n",
    "    def model_tau(theta, params, lam=fitwave, lam0=lam0):\n",
    "        tau0, syserr=theta\n",
    "        beta, y0=params\n",
    "        return tau0*((lam/lam0)**(beta) - y0)\n",
    "    \n",
    "    def model_X(theta, params, lam=fitwave, lam0=lam0):\n",
    "        logmdotedd, syserr=theta\n",
    "        M, beta, y0, FluxWeight=params\n",
    "        a = alpha(theta, FluxWeight=FluxWeight)\n",
    "        return a*((lam/lam0)**beta - y0)\n",
    "    \n",
    "    def residual_tau(params, x, y, yerr):\n",
    "        tau0 = params['tau0'].value\n",
    "        params=[tau0]\n",
    "        return (y - model(params, lam=x))/yerr\n",
    "    \n",
    "    def residual_X(params, x, y, yerr, FluxWeight):\n",
    "        # M = params['M'].value\n",
    "        logmdotedd = params['logmdotedd'].value\n",
    "        params = np.array([logmdotedd])\n",
    "        return (y - timelag(params, lam=x, FluxWeight=FluxWeight))/yerr\n",
    "    \n",
    "    def lnlike_tau(theta, params, x, y, yerr):\n",
    "        tau0, syserr = theta\n",
    "        beta, y0 = params\n",
    "        # inputs = [tau0, beta, y0, syserr]\n",
    "        return -1/2 * np.sum(((y - model_tau(theta, params, x))/yerr)**2 +np.log(yerr**2+syserr**2) )\n",
    "\n",
    "    def lnprior_tau(theta):\n",
    "        tau0, syserr = theta\n",
    "        if -5.0 < tau0 < 5.0 and -0 < syserr < 5.0:\n",
    "            return 0.0\n",
    "        return -np.inf\n",
    "\n",
    "    def lnprob_tau(theta, params, x, y, yerr):\n",
    "        lp = lnprior_tau(theta)\n",
    "        if not np.isfinite(lp):\n",
    "            return -np.inf\n",
    "        return lp + lnlike_tau(theta, params, x, y, yerr)\n",
    "    \n",
    "    def lnlike_X(theta, params, x, y, yerr):\n",
    "        logmdotedd, syserr = theta\n",
    "        M, beta, y0, FluxWeight = params\n",
    "        # inputs = [tau0, beta, y0, syserr]\n",
    "        return -1/2 * np.sum(((y - model_X(theta, params, x))/yerr)**2 +np.log(yerr**2+syserr**2) )\n",
    "\n",
    "    def lnprior_X(theta):\n",
    "        logmdotedd, syserr = theta\n",
    "        if np.log10(2e-2) < logmdotedd < np.log10(18e-2) and 0 < syserr < 5.0:\n",
    "            return 0.0\n",
    "        return -np.inf\n",
    "\n",
    "    def lnprob_X(theta, params, x, y, yerr):\n",
    "        lp = lnprior_X(theta)\n",
    "        if not np.isfinite(lp):\n",
    "            return -np.inf\n",
    "        return lp + lnlike_X(theta, params, x, y, yerr)\n",
    "    \n",
    "    def fitter2(data, residual, fit_params, model, params, functions, labels, file, nwalkers=500, niter=10000):\n",
    "        lnlike, lnprior, lnprob = functions\n",
    "        \n",
    "        result = lmft.minimize(residual, fit_params, args=data)\n",
    "        \n",
    "        keys = [result.params[key].value for key in result.params]\n",
    "        keys.append(1) #for syserr\n",
    "        \n",
    "        initial = np.array(keys)\n",
    "        ndim = len(initial)\n",
    "        p0 = [np.array(initial) + 1e-7 * np.random.randn(ndim) for i in range(nwalkers)]\n",
    "        \n",
    "        if len(data) == 4:\n",
    "            args = (params, *data[:-1])\n",
    "        else:\n",
    "            args = (params, *data)\n",
    "            \n",
    "        filename = f\"{file}\"\n",
    "        backend = emcee.backends.HDFBackend(filename)\n",
    "        # print(backend.iteration)\n",
    "        # backend.reset(nwalkers, ndim)\n",
    "            \n",
    "        sampler = emcee.EnsembleSampler(nwalkers, ndim, lnprob, args=args, threads=12, backend=backend)\n",
    "        if resume == True:\n",
    "            p0 = None\n",
    "            print(\"true\")\n",
    "            pos, prob, state = sampler.run_mcmc(p0, niter, progress=True)\n",
    "        elif resume == False:\n",
    "            print(\"false\")\n",
    "            p0, _, _ = sampler.run_mcmc(p0, 100, progress=True)\n",
    "            sampler.reset()\n",
    "            pos, prob, state = sampler.run_mcmc(p0, niter, progress=True)\n",
    "        \n",
    "        samples = sampler.flatchain\n",
    "        \n",
    "        # chains(sampler, labels, ndim)\n",
    "        \n",
    "        print(sampler.get_autocorr_time())\n",
    "        \n",
    "        flat_samples = sampler.get_chain(discard=3*round(np.mean(sampler.get_autocorr_time())), thin=15, flat=True)\n",
    "        \n",
    "        # emcee_corner = corner.corner(flat_samples, labels=labels, show_titles=True)\n",
    "        \n",
    "        theta_max = samples[np.argmax(sampler.flatlnprobability)]\n",
    "        \n",
    "        hp = []\n",
    "        stdminus = []\n",
    "        stdplus = []\n",
    "        for i in range(ndim):\n",
    "            mcmc = np.percentile(flat_samples[:,i], [16,50,84])\n",
    "            q = np.diff(mcmc)\n",
    "            hp.append(mcmc[1])\n",
    "            stdminus.append(q[0])\n",
    "            stdplus.append(q[1])\n",
    "        \n",
    "        sig = np.sqrt(fiterr**2 + hp[-1])\n",
    "    \n",
    "        def sample_walkers(model, params, nsamples=10000, flattened_chain=flat_samples):\n",
    "            models=[]\n",
    "            draw = np.floor(np.random.uniform(0, len(flat_samples), size=nsamples)).astype(int)\n",
    "            thetas = flattened_chain[draw]\n",
    "            for i in thetas:\n",
    "                mod = model(i, params)\n",
    "                models.append(mod)\n",
    "            spread = np.std(models, axis=0)\n",
    "            med_model = np.median(models, axis=0)\n",
    "            return med_model, spread\n",
    "        \n",
    "        med_model, spread = sample_walkers(model, params)\n",
    "        # print(len(med_model), len(spread))\n",
    "        \n",
    "        return theta_max, stdminus, stdplus, med_model, spread, sampler, sampler.get_chain(), flat_samples\n",
    "        \n",
    "    \n",
    "    fit_params_tau = lmft.create_params(tau0={'value':3, 'min':0, 'max':10,'vary':True})\n",
    "    fit_params_X = lmft.create_params(logmdotedd={'value':-1.17, 'min':np.log10(2e-2), 'max':np.log10(18e-2), 'vary':True})\n",
    "    \n",
    "    data = (fitwave, fitlag, fiterr)\n",
    "    labels = [r'tau_0', 'syserr']\n",
    "    params = [4/3, 1]\n",
    "    functions = [lnlike_tau, lnprior_tau, lnprob_tau]\n",
    "    file = \"bestdisc2.h5\"\n",
    "    name = \"bestdisc2\"\n",
    "    resume=False\n",
    "    \n",
    "    keys, stdminus, stdplus, med_model, spread, sampler, samples, flat_samples = fitter2(data, residual_tau, fit_params_tau, model_tau, params, functions, labels, file)\n",
    "    taumodel = [keys, stdminus, stdplus, med_model, spread, sampler, samples, flat_samples]\n",
    "    \n",
    "    \n",
    "    df = pd.DataFrame({'keys':keys, 'stdplus':stdplus, 'stdminus':stdminus})\n",
    "    df.to_pickle(f\"{name}_model2.pkl\")\n",
    "    \n",
    "    df = pd.DataFrame({'med_model':med_model, 'spread':spread})\n",
    "    df.to_pickle(f\"{name}_uncertainty2.pkl\")\n",
    "    \n",
    "    values = ['tau0']\n",
    "    \n",
    "    df = pd.DataFrame({'syserr_step':samples[:, :, -1][0], 'syserr': samples[:, :, -1][1]})\n",
    "    for i in range(len(values)):\n",
    "        df.loc[:, fr'{values[i]}_step'] = samples[:, :, i][0]\n",
    "        df.loc[:, fr'{values[i]}'] = samples[:, :, i][1]\n",
    "    df.to_pickle(f\"{name}_samples2.pkl\")\n",
    "    \n",
    "    df = pd.DataFrame({'syserr':flat_samples[:,-1]})\n",
    "    for i in range(len(values)):\n",
    "        df.loc[:, fr'{values[i]}'] = flat_samples[:,i]\n",
    "    df.to_pickle(f\"{name}_flat_samples2.pkl\")\n",
    "    \n",
    "    best_fit_model = model(keys)\n",
    "    \n",
    "    df = pd.DataFrame({'best_fit':best_fit_model})\n",
    "    df.to_pickle(f\"{name}_components2.pkl\")\n",
    "    \n",
    "    \n",
    "    data = (fitwave, fitlag, fiterr, True)\n",
    "    labels = [r'\\log{\\dot{M}_{\\rm Edd}}', 'syserr']\n",
    "    params = [10**8*1.989e30, 4/3, 1, True]\n",
    "    functions = [lnlike_X, lnprior_X, lnprob_X]\n",
    "    file = \"X2492.h5\"\n",
    "    name = \"X2492\"\n",
    "    resume=False\n",
    "    \n",
    "    keys2, stdminus2, stdplus2, med_model2, spread2, sampler2, samples2, flat_samples2 = fitter2(data, residual_X, fit_params_X, model_X, params, functions, labels, file)\n",
    "    X2 = [keys2, stdminus2, stdplus2, med_model2, spread2, sampler2, samples2, flat_samples2]\n",
    "    \n",
    "    df = pd.DataFrame({'keys':keys2, 'stdplus':stdplus2, 'stdminus':stdminus2})\n",
    "    df.to_pickle(f\"{name}_model2.pkl\")\n",
    "    \n",
    "    df = pd.DataFrame({'med_model':med_model2, 'spread':spread2})\n",
    "    df.to_pickle(f\"{name}_uncertainty2.pkl\")\n",
    "    \n",
    "    values = ['logmdotedd']\n",
    "    \n",
    "    df = pd.DataFrame({'syserr_step':samples2[:, :, -1][0], 'syserr': samples2[:, :, -1][1]})\n",
    "    for i in range(len(values)):\n",
    "        df.loc[:, fr'{values[i]}_step'] = samples2[:, :, i][0]\n",
    "        df.loc[:, fr'{values[i]}'] = samples2[:, :, i][1]\n",
    "    df.to_pickle(f\"{name}_samples2.pkl\")\n",
    "    \n",
    "    df = pd.DataFrame({'syserr':flat_samples2[:,-1]})\n",
    "    for i in range(len(values)):\n",
    "        df.loc[:, fr'{values[i]}'] = flat_samples2[:,i]\n",
    "    df.to_pickle(f\"{name}_flat_samples2.pkl\")\n",
    "    \n",
    "    best_fit_model2 = timelag(keys2, FluxWeight=True)\n",
    "    \n",
    "    df = pd.DataFrame({'best_fit':best_fit_model2})\n",
    "    df.to_pickle(f\"{name}_components2.pkl\")\n",
    "    \n",
    "    data = (fitwave, fitlag, fiterr, False)\n",
    "    params = [10**8*1.989e30, 4/3, 1, False]\n",
    "    # file = \"X249.h5\"\n",
    "    file=\"X4972.h5\"\n",
    "    name = \"X4972\"\n",
    "    resume=False\n",
    "    \n",
    "    keys4, stdminus4, stdplus4, med_model4, spread4, sampler4, samples4, flat_samples4 = fitter2(data, residual_X, fit_params_X, model_X, params, functions, labels, file)\n",
    "    X4 = [keys4, stdminus4, stdplus4, med_model4, spread4, sampler4, samples4, flat_samples4]\n",
    "    \n",
    "    df = pd.DataFrame({'keys':keys4, 'stdplus':stdplus4, 'stdminus':stdminus4})\n",
    "    df.to_pickle(f\"{name}_model2.pkl\")\n",
    "    \n",
    "    df = pd.DataFrame({'med_model':med_model4, 'spread':spread4})\n",
    "    df.to_pickle(f\"{name}_uncertainty2.pkl\")\n",
    "    \n",
    "    values = ['logmdotedd']\n",
    "    \n",
    "    df = pd.DataFrame({'syserr_step':samples4[:, :, -1][0], 'syserr': samples4[:, :, -1][1]})\n",
    "    for i in range(len(values)):\n",
    "        df.loc[:, fr'{values[i]}_step'] = samples4[:, :, i][0]\n",
    "        df.loc[:, fr'{values[i]}'] = samples4[:, :, i][1]\n",
    "    df.to_pickle(f\"{name}_samples2.pkl\")\n",
    "    \n",
    "    df = pd.DataFrame({'syserr':flat_samples4[:,-1]})\n",
    "    for i in range(len(values)):\n",
    "        df.loc[:, fr'{values[i]}'] = flat_samples4[:,i]\n",
    "    df.to_pickle(f\"{name}_flat_samples2.pkl\")\n",
    "    \n",
    "    best_fit_model4 = timelag(keys4, FluxWeight=False)\n",
    "    \n",
    "    df = pd.DataFrame({'best_fit':best_fit_model4})\n",
    "    df.to_pickle(f\"{name}_components2.pkl\")\n",
    "    \n",
    "    ax.plot(fitwave, timelag(keys2, FluxWeight=True), label=\"X=2.49 disc expectation\", color='blue')\n",
    "    ax.plot(fitwave, timelag(keys4, FluxWeight=False), label=\"X=4.97 disc expectation\", color='red')\n",
    "    ax.plot(fitwave, model(keys), label=fr\"Highest likelihood disc model, $\\tau_0$={round(keys[0],2)}\", color='green')\n",
    "    \n",
    "    ax.fill_between(fitwave, med_model4-spread4, med_model4+spread4, color='red', alpha=0.2, label=fr\"1 $\\sigma$ spread\")\n",
    "    ax.fill_between(fitwave, med_model2-spread2, med_model2+spread2, color='blue', alpha=0.1, label=fr\"1 $\\sigma$ spread\")\n",
    "    ax.fill_between(fitwave, med_model-spread, med_model+spread, color='green', alpha=0.2, label=fr\"1 $\\sigma$ spread\")\n",
    "    \n",
    "    ax.set_xlim(min(fitwave)-200, max(fitwave)+200)\n",
    "    ax.set_ylim(min(fitlag)*1.1, max(fitlag)*1.1)\n",
    "\n",
    "    lines, labels = ax.get_legend_handles_labels()\n",
    "    \n",
    "    plt.legend([lines[0], lines[1], lines[2], lines[3], (lines[4], lines[5], lines[6])], \n",
    "                 labels, handler_map={tuple: HandlerTuple(ndivide=None)})\n",
    "    plt.savefig(\"delspecdisc2.png\", dpi=300)\n",
    "    plt.show()\n",
    "    return taumodel, X2, X4\n",
    "\n",
    "models = DiscLagSpectrum(filters,delay_ref,outputdirs=outputdir,\n",
    "                burnin=burnin,\n",
    "                band_colors=band_colors,\n",
    "                wavelengths=waves,redshift=redshift,\n",
    "                dataframe = DF_FCCF, resume=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILTERS = [\"UVW2\", \"gp1\", \"gp\", \"V\", \"rp\", \"ip\", \"zs\"]\n",
    "filters = FILTERS\n",
    "waves = [1894, 4770, 5510, 6231, 7625, 9134]\n",
    "redshift = 0.0364\n",
    "band_colors=['#0652DD','#1289A7','#006266','#A3CB38','orange','#EE5A24','brown']\n",
    "delay_ref = 'gp1'\n",
    "outputdir = \"/home/jovyan/PyROA2\"\n",
    "burnin=150000\n",
    "\n",
    "# following functions are adapted from Utils.py accessible at https://github.com/Alymantara/PyROA\n",
    "# Reference: Donnan et al. 2021 (https://ui.adsabs.harvard.edu/abs/2021arXiv210712318D/abstract)\n",
    "def DiscLagSpectrumPlot(filters,delay_ref,wavelengths,\n",
    "                burnin=0,samples_file='samples_flat.obj',\n",
    "                outputdirs = './',\n",
    "                band_colors = None,\n",
    "                redshift=0.0,\n",
    "                savefig=True,figname=None,\n",
    "                dataframe=None, resume=False, add=\"\"):\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(15,10))\n",
    "    outputdir=outputdirs\n",
    "    color='k'\n",
    "    marker='o'\n",
    "    if outputdir[-1] != '/': outputdir += '/'\n",
    "    file = open(outputdir+samples_file,'rb')\n",
    "    samples = pickle.load(file)[burnin:]\n",
    "\n",
    "    ss = np.where(np.array(filters) == delay_ref)[0][0]\n",
    "    #print(ss)\n",
    "    labels = []\n",
    "    for i in range(len(filters)):\n",
    "        for j in [\"A\", \"B\",r\"$\\tau$\", r\"$\\sigma$\"]:\n",
    "            labels.append(j+r'$_{'+filters[i]+r'}$')\n",
    "    labels.append(r'$\\Delta$')\n",
    "    all_labels = labels.copy()\n",
    "    del labels[ss*4+2]\n",
    "\n",
    "    # To get ONLY lags\n",
    "    shifter = 2\n",
    "\n",
    "    list_only = []\n",
    "    mm = 0\n",
    "    ndim = len(filters)\n",
    "    for i in range(ndim):\n",
    "        if i != ss:\n",
    "            list_only.append(i*4+shifter+mm)\n",
    "        if i == ss:\n",
    "            mm = -1\n",
    "    # Get the \n",
    "    lag,lag_m,lag_p = np.zeros(ndim-1),np.zeros(ndim-1),np.zeros(ndim-1)\n",
    "    for j,i in enumerate(list_only):\n",
    "        #print(i)\n",
    "        q50 = np.percentile(samples[:,i],50)\n",
    "        q84 = np.percentile(samples[:,i],84)\n",
    "        q16 = np.percentile(samples[:,i],16)\n",
    "        lag[j] = q50\n",
    "        lag_m[j] = q50-q16\n",
    "        lag_p[j] = q84-q50\n",
    "\n",
    "    # ax = fig.add_subplot(111)\n",
    "\n",
    "    plt.axhline(y=0,ls='--',alpha=0.5,color='k')\n",
    "\n",
    "    if band_colors == None: band_colors = 'k'*7\n",
    "\n",
    "    mm = 0\n",
    "    for i in range(lag.size):        \n",
    "        plt.errorbar(wavelengths[i]/(1+redshift),lag[i]/(1+redshift),\n",
    "                    yerr=lag_m[i],marker=marker,\n",
    "                    color=color, ms=15)\n",
    "\n",
    "    if redshift > 0:\n",
    "        plt.xlabel(r'Rest Wavelength / $\\mathrm{\\AA}$')\n",
    "        plt.ylabel(r'$\\tau_{\\rm rest}$ / day')\n",
    "    else:\n",
    "        plt.xlabel(r'Observed Wavelength / $\\mathrm{\\AA}$')\n",
    "        plt.ylabel(r'$\\tau$ / day')\n",
    "        \n",
    "    delta = round(np.mean([np.median(samples[:, -1]), np.mean(samples[:, -1]), sp.stats.mode(samples[:, -1])[0]]))\n",
    "\n",
    "    ax.lines[-1].set_label(fr'lags, $\\Delta$={delta}')\n",
    "        \n",
    "    lam0 = dataframe.loc[(dataframe['Filter'] == 'gp')]['Wavelength'].values[0]/(1+redshift)\n",
    "    \n",
    "    fitwave = np.array(wavelengths)/(1+redshift)\n",
    "    fitlag = np.array(lag)/(1+redshift)\n",
    "    fiterr = np.array(lag_m)\n",
    "    \n",
    "    ax.axvline(lam0, color='k', linestyle=\"--\", alpha=0.5, zorder=0)\n",
    "    \n",
    "    def alpha(theta, FluxWeight=True, lam0=lam0):\n",
    "        # print(lam0)\n",
    "        logmdotedd = theta[0]\n",
    "        c = 299792458 #m/s\n",
    "        k = 1.3806452e-23 #J/K\n",
    "        lam0 = lam0 * 1e-10 #m\n",
    "        h = 6.62607015e-34 #J/Hz\n",
    "        G = 6.6743e-11 #Nm^2/kg^2\n",
    "        M = (10**8)*1.989e30 #kg\n",
    "        sigma = 5.670374419e-8 #W/m^2K^4\n",
    "        eta = 0.1\n",
    "        kappa = 1\n",
    "        mdotedd = 10**logmdotedd #LBol/LEdd\n",
    "\n",
    "        if FluxWeight == True:\n",
    "            X = 2.49\n",
    "        else:\n",
    "            X=4.97\n",
    "\n",
    "        Ledd = lumin(M)\n",
    "        alpha = (1/c)*(X*k*lam0/(h*c) )**(4/3) * ( (G*M/(8*np.pi*sigma)) * (Ledd/(eta*c**2)) * (3+kappa)*mdotedd )**(1/3)\n",
    "        return alpha/(60*60*24)\n",
    "    \n",
    "    def timelag(theta, lam=fitwave, FluxWeight=True, lam0=lam0):\n",
    "        # A, x, beta, y0, logmdotedd = theta\n",
    "        logmdotedd = theta[0]\n",
    "        a = alpha(theta, FluxWeight=FluxWeight)\n",
    "        return a*((lam/lam0)**(4/3) - 1)\n",
    "    \n",
    "    def model(params, lam=fitwave, lam0=lam0):\n",
    "        tau0=params[0]\n",
    "        return tau0*((lam/lam0)**(4/3) - 1)\n",
    "    \n",
    "    def model_tau(theta, params, lam=fitwave, lam0=lam0):\n",
    "        tau0, syserr=theta\n",
    "        beta, y0=params\n",
    "        return tau0*((lam/lam0)**(beta) - y0)\n",
    "    \n",
    "    def model_X(theta, params, lam=fitwave, lam0=lam0):\n",
    "        logmdotedd, syserr=theta\n",
    "        M, beta, y0, FluxWeight=params\n",
    "        a = alpha(theta, FluxWeight=FluxWeight)\n",
    "        return a*((lam/lam0)**beta - y0)\n",
    "    \n",
    "    def residual_tau(params, x, y, yerr):\n",
    "        tau0 = params['tau0'].value\n",
    "        params=[tau0]\n",
    "        return (y - model(params, lam=x))/yerr\n",
    "    \n",
    "    def residual_X(params, x, y, yerr, FluxWeight):\n",
    "        # M = params['M'].value\n",
    "        logmdotedd = params['logmdotedd'].value\n",
    "        params = np.array([logmdotedd])\n",
    "        return (y - timelag(params, lam=x, FluxWeight=FluxWeight))/yerr\n",
    "    \n",
    "    def lnlike_tau(theta, params, x, y, yerr):\n",
    "        tau0, syserr = theta\n",
    "        beta, y0 = params\n",
    "        # inputs = [tau0, beta, y0, syserr]\n",
    "        return -1/2 * np.sum(((y - model_tau(theta, params, x))/yerr)**2 +np.log(yerr**2+syserr**2) )\n",
    "\n",
    "    def lnprior_tau(theta):\n",
    "        tau0, syserr = theta\n",
    "        if -5.0 < tau0 < 5.0 and -0 < syserr < 5.0:\n",
    "            return 0.0\n",
    "        return -np.inf\n",
    "\n",
    "    def lnprob_tau(theta, params, x, y, yerr):\n",
    "        lp = lnprior_tau(theta)\n",
    "        if not np.isfinite(lp):\n",
    "            return -np.inf\n",
    "        return lp + lnlike_tau(theta, params, x, y, yerr)\n",
    "    \n",
    "    def lnlike_X(theta, params, x, y, yerr):\n",
    "        logmdotedd, syserr = theta\n",
    "        M, beta, y0, FluxWeight = params\n",
    "        # inputs = [tau0, beta, y0, syserr]\n",
    "        return -1/2 * np.sum(((y - model_X(theta, params, x))/yerr)**2 +np.log(yerr**2+syserr**2) )\n",
    "\n",
    "    def lnprior_X(theta):\n",
    "        logmdotedd, syserr = theta\n",
    "        if np.log10(2e-2) < logmdotedd < np.log10(18e-2) and 0 < syserr < 5.0:\n",
    "            return 0.0\n",
    "        return -np.inf\n",
    "\n",
    "    def lnprob_X(theta, params, x, y, yerr):\n",
    "        lp = lnprior_X(theta)\n",
    "        if not np.isfinite(lp):\n",
    "            return -np.inf\n",
    "        return lp + lnlike_X(theta, params, x, y, yerr)\n",
    "    \n",
    "    def fitter2(data, residual, fit_params, model, params, functions, labels, file, nwalkers=500, niter=10000):\n",
    "        lnlike, lnprior, lnprob = functions\n",
    "        \n",
    "        result = lmft.minimize(residual, fit_params, args=data)\n",
    "        \n",
    "        keys = [result.params[key].value for key in result.params]\n",
    "        keys.append(1) #for syserr\n",
    "        \n",
    "        initial = np.array(keys)\n",
    "        ndim = len(initial)\n",
    "        p0 = [np.array(initial) + 1e-7 * np.random.randn(ndim) for i in range(nwalkers)]\n",
    "        \n",
    "        if len(data) == 4:\n",
    "            args = (params, *data[:-1])\n",
    "        else:\n",
    "            args = (params, *data)\n",
    "            \n",
    "        filename = f\"{file}\"\n",
    "        backend = emcee.backends.HDFBackend(filename)\n",
    "        # print(backend.iteration)\n",
    "        # backend.reset(nwalkers, ndim)\n",
    "            \n",
    "        sampler = emcee.EnsembleSampler(nwalkers, ndim, lnprob, args=args, threads=12, backend=backend)\n",
    "        if resume == True:\n",
    "            p0 = None\n",
    "            print(\"true\")\n",
    "            pos, prob, state = sampler.run_mcmc(p0, niter, progress=True)\n",
    "        elif resume == False:\n",
    "            print(\"false\")\n",
    "            p0, _, _ = sampler.run_mcmc(p0, 100, progress=True)\n",
    "            sampler.reset()\n",
    "            pos, prob, state = sampler.run_mcmc(p0, niter, progress=True)\n",
    "        \n",
    "        samples = sampler.flatchain\n",
    "        \n",
    "        # chains(sampler, labels, ndim)\n",
    "        \n",
    "        print(sampler.get_autocorr_time())\n",
    "        \n",
    "        flat_samples = sampler.get_chain(discard=3*round(np.mean(sampler.get_autocorr_time())), thin=15, flat=True)\n",
    "        \n",
    "        # emcee_corner = corner.corner(flat_samples, labels=labels, show_titles=True)\n",
    "        \n",
    "        theta_max = samples[np.argmax(sampler.flatlnprobability)]\n",
    "        \n",
    "        hp = []\n",
    "        stdminus = []\n",
    "        stdplus = []\n",
    "        for i in range(ndim):\n",
    "            mcmc = np.percentile(flat_samples[:,i], [16,50,84])\n",
    "            q = np.diff(mcmc)\n",
    "            hp.append(mcmc[1])\n",
    "            stdminus.append(q[0])\n",
    "            stdplus.append(q[1])\n",
    "        \n",
    "        sig = np.sqrt(fiterr**2 + hp[-1])\n",
    "    \n",
    "        def sample_walkers(model, params, nsamples=10000, flattened_chain=flat_samples):\n",
    "            models=[]\n",
    "            draw = np.floor(np.random.uniform(0, len(flat_samples), size=nsamples)).astype(int)\n",
    "            thetas = flattened_chain[draw]\n",
    "            for i in thetas:\n",
    "                mod = model(i, params)\n",
    "                models.append(mod)\n",
    "            spread = np.std(models, axis=0)\n",
    "            med_model = np.median(models, axis=0)\n",
    "            return med_model, spread\n",
    "        \n",
    "        med_model, spread = sample_walkers(model, params)\n",
    "        # print(len(med_model), len(spread))\n",
    "        \n",
    "        return theta_max, stdminus, stdplus, med_model, spread, sampler, sampler.get_chain(), flat_samples\n",
    "        \n",
    "    \n",
    "    fit_params_tau = lmft.create_params(tau0={'value':3, 'min':0, 'max':10,'vary':True})\n",
    "    fit_params_X = lmft.create_params(logmdotedd={'value':-1.17, 'min':np.log10(2e-2), 'max':np.log10(18e-2), 'vary':True})\n",
    "    \n",
    "    data = (fitwave, fitlag, fiterr)\n",
    "    labels = [r'tau_0', 'syserr']\n",
    "    params = [4/3, 1]\n",
    "    functions = [lnlike_tau, lnprior_tau, lnprob_tau]\n",
    "    file = \"bestdisc2.h5\"\n",
    "    name = \"bestdisc2\"\n",
    "    resume=False\n",
    "    \n",
    "    df = pd.read_pickle(f\"{name}_model{add}.pkl\")\n",
    "    keys = df['keys'].values\n",
    "    stdplus = df['stdplus'].values\n",
    "    stdminus=df['stdminus'].values\n",
    "    \n",
    "    df = pd.read_pickle(f\"{name}_uncertainty{add}.pkl\")\n",
    "    med_model = df['med_model'].values\n",
    "    spread = df['spread'].values\n",
    "    \n",
    "    values = ['tau0', 'syserr']\n",
    "    df = pd.read_pickle(f\"{name}_flat_samples{add}.pkl\")\n",
    "    flat_samples = np.array([df[f'{v}'] for v in values]).T\n",
    "    \n",
    "    df = pd.read_pickle(f\"{name}_samples{add}.pkl\")\n",
    "    samples = []\n",
    "    for i in range(len(values)):\n",
    "        v=values[i]\n",
    "        smpl = np.array(df[f'{v}_step'], df[f'{v}'])\n",
    "        samples.append(smpl)\n",
    "    \n",
    "    df = pd.read_pickle(f\"{name}_components{add}.pkl\")\n",
    "    best_fit_model = df['best_fit'].values\n",
    "        \n",
    "    \n",
    "    upper = []\n",
    "    lower = []\n",
    "    for i in range(len(med_model)):\n",
    "        upper.append(med_model[i]+spread[i])\n",
    "        lower.append(med_model[i]-spread[i])\n",
    "    \n",
    "    # keys, stdminus, stdplus, med_model, spread, sampler, samples, flat_samples = fitter2(data, residual_tau, fit_params_tau, model_tau, params, functions, labels, file)\n",
    "    taumodel = [keys, stdminus, stdplus, med_model, spread, samples, flat_samples]\n",
    "    \n",
    "    \n",
    "    data = (fitwave, fitlag, fiterr, True)\n",
    "    labels = [r'\\log{\\dot{M}_{\\rm Edd}}', 'syserr']\n",
    "    params = [10**8*1.989e30, 4/3, 1, True]\n",
    "    functions = [lnlike_X, lnprior_X, lnprob_X]\n",
    "    file = \"X2492.h5\"\n",
    "    name = \"X2492\"\n",
    "    resume=False\n",
    "    \n",
    "    df = pd.read_pickle(f\"{name}_model{add}.pkl\")\n",
    "    keys2 = df['keys'].values\n",
    "    stdplus2 = df['stdplus'].values\n",
    "    stdminus2=df['stdminus'].values\n",
    "    \n",
    "    df = pd.read_pickle(f\"{name}_uncertainty{add}.pkl\")\n",
    "    med_model2 = df['med_model'].values\n",
    "    spread2 = df['spread'].values\n",
    "    \n",
    "    values = ['logmdotedd', 'syserr']\n",
    "    df = pd.read_pickle(f\"{name}_flat_samples{add}.pkl\")\n",
    "    flat_samples2 = np.array([df[f'{v}'] for v in values]).T\n",
    "    \n",
    "    df = pd.read_pickle(f\"{name}_samples{add}.pkl\")\n",
    "    samples2 = []\n",
    "    for i in range(len(values)):\n",
    "        v=values[i]\n",
    "        smpl = np.array(df[f'{v}_step'], df[f'{v}'])\n",
    "        samples2.append(smpl)\n",
    "    \n",
    "    df = pd.read_pickle(f\"{name}_components{add}.pkl\")\n",
    "    best_fit_model2 = df['best_fit'].values\n",
    "    \n",
    "    upper2 = []\n",
    "    lower2 = []\n",
    "    for i in range(len(med_model2)):\n",
    "        upper2.append(med_model2[i]+spread2[i])\n",
    "        lower2.append(med_model2[i]-spread2[i])\n",
    "    \n",
    "    # keys2, stdminus2, stdplus2, med_model2, spread2, sampler2, samples2, flat_samples2 = fitter2(data, residual_X, fit_params_X, model_X, params, functions, labels, file)\n",
    "    X2 = [keys2, stdminus2, stdplus2, med_model2, spread2, samples2, flat_samples2]\n",
    "    \n",
    "    data = (fitwave, fitlag, fiterr, False)\n",
    "    params = [10**8*1.989e30, 4/3, 1, False]\n",
    "    # file = \"X249.h5\"\n",
    "    file=\"X4972.h5\"\n",
    "    name = \"X4972\"\n",
    "    resume=False\n",
    "    \n",
    "    df = pd.read_pickle(f\"{name}_model{add}.pkl\")\n",
    "    keys4 = df['keys'].values\n",
    "    stdplus4 = df['stdplus'].values\n",
    "    stdminus4=df['stdminus'].values\n",
    "    \n",
    "    df = pd.read_pickle(f\"{name}_uncertainty{add}.pkl\")\n",
    "    med_model4 = df['med_model'].values\n",
    "    spread4 = df['spread'].values\n",
    "    \n",
    "    values = ['logmdotedd', 'syserr']\n",
    "    df = pd.read_pickle(f\"{name}_flat_samples{add}.pkl\")\n",
    "    flat_samples4 = np.array([df[f'{v}'] for v in values]).T\n",
    "    \n",
    "    df = pd.read_pickle(f\"{name}_samples{add}.pkl\")\n",
    "    samples4 = []\n",
    "    for i in range(len(values)):\n",
    "        v=values[i]\n",
    "        smpl = np.array(df[f'{v}_step'], df[f'{v}'])\n",
    "        samples4.append(smpl)\n",
    "    \n",
    "    df = pd.read_pickle(f\"{name}_components{add}.pkl\")\n",
    "    best_fit_model4 = df['best_fit'].values\n",
    "    \n",
    "    upper4 = []\n",
    "    lower4 = []\n",
    "    for i in range(len(med_model4)):\n",
    "        upper4.append(med_model4[i]+spread4[i])\n",
    "        lower4.append(med_model4[i]-spread4[i])\n",
    "    \n",
    "    # keys4, stdminus4, stdplus4, med_model4, spread4, sampler4, samples4, flat_samples4 = fitter2(data, residual_X, fit_params_X, model_X, params, functions, labels, file)\n",
    "    X4 = [keys4, stdminus4, stdplus4, med_model4, spread4, samples4, flat_samples4]\n",
    "    \n",
    "    ax.plot(fitwave, best_fit_model2, label=\"X=2.49 disc expectation\", color='blue')\n",
    "    ax.plot(fitwave, best_fit_model4, label=\"X=4.97 disc expectation\", color='red')\n",
    "    ax.plot(fitwave, best_fit_model, label=fr\"Highest likelihood disc model, $\\tau_0$={round(keys[0],2)}\", color='green')\n",
    "    \n",
    "    ax.fill_between(fitwave, lower4, upper4, color='red', alpha=0.2, label=fr\"1 $\\sigma$ spread\")\n",
    "    ax.fill_between(fitwave, lower2, upper2, color='blue', alpha=0.1, label=fr\"1 $\\sigma$ spread\")\n",
    "    ax.fill_between(fitwave, lower, upper, color='green', alpha=0.2, label=fr\"1 $\\sigma$ spread\")\n",
    "    \n",
    "    ax.set_xlim(min(fitwave)-200, max(fitwave)+200)\n",
    "    ax.set_ylim(min(fitlag)*1.1, max(fitlag)*1.1)\n",
    "\n",
    "    lines, labels = ax.get_legend_handles_labels()\n",
    "    \n",
    "    plt.legend([lines[0], lines[1], lines[2], lines[3], (lines[4], lines[5], lines[6])], \n",
    "                 labels, handler_map={tuple: HandlerTuple(ndivide=None)}, ncol=3, bbox_to_anchor=(1.25,-0.2), fontsize=25)\n",
    "    plt.savefig(\"delspecdisc2.png\", dpi=300, bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "    return taumodel, X2, X4\n",
    "\n",
    "models = DiscLagSpectrumPlot(filters,delay_ref,outputdirs=outputdir,\n",
    "                burnin=burnin,\n",
    "                band_colors=band_colors,\n",
    "                wavelengths=waves,redshift=redshift,\n",
    "                dataframe = DF_FCCF, resume=False, add=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = []\n",
    "dfname = [\"X249\", \"X497\", \"best\"]\n",
    "for i in range(len(models)):\n",
    "    model = models[i]\n",
    "    df = pd.DataFrame([{'theta':model[0], 'stdminus':model[1], 'stdplus':model[2], 'medmod':model[3], 'spread':model[4], 'samples':model[-2], 'flat_samples':model[-1]}])\n",
    "    df.to_csv(fr\"{dfname[i]}.csv\")\n",
    "    dfs.append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tauCompare(filters,delay_ref,wavelengths,\n",
    "                burnin=0,samples_file='samples_flat.obj',\n",
    "                outputdirs = './',\n",
    "                band_colors = None,\n",
    "                redshift=0.0,\n",
    "                savefig=True,figname=None,\n",
    "                dataframe=None, database=None, theta=None,flat_samples=None, params=None):\n",
    "    outputdir=outputdirs\n",
    "    if outputdir[-1] != '/': outputdir += '/'\n",
    "    file = open(outputdir+samples_file,'rb')\n",
    "    samples = pickle.load(file)[burnin:]\n",
    "    ndim = len(filters)\n",
    "    \n",
    "    ss = np.where(np.array(filters) == delay_ref)[0][0]\n",
    "    \n",
    "    shifter = 2\n",
    "\n",
    "    list_only = []\n",
    "    mm = 0\n",
    "    ndim = len(filters)\n",
    "    for i in range(ndim):\n",
    "        if i != ss:\n",
    "            list_only.append(i*4+shifter+mm)\n",
    "        if i == ss:\n",
    "            mm = -1\n",
    "   \n",
    "    lag,lag_m,lag_p = np.zeros(ndim-1),np.zeros(ndim-1),np.zeros(ndim-1)\n",
    "    for j,i in enumerate(list_only):\n",
    "        #print(i)\n",
    "        q50 = np.percentile(samples[:,i],50)\n",
    "        q84 = np.percentile(samples[:,i],84)\n",
    "        q16 = np.percentile(samples[:,i],16)\n",
    "        lag[j] = q50\n",
    "        lag_m[j] = q50-q16\n",
    "        lag_p[j] = q84-q50\n",
    "        \n",
    "    lam0 = dataframe.loc[(dataframe['Filter'] == 'gp')]['Wavelength'].values[0]/(1+redshift)\n",
    "    \n",
    "    fitwave = np.array(wavelengths)/(1+redshift)\n",
    "    fitlag = np.array(lag)/(1+redshift)\n",
    "    fiterr = np.array(lag_m)\n",
    "    \n",
    "    def alpha(theta, FluxWeight=True, lam0=lam0):\n",
    "        logmdotedd = theta[0]\n",
    "        c = 299792458 #m/s\n",
    "        k = 1.3806452e-23 #J/K\n",
    "        lam0 = lam0 * 1e-10 #m\n",
    "        h = 6.62607015e-34 #J/Hz\n",
    "        G = 6.6743e-11 #Nm^2/kg^2\n",
    "        M = (10**8)*1.989e30 #kg\n",
    "        sigma = 5.670374419e-8 #W/m^2K^4\n",
    "        eta = 0.1\n",
    "        kappa = 1\n",
    "        mdotedd = 10**logmdotedd #LBol/LEdd\n",
    "\n",
    "        if FluxWeight == True:\n",
    "            X = 2.49\n",
    "        else:\n",
    "            X=4.97\n",
    "\n",
    "        Ledd = lumin(M)\n",
    "        alpha = (1/c)*(X*k*lam0/(h*c) )**(4/3) * ( (G*M/(8*np.pi*sigma)) * (Ledd/(eta*c**2)) * (3+kappa)*mdotedd )**(1/3)\n",
    "        return alpha/(60*60*24)\n",
    "    \n",
    "    def model_X(theta, params, lam=fitwave, lam0=lam0):\n",
    "        logmdotedd, syserr=theta\n",
    "        M, beta, y0, FluxWeight=params\n",
    "        a = alpha(theta, FluxWeight=FluxWeight)\n",
    "        return a\n",
    "    \n",
    "    def sample_walkers(model, params, nsamples=100, flattened_chain=flat_samples):\n",
    "        models=[]\n",
    "        draw = np.floor(np.random.uniform(0, len(flat_samples), size=nsamples)).astype(int)\n",
    "        thetas = flattened_chain[draw]\n",
    "        for i in thetas:\n",
    "            mod = model(i, params)\n",
    "            models.append(mod)\n",
    "        spread = np.std(models, axis=0)\n",
    "        med_model = np.median(models, axis=0)\n",
    "        return med_model, spread\n",
    "    \n",
    "    med, spread = sample_walkers(model_X, params)\n",
    "    \n",
    "    print('%7.3f +- %7.3f days'%(med, spread))\n",
    "    \n",
    "    return med, spread"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X=2.49\n",
    "params = [10**8*1.989e30, 4/3, 1, True]\n",
    "med2, spread2 = tauCompare(filters,delay_ref,outputdirs=outputdir,\n",
    "           burnin=burnin,\n",
    "           band_colors=band_colors,\n",
    "           wavelengths=waves,redshift=redshift,\n",
    "           dataframe = DF_FCCF, theta = models[1][0][0], flat_samples = models[1][-1], params=params)\n",
    "\n",
    "#X=4.97\n",
    "params = [10**8*1.989e30, 4/3, 1, False]\n",
    "med4, spread4 = tauCompare(filters,delay_ref,outputdirs=outputdir,\n",
    "           burnin=burnin,\n",
    "           band_colors=band_colors,\n",
    "           wavelengths=waves,redshift=redshift,\n",
    "           dataframe = DF_FCCF, theta = models[2][0][0], flat_samples = models[2][-1], params=params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Observed tau0\n",
    "real = dfs[0]['theta'][0][0]\n",
    "minus = dfs[0]['stdminus'][0][0]\n",
    "plus = dfs[0]['stdplus'][0][0]\n",
    "print('%7.3f - %7.1f + %7.3f days'%(real, minus, plus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ratios\n",
    "print(real/med2, real/med2 * np.sqrt((minus/real)**2 + (spread2/med2)**2), real/med4, real/med4 * np.sqrt((minus/real)**2 + (spread4/med4)**2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## disc+BLR lag spectrum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# following functions are adapted from Utils.py accessible at https://github.com/Alymantara/PyROA\n",
    "# Reference: Donnan et al. 2021 (https://ui.adsabs.harvard.edu/abs/2021arXiv210712318D/abstract)\n",
    "def autocorr(samples_flat):\n",
    "\n",
    "    chain = samples_flat[:,:,0].T\n",
    "  # Compute the estimators for a few different chain lengths\n",
    "    N = np.exp(np.linspace(np.log(init_chain_length), np.log(chain.shape[0]), 10)).astype(int)\n",
    "    gw2010 = np.empty(len(N))\n",
    "    new = np.empty(len(N))\n",
    "    for i, n in enumerate(N):\n",
    "            gw2010[i] = autocorr_gw2010(chain[:, :n])\n",
    "            new[i] = autocorr_new(chain[:, :n])\n",
    "\n",
    "    fig = plt.figure(figsize=(8,6))\n",
    "    # Plot the comparisons\n",
    "    plt.loglog(N, gw2010, \"o-\", label=\"G&W 2010\")\n",
    "    plt.loglog(N, new, \"o-\", label=\"new\")\n",
    "    ylim = plt.gca().get_ylim()\n",
    "    plt.plot(N, N / 50., \"--k\", label=r\"$\\tau = N/50$\")\n",
    "    plt.ylim(ylim)\n",
    "    plt.xlabel(\"number of samples, $N$\")\n",
    "    plt.ylabel(r\"$\\tau$ estimates\")\n",
    "    plt.legend(fontsize=14)\n",
    "\n",
    "\n",
    "# Automated windowing procedure following Sokal (1989)\n",
    "def auto_window(taus, c):\n",
    "    m = np.arange(len(taus)) < c * taus\n",
    "    if np.any(m):\n",
    "            return np.argmin(m)\n",
    "    return len(taus) - 1\n",
    "\n",
    "\n",
    "# Following the suggestion from Goodman & Weare (2010)\n",
    "def autocorr_gw2010(y, c=5.0):\n",
    "    f = autocorr_func_1d(np.mean(y, axis=0))\n",
    "    taus = 2.0 * np.cumsum(f) - 1.0\n",
    "    window = auto_window(taus, c)\n",
    "    return taus[window]\n",
    "\n",
    "\n",
    "def autocorr_new(y, c=5.0):\n",
    "    f = np.zeros(y.shape[1])\n",
    "    for yy in y:\n",
    "            f += autocorr_func_1d(yy)\n",
    "    f /= len(y)\n",
    "    taus = 2.0 * np.cumsum(f) - 1.0\n",
    "    window = auto_window(taus, c)\n",
    "    return taus[window]\n",
    "\n",
    "def next_pow_two(n):\n",
    "    i = 1\n",
    "    while i < n:\n",
    "            i = i << 1\n",
    "    return i\n",
    "\n",
    "\n",
    "def autocorr_func_1d(x, norm=True):\n",
    "    x = np.atleast_1d(x)\n",
    "    if len(x.shape) != 1:\n",
    "            raise ValueError(\"invalid dimensions for 1D autocorrelation function\")\n",
    "    n = next_pow_two(len(x))\n",
    "\n",
    "    # Compute the FFT and then (from that) the auto-correlation function\n",
    "    f = np.fft.fft(x - np.mean(x), n=2 * n)\n",
    "    acf = np.fft.ifft(f * np.conjugate(f))[: len(x)].real\n",
    "    acf /= 4 * n\n",
    "\n",
    "    # Optionally normalize\n",
    "    if norm:\n",
    "            acf /= acf[0]\n",
    "\n",
    "    return acf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Eddington luminosity function\n",
    "def lumin(M = (10**8)*1.989e30):\n",
    "    G = 6.6743e-11 #Nm^2/kg^2\n",
    "    # M = (10**8.05)*1.989e30 #kg\n",
    "    c = 299792458 #m/s\n",
    "    mp = 1.67262192*1e-27\n",
    "    sigmaTh = 6.6524587321e-29 #m^2\n",
    "    Ledd = 4*np.pi*G*M*c*mp/sigmaTh\n",
    "    return Ledd*1e7 #erg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILTERS = [\"UVW2\", \"gp1\", \"gp\", \"V\", \"rp\", \"ip\", \"zs\"]\n",
    "filters = FILTERS\n",
    "waves = [1894, 4770, 5510, 6231, 7625, 9134]\n",
    "redshift = 0.0364\n",
    "band_colors=['#0652DD','#1289A7','#006266','#A3CB38','orange','#EE5A24','brown']\n",
    "delay_ref = 'gp1'\n",
    "outputdir = \"/home/jovyan/PyROA2/\"\n",
    "# AS5101/samples_flat.obj\n",
    "burnin=150000\n",
    "def BLRLagTrue(filters,delay_ref,wavelengths,\n",
    "                burnin=0,samples_file='samples_flat.obj',\n",
    "                outputdir = './',\n",
    "                band_colors = None,\n",
    "                redshift=0.0,\n",
    "                savefig=True,figname=None,\n",
    "                dataframe=None, resume=False):\n",
    "    \n",
    "    if outputdir[-1] != '/': outputdir += '/'\n",
    "    file = open(outputdir+samples_file,'rb')\n",
    "    samples = pickle.load(file)[burnin:]\n",
    "\n",
    "    ss = np.where(np.array(filters) == delay_ref)[0][0]\n",
    "    #print(ss)\n",
    "    labels = []\n",
    "    for i in range(len(filters)):\n",
    "        for j in [\"A\", \"B\",r\"$\\tau$\", r\"$\\sigma$\"]:\n",
    "            labels.append(j+r'$_{'+filters[i]+r'}$')\n",
    "    labels.append(r'$\\Delta$')\n",
    "    all_labels = labels.copy()\n",
    "    del labels[ss*4+2]\n",
    "\n",
    "    # To get ONLY lags\n",
    "    shifter = 2\n",
    "\n",
    "    list_only = []\n",
    "    mm = 0\n",
    "    ndim = len(filters)\n",
    "    for i in range(ndim):\n",
    "        if i != ss:\n",
    "            list_only.append(i*4+shifter+mm)\n",
    "        if i == ss:\n",
    "            mm = -1\n",
    "    # Get the \n",
    "    lag,lag_m,lag_p = np.zeros(ndim-1),np.zeros(ndim-1),np.zeros(ndim-1)\n",
    "    for j,i in enumerate(list_only):\n",
    "        #print(i)\n",
    "        q50 = np.percentile(samples[:,i],50)\n",
    "        q84 = np.percentile(samples[:,i],84)\n",
    "        q16 = np.percentile(samples[:,i],16)\n",
    "        lag[j] = q50\n",
    "        lag_m[j] = q50-q16\n",
    "        lag_p[j] = q84-q50\n",
    "    fig, ax = plt.subplots(2, figsize=(17,12), height_ratios=[5,1],sharex=True)\n",
    "    plt.subplots_adjust(wspace=0, hspace=0)\n",
    "\n",
    "    ax[0].axhline(y=0,ls='--',alpha=0.5,color='k')\n",
    "\n",
    "    if band_colors == None: band_colors = 'k'*7\n",
    "    xerr = [584.89/2, 1262.68/2, 840/2, 1149.52/2, 1238.95/2, 994.39/2]\n",
    "    mm = 0\n",
    "    for i in range(lag.size):        \n",
    "        ax[0].errorbar(wavelengths[i]/(1+redshift),lag[i]/(1+redshift),\n",
    "                    yerr=lag_m[i], xerr=xerr[i],marker='o',\n",
    "                    color='k', zorder=100)\n",
    "\n",
    "    if redshift > 0:\n",
    "        ax[1].set_xlabel(r'Rest Wavelength ($\\mathrm{\\AA})$')\n",
    "        ax[0].set_ylabel(r'$\\tau_{\\rm rest}$ (day)')\n",
    "    else:\n",
    "        ax[1].set_xlabel(r'Observed Wavelength ($\\mathrm{\\AA})$')\n",
    "        ax[0].set_ylabel(r'$\\tau$ / day')\n",
    "        \n",
    "    delta = round(np.mean([np.median(samples[:, -1]), np.mean(samples[:, -1]), sp.stats.mode(samples[:, -1])[0]]))\n",
    "        \n",
    "    ax[0].lines[-1].set_label(fr'lags, $\\Delta$={round(delta,2)}')\n",
    "    \n",
    "    lam0 = dataframe.loc[(dataframe['Filter'] == 'gp')]['Wavelength'].values[0]/(1+redshift)\n",
    "    \n",
    "    ax[0].axvline(lam0, color='k', linestyle=\"--\", alpha=0.5, zorder=0)\n",
    "\n",
    "    fitwave = np.array(wavelengths)/(1+redshift)\n",
    "    fitlag = np.array(lag)/(1+redshift)\n",
    "    fiterr = np.array(lag_m)\n",
    "    BLRX = BLR_DF['X'].values/(1+redshift)\n",
    "    BLRY = BLR_DF['Y'].values\n",
    "    XRYX = np.linspace(1, max(BLRX),1000)\n",
    "    \n",
    "    def alpha(theta, FluxWeight=True, lam0=1):\n",
    "        # print(\"alpha\")\n",
    "        \n",
    "        A, x, beta, y0, logmdotedd = theta\n",
    "        \n",
    "        c = 299792458 #m/s\n",
    "        k = 1.3806452e-23 #J/K\n",
    "        lam0 = lam0*1e-10#4770*1e-10 #m\n",
    "        h = 6.62607015e-34 #J/Hz\n",
    "        G = 6.6743e-11 #Nm^2/kg^2\n",
    "        M = (10**8.00)*1.989e30 #kg\n",
    "        sigma = 5.670374419e-8 #W/m^2K^4\n",
    "        eta = 0.1\n",
    "        kappa = 1\n",
    "        mdotedd = 10**logmdotedd\n",
    "        # mdotedd = 9e-2 #LBol/LEdd; from https://academic.oup.com/mnras/article/419/3/2529/1069495\n",
    "        Ledd = lumin(M)/1e7 #W\n",
    "        \n",
    "        if FluxWeight == True:\n",
    "            X = 2.49\n",
    "        else:\n",
    "            X=4.97\n",
    "            \n",
    "        alpha = (1/c)*( X*k*lam0/(h*c) )**(4/3) * ( (G*M/(8*np.pi*sigma)) * (Ledd/(eta*c**2)) * (3+kappa)*mdotedd )**(1/3)\n",
    "        return alpha/(60*60*24)\n",
    "    \n",
    "    \n",
    "    def timelag(theta, lam=fitwave, lam0=1):\n",
    "        # print(\"timelag\")\n",
    "        A, x, beta, y0, logmdotedd = theta\n",
    "        \n",
    "        alph = alpha(theta)\n",
    "        return alph*((lam/lam0)**(beta) - y0), alph\n",
    "    \n",
    "    #for calculating the lag at 4770\n",
    "    def model0(theta, lam=lam0, lam0=lam0, BLRY=BLRY):\n",
    "        # print(0)\n",
    "        A, x, beta, y0, logmdotedd = theta\n",
    "       \n",
    "        powerlaw = timelag(theta, lam=lam)[0] * ((1-x)/(1-A*x))\n",
    "        # print(\"powerlaw0\", powerlaw)\n",
    "        # powerlaw = tau0*((lam/lam0)**(beta)-y0)\n",
    "        \n",
    "        mdotedd=10**logmdotedd\n",
    "        L841 = lumin()*mdotedd\n",
    "        L5548 = lumin(M = 14.22*1e30*1e7)*0.03\n",
    "        # L841 = 5.1720e+43\n",
    "        BLRY = BLRY*(L841/L5548)**0.5\n",
    "        BLRY = BLRY*(((1-A)*x)/(1-A*x))\n",
    "        tck = sp.interpolate.splrep(BLRX, BLRY, k=1)\n",
    "        BLR = sp.interpolate.splev(lam, tck, ext=1)\n",
    "        total = powerlaw+BLR\n",
    "        # print(\"end0\", total)\n",
    "        return total\n",
    "    \n",
    "    #for comparison against wavelength\n",
    "    def model(theta, lam=fitwave, lam0=lam0, idx=0):\n",
    "        # print(\"mod\")\n",
    "        total = modelcal(theta)[idx]\n",
    "        tck = sp.interpolate.splrep(XRYX, total, k=1)\n",
    "        result = sp.interpolate.splev(lam, tck)\n",
    "        # print(\"end mod\")\n",
    "        return result\n",
    "    \n",
    "    def model3(theta, lam=fitwave, lam0=lam0):\n",
    "        A, X, beta, y0, logmdotedd, syserr = theta\n",
    "        inputs = [A, X, beta, y0, logmdotedd]\n",
    "        total = modelcal(inputs)[0]\n",
    "        tck = sp.interpolate.splrep(XRYX, total, k=1)\n",
    "        result = sp.interpolate.splev(lam, tck)\n",
    "        return result\n",
    "    \n",
    "    #for cal\n",
    "    def modelcal(theta, lam=fitwave, lam0=lam0, BLRY=BLRY):\n",
    "        # print(\"cal\")\n",
    "        A, x, beta, y0, logmdotedd = theta\n",
    "\n",
    "        reference = model0(theta)\n",
    "        # print(\"ref\", reference)\n",
    "        powerlaw = timelag(theta)[0] * ((1-x)/(1-A*x))\n",
    "        tck = sp.interpolate.splrep(lam, powerlaw)\n",
    "        attempt = sp.interpolate.splev(XRYX, tck)\n",
    "        # print(\"power\", attempt)\n",
    "        mdotedd=10**logmdotedd\n",
    "        L841 = lumin()*mdotedd\n",
    "        L5548 = lumin(M = 14.22*1e30*1e7)*0.03\n",
    "        # print(L841/L5548)\n",
    "        BLRY = BLRY*(L841/L5548)**0.5\n",
    "        # print(BLRY)\n",
    "        BLRY = BLRY*(((1-A)*x)/(1-A*x))\n",
    "        # print(BLRY)\n",
    "        tck = sp.interpolate.splrep(BLRX, BLRY,k=1)\n",
    "        BLRY = sp.interpolate.splev(XRYX, tck, ext=1)\n",
    "        # print(\"BLRY\", BLRY)\n",
    "        total = attempt+BLRY - reference\n",
    "        # print(\"endcal\", total)\n",
    "        return total, attempt+BLRY\n",
    "    \n",
    "    def modelplt(theta, lam=fitwave, lam0=lam0, BLRY=BLRY):\n",
    "        A, x, beta, y0, logmdotedd = theta\n",
    "\n",
    "        reference = model0(theta)\n",
    "        powerlaw, alph = timelag(theta)\n",
    "        powerlaw = powerlaw * ((1-x)/(1-A*x))\n",
    "        tck = sp.interpolate.splrep(lam, powerlaw)\n",
    "        attempt = sp.interpolate.splev(XRYX, tck)\n",
    "        \n",
    "        mdotedd=10**logmdotedd\n",
    "        L841 = lumin()*mdotedd\n",
    "        L5548 = lumin(M = 14.22*1e30*1e7)*0.03\n",
    "        BLRY = BLRY*(L841/L5548)**0.5\n",
    "        BLRY = BLRY*(((1-A)*x)/(1-A*x))# - BLR_corr\n",
    "        tck = sp.interpolate.splrep(BLRX, BLRY,k=1)\n",
    "        BLRY = sp.interpolate.splev(XRYX, tck, ext=1)\n",
    "                                       \n",
    "        total = attempt+BLRY - reference\n",
    "        return total, attempt-reference, BLRY-reference, reference, attempt, BLRY\n",
    "    \n",
    "    def residual(theta, x, y, yerr):\n",
    "        # print(\"res\")\n",
    "        A = theta['A'].value\n",
    "        X = theta['X'].value\n",
    "        beta = theta['beta'].value\n",
    "        y0 = theta['y0'].value\n",
    "        logmdotedd = theta['logmdotedd'].value\n",
    "        params = np.array([A, X, beta, y0, logmdotedd])\n",
    "        res = (y - model(params, x))/yerr\n",
    "        # print(\"endres\", res)\n",
    "        return res\n",
    "    \n",
    "    def residualplt(theta, x, y, yerr):\n",
    "        res = (y - model(theta, x))/yerr\n",
    "        reserr = 1\n",
    "        return res, reserr\n",
    "    \n",
    "    def lnlike(theta, x, y, yerr):\n",
    "        A, X, beta, y0, logmdotedd, syserr = theta\n",
    "        return -1/2 * np.sum(((y - model3(theta, x))/yerr)**2 +np.log(yerr**2+syserr**2) )\n",
    "\n",
    "    def lnprior(theta):\n",
    "        A, X, beta, y0, logmdotedd, syserr = theta\n",
    "        if 0 < A < 1 and 0 < X < 1 and 4/3-1e-1 < beta < 4/3+1e-1 and 0 < y0 < 1 and np.log10(2e-2) < logmdotedd < np.log10(18e-2) and 0 < syserr < 5.0:\n",
    "            return 0.0\n",
    "        return -np.inf\n",
    "\n",
    "    def lnprob(theta, x, y, yerr):\n",
    "        lp = lnprior(theta)\n",
    "        if not np.isfinite(lp):\n",
    "            return -np.inf\n",
    "        return lp + lnlike(theta, x, y, yerr)\n",
    " \n",
    "    def fitter2(data, residual, fit_params, model, labels, nwalkers=500, niter=200000):\n",
    "        \n",
    "        result = lmft.minimize(residual, fit_params, args=data)\n",
    "        # display(result.params)\n",
    "        keys = [result.params[key].value for key in result.params]\n",
    "        # print(keys)\n",
    "        keys.append(1) #add syserr\n",
    "        # print(keys)\n",
    "        # params = [keys[0], 10**8*1.989e30, 4/3, 1]\n",
    "        \n",
    "        initial = np.array(keys)\n",
    "        ndim = len(initial)\n",
    "        p0 = [np.array(initial) + 1e-7 * np.random.randn(ndim) for i in range(nwalkers)]\n",
    "        \n",
    "        filename = f\"blr_sample.h5\"\n",
    "        backend = emcee.backends.HDFBackend(filename)\n",
    "        # backend.reset(nwalkers, ndim)\n",
    "            \n",
    "        sampler = emcee.EnsembleSampler(nwalkers, ndim, lnprob, args=data, threads=12, backend=backend)\n",
    "        if resume == True:\n",
    "            p0 = None\n",
    "            pos, prob, state = sampler.run_mcmc(p0, 1, progress=True)\n",
    "        else:\n",
    "            p0, _, _ = sampler.run_mcmc(p0, 100, progress=True)\n",
    "            sampler.reset()\n",
    "            pos, prob, state = sampler.run_mcmc(p0, niter, progress=True)\n",
    "        \n",
    "        samples = sampler.flatchain\n",
    "        \n",
    "        # chains(sampler, labels, ndim)\n",
    "        \n",
    "        print(sampler.get_autocorr_time())\n",
    "        \n",
    "        flat_samples = sampler.get_chain(discard=3*round(np.mean(sampler.get_autocorr_time())), thin=15, flat=True)\n",
    "        \n",
    "        # emcee_corner = corner.corner(flat_samples, labels=labels, show_titles=True)\n",
    "        \n",
    "        theta_max = samples[np.argmax(sampler.flatlnprobability)]\n",
    "        \n",
    "        hp = []\n",
    "        stdminus = []\n",
    "        stdplus = []\n",
    "        for i in range(ndim):\n",
    "            mcmc = np.percentile(flat_samples[:,i], [16,50,84])\n",
    "            q = np.diff(mcmc)\n",
    "            hp.append(mcmc[1])\n",
    "            stdminus.append(q[0])\n",
    "            stdplus.append(q[1])\n",
    "        \n",
    "        sig = np.sqrt(fiterr**2 + hp[-1])\n",
    "    \n",
    "        def sample_walkers(nsamples=10000, flattened_chain=flat_samples):\n",
    "            models=[]\n",
    "            powers=[]\n",
    "            BLRS=[]\n",
    "            refs=[]\n",
    "            draw = np.floor(np.random.uniform(0, len(flat_samples), size=nsamples)).astype(int)\n",
    "            thetas = flattened_chain[draw]\n",
    "            for i in thetas:\n",
    "                best_fit_model, power_model, BLR_model, reference, power, BLR = modelplt(i[:-1])\n",
    "                models.append(best_fit_model)\n",
    "                powers.append(power_model)\n",
    "                BLRS.append(BLR_model)\n",
    "                refs.append(reference)\n",
    "            values = [models, powers, BLRS, refs]\n",
    "            spread = [np.std(value, axis=0) for value in values]\n",
    "            med_model = [np.median(value, axis=0) for value in values]\n",
    "            return med_model, spread\n",
    "        \n",
    "        med_model, spread = sample_walkers()\n",
    "        \n",
    "        return theta_max, stdminus, stdplus, med_model, spread, sampler, sampler.get_chain(), flat_samples\n",
    "    \n",
    "    \n",
    "    \n",
    "    fit_params = lmft.create_params(A={'value':0.5, 'min':0, 'max':1,'vary':True},\n",
    "                                    X={'value':0.6, 'min':0, 'max':1,'vary':True},\n",
    "                                    beta={'value':4/3, 'min':4/3-1e-1, 'max':4/3+1e-1,'vary':True},\n",
    "                                    y0={'value':0.94409927, 'min':0, 'max':1, 'vary':True},\n",
    "                                    logmdotedd={'value':-1.17, 'min':np.log10(2e-2), 'max':np.log10(18e-2), 'vary':True})\n",
    "    \n",
    "    data = (fitwave, fitlag, fiterr)\n",
    "    \n",
    "    keys, stdminus, stdplus, med_model, spread, sampler, samples, flat_samples = fitter2(data, residual, fit_params, model3, labels)\n",
    "    bestmodel = [keys, stdminus, stdplus, med_model, spread, sampler, samples, flat_samples]\n",
    "    \n",
    "    best_fit_model, power_model, BLR_model, reference, power, BLR = modelplt(keys[:-1])\n",
    "    \n",
    "    allcomp = [best_fit_model, power_model, BLR_model, reference, power, BLR]\n",
    "    \n",
    "    upper = []\n",
    "    lower = []\n",
    "    for i in range(len(med_model)):\n",
    "        upper.append(med_model[i]+spread[i])\n",
    "        lower.append(med_model[i]-spread[i])\n",
    "    \n",
    "    ax[0].plot(XRYX, best_fit_model, label=\"disc+BLR\", color=\"orange\", zorder=5)\n",
    "    ax[0].plot(XRYX, power_model,linestyle=\"--\", color=\"deeppink\", alpha=0.5, zorder=0, label=\"disc component\")\n",
    "    ax[0].plot(XRYX, BLR_model,linestyle=\"--\", color=\"dodgerblue\", alpha=0.5, zorder=0, label=\"BLR component\")\n",
    "    # ax[0].axhline(reference, linestyle=\"dashdot\", color=\"indigo\", alpha=0.5, zorder=0, label=fr\"$\\lambda_0$ offset\")\n",
    "    \n",
    "    ax[0].fill_between(XRYX, upper[0], lower[0], color=\"orange\", alpha=0.2, zorder=0, label=fr\"$1\\sigma$ spread\")\n",
    "    ax[0].fill_between(XRYX, upper[1], lower[1], color=\"deeppink\", alpha=0.2, zorder=0, label=fr\"$1\\sigma$ spread\")\n",
    "    ax[0].fill_between(XRYX, upper[2], lower[2], color=\"dodgerblue\", alpha=0.2, zorder=0, label=fr\"$1\\sigma$ spread\")\n",
    "    # ax[0].fill_between(XRYX, upper[3], lower[3], color=\"indigo\", alpha=0.2, zorder=0, label=fr\"$1\\sigma$ spread\")\n",
    "    \n",
    "    ax[1].set_ylabel(fr'$\\chi$')\n",
    "    res, reserr = residualplt(keys[:-1], fitwave, fitlag, fiterr)\n",
    "    ax[1].errorbar(fitwave, res,yerr=reserr, marker='o', color=\"k\", ls='none')\n",
    "    ax[1].axhline(0, color='k', linestyle=\"--\", alpha=0.5, zorder=0)\n",
    "    \n",
    "    lines, labels = ax[0].get_legend_handles_labels()\n",
    "    \n",
    "    ax[0].legend([lines[0], lines[1], lines[2], lines[3], (lines[4], lines[5], lines[6])], \n",
    "                 labels, handler_map={tuple: HandlerTuple(ndivide=None)})\n",
    "    ax[0].yaxis.grid(True, which='minor')\n",
    "    plt.minorticks_on()\n",
    "    \n",
    "    df = pd.DataFrame({'keys':keys, 'stdplus':stdplus, 'stdminus':stdminus})\n",
    "    df.to_pickle(f\"BLR_model.pkl\")\n",
    "    \n",
    "    df = pd.DataFrame({'med_model':med_model, 'spread':spread})\n",
    "    df.to_pickle(f\"BLR_uncertainty.pkl\")\n",
    "    \n",
    "    values = ['A', 'X', 'beta', 'y0', 'logmdotedd']\n",
    "    \n",
    "    df = pd.DataFrame({'syserr_step':samples[:, :, -1][0], 'syserr': samples[:, :, -1][1]})\n",
    "    for i in range(len(values)):\n",
    "        df.loc[:, fr'{values[i]}_step'] = samples[:, :, i][0]\n",
    "        df.loc[:, fr'{values[i]}'] = samples[:, :, i][1]\n",
    "    df.to_pickle(f\"BLR_samples.pkl\")\n",
    "    \n",
    "    df = pd.DataFrame({'syserr':flat_samples[:,-1]})\n",
    "    for i in range(len(values)):\n",
    "        df.loc[:, fr'{values[i]}'] = flat_samples[:,i]\n",
    "    df.to_pickle(f\"BLR_flat_samples.pkl\")\n",
    "    \n",
    "    df = pd.DataFrame({'best_fit':best_fit_model, 'power_model':power_model, 'BLR_model':BLR_model, 'power':power, 'BLR': BLR})\n",
    "    df.to_pickle(f\"BLR_components.pkl\")\n",
    "    \n",
    "    df = pd.DataFrame({'res':res, 'reserr':reserr})\n",
    "    df.to_pickle(f\"BLR_residuals.pkl\")\n",
    "    \n",
    "    plt.savefig(\"BLR_fig.png\", dpi=300)\n",
    "    \n",
    "    return bestmodel, allcomp, [res, reserr]\n",
    "\n",
    "outputdir = \"/home/jovyan/PyROA2/\"\n",
    "blr, comp, residuals = BLRLagTrue(filters,delay_ref,outputdir=outputdir,\n",
    "                                burnin=burnin,\n",
    "                                band_colors=band_colors,\n",
    "                                wavelengths=waves,redshift=redshift,\n",
    "                                dataframe = DF_FCCF, resume = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILTERS = [\"UVW2\", \"gp1\", \"gp\", \"V\", \"rp\", \"ip\", \"zs\"]\n",
    "filters = FILTERS\n",
    "waves = [1894, 4770, 5510, 6231, 7625, 9134]\n",
    "redshift = 0.0364\n",
    "band_colors=['#0652DD','#1289A7','#006266','#A3CB38','orange','#EE5A24','brown']\n",
    "delay_ref = 'gp1'\n",
    "outputdir = \"/home/jovyan/PyROA2/\"\n",
    "# AS5101/samples_flat.obj\n",
    "burnin=150000\n",
    "def BLRLagFalse(filters,delay_ref,wavelengths,\n",
    "                burnin=0,samples_file='samples_flat.obj',\n",
    "                outputdir = './',\n",
    "                band_colors = None,\n",
    "                redshift=0.0,\n",
    "                savefig=True,figname=None,\n",
    "                dataframe=None, resume=False):\n",
    "    \n",
    "    if outputdir[-1] != '/': outputdir += '/'\n",
    "    file = open(outputdir+samples_file,'rb')\n",
    "    samples = pickle.load(file)[burnin:]\n",
    "\n",
    "    ss = np.where(np.array(filters) == delay_ref)[0][0]\n",
    "    #print(ss)\n",
    "    labels = []\n",
    "    for i in range(len(filters)):\n",
    "        for j in [\"A\", \"B\",r\"$\\tau$\", r\"$\\sigma$\"]:\n",
    "            labels.append(j+r'$_{'+filters[i]+r'}$')\n",
    "    labels.append(r'$\\Delta$')\n",
    "    all_labels = labels.copy()\n",
    "    del labels[ss*4+2]\n",
    "\n",
    "    # To get ONLY lags\n",
    "    shifter = 2\n",
    "\n",
    "    list_only = []\n",
    "    mm = 0\n",
    "    ndim = len(filters)\n",
    "    for i in range(ndim):\n",
    "        if i != ss:\n",
    "            list_only.append(i*4+shifter+mm)\n",
    "        if i == ss:\n",
    "            mm = -1\n",
    "    # Get the \n",
    "    lag,lag_m,lag_p = np.zeros(ndim-1),np.zeros(ndim-1),np.zeros(ndim-1)\n",
    "    for j,i in enumerate(list_only):\n",
    "        #print(i)\n",
    "        q50 = np.percentile(samples[:,i],50)\n",
    "        q84 = np.percentile(samples[:,i],84)\n",
    "        q16 = np.percentile(samples[:,i],16)\n",
    "        lag[j] = q50\n",
    "        lag_m[j] = q50-q16\n",
    "        lag_p[j] = q84-q50\n",
    "    fig, ax = plt.subplots(2, figsize=(17,12), height_ratios=[5,1],sharex=True)\n",
    "    plt.subplots_adjust(wspace=0, hspace=0)\n",
    "\n",
    "    ax[0].axhline(y=0,ls='--',alpha=0.5,color='k')\n",
    "\n",
    "    if band_colors == None: band_colors = 'k'*7\n",
    "    xerr = [584.89/2, 1262.68/2, 840/2, 1149.52/2, 1238.95/2, 994.39/2]\n",
    "    mm = 0\n",
    "    for i in range(lag.size):        \n",
    "        ax[0].errorbar(wavelengths[i]/(1+redshift),lag[i]/(1+redshift),\n",
    "                    yerr=lag_m[i], xerr=xerr[i],marker='o',\n",
    "                    color='k', zorder=100)\n",
    "\n",
    "    if redshift > 0:\n",
    "        ax[1].set_xlabel(r'Rest Wavelength ($\\mathrm{\\AA})$')\n",
    "        ax[0].set_ylabel(r'$\\tau_{\\rm rest}$ (day)')\n",
    "    else:\n",
    "        ax[1].set_xlabel(r'Observed Wavelength ($\\mathrm{\\AA})$')\n",
    "        ax[0].set_ylabel(r'$\\tau$ / day')\n",
    "        \n",
    "    delta = round(np.mean([np.median(samples[:, -1]), np.mean(samples[:, -1]), sp.stats.mode(samples[:, -1])[0]]))\n",
    "        \n",
    "    ax[0].lines[-1].set_label(fr'lags, $\\Delta$={round(delta,2)}')\n",
    "    \n",
    "    lam0 = dataframe.loc[(dataframe['Filter'] == 'gp')]['Wavelength'].values[0]/(1+redshift)\n",
    "    \n",
    "    ax[0].axvline(lam0, color='k', linestyle=\"--\", alpha=0.5, zorder=0)\n",
    "\n",
    "    fitwave = np.array(wavelengths)/(1+redshift)\n",
    "    fitlag = np.array(lag)/(1+redshift)\n",
    "    fiterr = np.array(lag_m)\n",
    "    BLRX = BLR_DF['X'].values/(1+redshift)\n",
    "    BLRY = BLR_DF['Y'].values\n",
    "    XRYX = np.linspace(1, max(BLRX),1000)\n",
    "    \n",
    "    def alpha(theta, FluxWeight=False, lam0=1):\n",
    "        # print(\"alpha\")\n",
    "        \n",
    "        A, x, beta, y0, logmdotedd = theta\n",
    "        \n",
    "        c = 299792458 #m/s\n",
    "        k = 1.3806452e-23 #J/K\n",
    "        lam0 = lam0*1e-10#4770*1e-10 #m\n",
    "        h = 6.62607015e-34 #J/Hz\n",
    "        G = 6.6743e-11 #Nm^2/kg^2\n",
    "        M = (10**8.00)*1.989e30 #kg\n",
    "        sigma = 5.670374419e-8 #W/m^2K^4\n",
    "        eta = 0.1\n",
    "        kappa = 1\n",
    "        mdotedd = 10**logmdotedd\n",
    "        # mdotedd = 9e-2 #LBol/LEdd; from https://academic.oup.com/mnras/article/419/3/2529/1069495\n",
    "        Ledd = lumin(M)/1e7 #W\n",
    "        \n",
    "        if FluxWeight == True:\n",
    "            X = 2.49\n",
    "        else:\n",
    "            X=4.97\n",
    "            \n",
    "        alpha = (1/c)*( X*k*lam0/(h*c) )**(4/3) * ( (G*M/(8*np.pi*sigma)) * (Ledd/(eta*c**2)) * (3+kappa)*mdotedd )**(1/3)\n",
    "        return alpha/(60*60*24)\n",
    "    \n",
    "    \n",
    "    def timelag(theta, lam=fitwave, lam0=1):\n",
    "        # print(\"timelag\")\n",
    "        A, x, beta, y0, logmdotedd = theta\n",
    "        \n",
    "        alph = alpha(theta)\n",
    "        return alph*((lam/lam0)**(beta) - y0), alph\n",
    "    \n",
    "    #for calculating the lag at 4770\n",
    "    def model0(theta, lam=lam0, lam0=lam0, BLRY=BLRY):\n",
    "        # print(0)\n",
    "        A, x, beta, y0, logmdotedd = theta\n",
    "       \n",
    "        powerlaw = timelag(theta, lam=lam)[0] * ((1-x)/(1-A*x))\n",
    "        # print(\"powerlaw0\", powerlaw)\n",
    "        # powerlaw = tau0*((lam/lam0)**(beta)-y0)\n",
    "        \n",
    "        mdotedd=10**logmdotedd\n",
    "        L841 = lumin()*mdotedd\n",
    "        L5548 = lumin(M = 14.22*1e30*1e7)*0.03\n",
    "        # L841 = 5.1720e+43\n",
    "        BLRY = BLRY*(L841/L5548)**0.5\n",
    "        BLRY = BLRY*(((1-A)*x)/(1-A*x))\n",
    "        tck = sp.interpolate.splrep(BLRX, BLRY, k=1)\n",
    "        BLR = sp.interpolate.splev(lam, tck, ext=1)\n",
    "        total = powerlaw+BLR\n",
    "        # print(\"end0\", total)\n",
    "        return total\n",
    "    \n",
    "    #for comparison against wavelength\n",
    "    def model(theta, lam=fitwave, lam0=lam0, idx=0):\n",
    "        # print(\"mod\")\n",
    "        total = modelcal(theta)[idx]\n",
    "        tck = sp.interpolate.splrep(XRYX, total, k=1)\n",
    "        result = sp.interpolate.splev(lam, tck)\n",
    "        # print(\"end mod\")\n",
    "        return result\n",
    "    \n",
    "    def model3(theta, lam=fitwave, lam0=lam0):\n",
    "        A, X, beta, y0, logmdotedd, syserr = theta\n",
    "        inputs = [A, X, beta, y0, logmdotedd]\n",
    "        total = modelcal(inputs)[0]\n",
    "        tck = sp.interpolate.splrep(XRYX, total, k=1)\n",
    "        result = sp.interpolate.splev(lam, tck)\n",
    "        return result\n",
    "    \n",
    "    #for cal\n",
    "    def modelcal(theta, lam=fitwave, lam0=lam0, BLRY=BLRY):\n",
    "        # print(\"cal\")\n",
    "        A, x, beta, y0, logmdotedd = theta\n",
    "\n",
    "        reference = model0(theta)\n",
    "        # print(\"ref\", reference)\n",
    "        powerlaw = timelag(theta)[0] * ((1-x)/(1-A*x))\n",
    "        tck = sp.interpolate.splrep(lam, powerlaw)\n",
    "        attempt = sp.interpolate.splev(XRYX, tck)\n",
    "        # print(\"power\", attempt)\n",
    "        mdotedd=10**logmdotedd\n",
    "        L841 = lumin()*mdotedd\n",
    "        L5548 = lumin(M = 14.22*1e30*1e7)*0.03\n",
    "        # print(L841/L5548)\n",
    "        BLRY = BLRY*(L841/L5548)**0.5\n",
    "        # print(BLRY)\n",
    "        BLRY = BLRY*(((1-A)*x)/(1-A*x))\n",
    "        # print(BLRY)\n",
    "        tck = sp.interpolate.splrep(BLRX, BLRY,k=1)\n",
    "        BLRY = sp.interpolate.splev(XRYX, tck, ext=1)\n",
    "        # print(\"BLRY\", BLRY)\n",
    "        total = attempt+BLRY - reference\n",
    "        # print(\"endcal\", total)\n",
    "        return total, attempt+BLRY\n",
    "    \n",
    "    def modelplt(theta, lam=fitwave, lam0=lam0, BLRY=BLRY):\n",
    "        A, x, beta, y0, logmdotedd = theta\n",
    "\n",
    "        reference = model0(theta)\n",
    "        powerlaw, alph = timelag(theta)\n",
    "        powerlaw = powerlaw * ((1-x)/(1-A*x))\n",
    "        tck = sp.interpolate.splrep(lam, powerlaw)\n",
    "        attempt = sp.interpolate.splev(XRYX, tck)\n",
    "        \n",
    "        mdotedd=10**logmdotedd\n",
    "        L841 = lumin()*mdotedd\n",
    "        L5548 = lumin(M = 14.22*1e30*1e7)*0.03\n",
    "        BLRY = BLRY*(L841/L5548)**0.5\n",
    "        BLRY = BLRY*(((1-A)*x)/(1-A*x))# - BLR_corr\n",
    "        tck = sp.interpolate.splrep(BLRX, BLRY,k=1)\n",
    "        BLRY = sp.interpolate.splev(XRYX, tck, ext=1)\n",
    "                                       \n",
    "        total = attempt+BLRY - reference\n",
    "        return total, attempt-reference, BLRY-reference, reference, attempt, BLRY\n",
    "    \n",
    "    def residual(theta, x, y, yerr):\n",
    "        # print(\"res\")\n",
    "        A = theta['A'].value\n",
    "        X = theta['X'].value\n",
    "        beta = theta['beta'].value\n",
    "        y0 = theta['y0'].value\n",
    "        logmdotedd = theta['logmdotedd'].value\n",
    "        params = np.array([A, X, beta, y0, logmdotedd])\n",
    "        res = (y - model(params, x))/yerr\n",
    "        # print(\"endres\", res)\n",
    "        return res\n",
    "    \n",
    "    def residualplt(theta, x, y, yerr):\n",
    "        res = (y - model(theta, x))/yerr\n",
    "        reserr = 1\n",
    "        return res, reserr\n",
    "    \n",
    "    def lnlike(theta, x, y, yerr):\n",
    "        A, X, beta, y0, logmdotedd, syserr = theta\n",
    "        return -1/2 * np.sum(((y - model3(theta, x))/yerr)**2 +np.log(yerr**2+syserr**2) )\n",
    "\n",
    "    def lnprior(theta):\n",
    "        A, X, beta, y0, logmdotedd, syserr = theta\n",
    "        if 0 < A < 1 and 0 < X < 1 and 4/3-1e-1 < beta < 4/3+1e-1 and 0 < y0 < 1 and np.log10(2e-2) < logmdotedd < np.log10(18e-2) and 0 < syserr < 5.0:\n",
    "            return 0.0\n",
    "        return -np.inf\n",
    "\n",
    "    def lnprob(theta, x, y, yerr):\n",
    "        lp = lnprior(theta)\n",
    "        if not np.isfinite(lp):\n",
    "            return -np.inf\n",
    "        return lp + lnlike(theta, x, y, yerr)\n",
    " \n",
    "    def fitter2(data, residual, fit_params, model, labels, nwalkers=500, niter=200000):\n",
    "        \n",
    "        result = lmft.minimize(residual, fit_params, args=data)\n",
    "        # display(result.params)\n",
    "        keys = [result.params[key].value for key in result.params]\n",
    "        # print(keys)\n",
    "        keys.append(1) #add syserr\n",
    "        # print(keys)\n",
    "        # params = [keys[0], 10**8*1.989e30, 4/3, 1]\n",
    "        \n",
    "        initial = np.array(keys)\n",
    "        ndim = len(initial)\n",
    "        p0 = [np.array(initial) + 1e-7 * np.random.randn(ndim) for i in range(nwalkers)]\n",
    "        \n",
    "        filename = f\"blr_sample.h5\"\n",
    "        backend = emcee.backends.HDFBackend(filename)\n",
    "        # backend.reset(nwalkers, ndim)\n",
    "            \n",
    "        sampler = emcee.EnsembleSampler(nwalkers, ndim, lnprob, args=data, threads=12, backend=backend)\n",
    "        if resume == True:\n",
    "            p0 = None\n",
    "            pos, prob, state = sampler.run_mcmc(p0, 1, progress=True)\n",
    "        else:\n",
    "            p0, _, _ = sampler.run_mcmc(p0, 100, progress=True)\n",
    "            sampler.reset()\n",
    "            pos, prob, state = sampler.run_mcmc(p0, niter, progress=True)\n",
    "        \n",
    "        samples = sampler.flatchain\n",
    "        \n",
    "        # chains(sampler, labels, ndim)\n",
    "        \n",
    "        print(sampler.get_autocorr_time())\n",
    "        \n",
    "        flat_samples = sampler.get_chain(discard=3*round(np.mean(sampler.get_autocorr_time())), thin=15, flat=True)\n",
    "        \n",
    "        # emcee_corner = corner.corner(flat_samples, labels=labels, show_titles=True)\n",
    "        \n",
    "        theta_max = samples[np.argmax(sampler.flatlnprobability)]\n",
    "        \n",
    "        hp = []\n",
    "        stdminus = []\n",
    "        stdplus = []\n",
    "        for i in range(ndim):\n",
    "            mcmc = np.percentile(flat_samples[:,i], [16,50,84])\n",
    "            q = np.diff(mcmc)\n",
    "            hp.append(mcmc[1])\n",
    "            stdminus.append(q[0])\n",
    "            stdplus.append(q[1])\n",
    "        \n",
    "        sig = np.sqrt(fiterr**2 + hp[-1])\n",
    "    \n",
    "        def sample_walkers(nsamples=10000, flattened_chain=flat_samples):\n",
    "            models=[]\n",
    "            powers=[]\n",
    "            BLRS=[]\n",
    "            refs=[]\n",
    "            draw = np.floor(np.random.uniform(0, len(flat_samples), size=nsamples)).astype(int)\n",
    "            thetas = flattened_chain[draw]\n",
    "            for i in thetas:\n",
    "                best_fit_model, power_model, BLR_model, reference, power, BLR = modelplt(i[:-1])\n",
    "                models.append(best_fit_model)\n",
    "                powers.append(power_model)\n",
    "                BLRS.append(BLR_model)\n",
    "                refs.append(reference)\n",
    "            values = [models, powers, BLRS, refs]\n",
    "            spread = [np.std(value, axis=0) for value in values]\n",
    "            med_model = [np.median(value, axis=0) for value in values]\n",
    "            return med_model, spread\n",
    "        \n",
    "        med_model, spread = sample_walkers()\n",
    "        \n",
    "        return theta_max, stdminus, stdplus, med_model, spread, sampler, sampler.get_chain(), flat_samples\n",
    "    \n",
    "    \n",
    "    \n",
    "    fit_params = lmft.create_params(A={'value':0.5, 'min':0, 'max':1,'vary':True},\n",
    "                                    X={'value':0.6, 'min':0, 'max':1,'vary':True},\n",
    "                                    beta={'value':4/3, 'min':4/3-1e-1, 'max':4/3+1e-1,'vary':True},\n",
    "                                    y0={'value':0.94409927, 'min':0, 'max':1, 'vary':True},\n",
    "                                    logmdotedd={'value':-1.17, 'min':np.log10(2e-2), 'max':np.log10(18e-2), 'vary':True})\n",
    "    \n",
    "    data = (fitwave, fitlag, fiterr)\n",
    "    \n",
    "    keys, stdminus, stdplus, med_model, spread, sampler, samples, flat_samples = fitter2(data, residual, fit_params, model3, labels)\n",
    "    bestmodel = [keys, stdminus, stdplus, med_model, spread, sampler, samples, flat_samples]\n",
    "    \n",
    "    best_fit_model, power_model, BLR_model, reference, power, BLR = modelplt(keys[:-1])\n",
    "    \n",
    "    allcomp = [best_fit_model, power_model, BLR_model, reference, power, BLR]\n",
    "    \n",
    "    upper = []\n",
    "    lower = []\n",
    "    for i in range(len(med_model)):\n",
    "        upper.append(med_model[i]+spread[i])\n",
    "        lower.append(med_model[i]-spread[i])\n",
    "    \n",
    "    ax[0].plot(XRYX, best_fit_model, label=\"disc+BLR\", color=\"orange\", zorder=5)\n",
    "    ax[0].plot(XRYX, power_model,linestyle=\"--\", color=\"deeppink\", alpha=0.5, zorder=0, label=\"disc component\")\n",
    "    ax[0].plot(XRYX, BLR_model,linestyle=\"--\", color=\"dodgerblue\", alpha=0.5, zorder=0, label=\"BLR component\")\n",
    "    # ax[0].axhline(reference, linestyle=\"dashdot\", color=\"indigo\", alpha=0.5, zorder=0, label=fr\"$\\lambda_0$ offset\")\n",
    "    \n",
    "    ax[0].fill_between(XRYX, upper[0], lower[0], color=\"orange\", alpha=0.2, zorder=0, label=fr\"$1\\sigma$ spread\")\n",
    "    ax[0].fill_between(XRYX, upper[1], lower[1], color=\"deeppink\", alpha=0.2, zorder=0, label=fr\"$1\\sigma$ spread\")\n",
    "    ax[0].fill_between(XRYX, upper[2], lower[2], color=\"dodgerblue\", alpha=0.2, zorder=0, label=fr\"$1\\sigma$ spread\")\n",
    "    # ax[0].fill_between(XRYX, upper[3], lower[3], color=\"indigo\", alpha=0.2, zorder=0, label=fr\"$1\\sigma$ spread\")\n",
    "    \n",
    "    ax[1].set_ylabel(fr'$\\chi$')\n",
    "    res, reserr = residualplt(keys[:-1], fitwave, fitlag, fiterr)\n",
    "    ax[1].errorbar(fitwave, res,yerr=reserr, marker='o', color=\"k\", ls='none')\n",
    "    ax[1].axhline(0, color='k', linestyle=\"--\", alpha=0.5, zorder=0)\n",
    "    \n",
    "    lines, labels = ax[0].get_legend_handles_labels()\n",
    "    \n",
    "    ax[0].legend([lines[0], lines[1], lines[2], lines[3], (lines[4], lines[5], lines[6])], \n",
    "                 labels, handler_map={tuple: HandlerTuple(ndivide=None)})\n",
    "    ax[0].yaxis.grid(True, which='minor')\n",
    "    plt.minorticks_on()\n",
    "    \n",
    "    df = pd.DataFrame({'keys':keys, 'stdplus':stdplus, 'stdminus':stdminus})\n",
    "    df.to_pickle(f\"BLR_model.pkl\")\n",
    "    \n",
    "    df = pd.DataFrame({'med_model':med_model, 'spread':spread})\n",
    "    df.to_pickle(f\"BLR_uncertainty.pkl\")\n",
    "    \n",
    "    values = ['A', 'X', 'beta', 'y0', 'logmdotedd']\n",
    "    \n",
    "    df = pd.DataFrame({'syserr_step':samples[:, :, -1][0], 'syserr': samples[:, :, -1][1]})\n",
    "    for i in range(len(values)):\n",
    "        df.loc[:, fr'{values[i]}_step'] = samples[:, :, i][0]\n",
    "        df.loc[:, fr'{values[i]}'] = samples[:, :, i][1]\n",
    "    df.to_pickle(f\"BLR_samples.pkl\")\n",
    "    \n",
    "    df = pd.DataFrame({'syserr':flat_samples[:,-1]})\n",
    "    for i in range(len(values)):\n",
    "        df.loc[:, fr'{values[i]}'] = flat_samples[:,i]\n",
    "    df.to_pickle(f\"BLR_flat_samples.pkl\")\n",
    "    \n",
    "    df = pd.DataFrame({'best_fit':best_fit_model, 'power_model':power_model, 'BLR_model':BLR_model, 'power':power, 'BLR': BLR})\n",
    "    df.to_pickle(f\"BLR_components.pkl\")\n",
    "    \n",
    "    df = pd.DataFrame({'res':res, 'reserr':reserr})\n",
    "    df.to_pickle(f\"BLR_residuals.pkl\")\n",
    "    \n",
    "    plt.savefig(\"BLR_fig.png\", dpi=300)\n",
    "    \n",
    "    return bestmodel, allcomp, [res, reserr]\n",
    "\n",
    "outputdir = \"/home/jovyan/PyROA2/\"\n",
    "blr, comp, residuals = BLRLagFalse(filters,delay_ref,outputdir=outputdir,\n",
    "                                burnin=burnin,\n",
    "                                band_colors=band_colors,\n",
    "                                wavelengths=waves,redshift=redshift,\n",
    "                                dataframe = DF_FCCF, resume = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILTERS = [\"UVW2\", \"gp1\", \"gp\", \"V\", \"rp\", \"ip\", \"zs\"]\n",
    "filters = FILTERS\n",
    "waves = [1894, 4770, 5510, 6231, 7625, 9134]\n",
    "redshift = 0.0364\n",
    "band_colors=['#0652DD','#1289A7','#006266','#A3CB38','orange','#EE5A24','brown']\n",
    "delay_ref = 'gp1'\n",
    "outputdir = \"/home/jovyan/PyROA2/\"\n",
    "# AS5101/samples_flat.obj\n",
    "burnin=150000\n",
    "def BLRLagPlot(filters,delay_ref,wavelengths,\n",
    "                burnin=0,samples_file='samples_flat.obj',\n",
    "                outputdir = './',\n",
    "                band_colors = None,\n",
    "                redshift=0.0,\n",
    "                savefig=True,figname=None,\n",
    "                dataframe=None, name=None, add=\"\"):\n",
    "    \n",
    "    if outputdir[-1] != '/': outputdir += '/'\n",
    "    file = open(outputdir+samples_file,'rb')\n",
    "    samples = pickle.load(file)[burnin:]\n",
    "\n",
    "    ss = np.where(np.array(filters) == delay_ref)[0][0]\n",
    "    #print(ss)\n",
    "    labels = []\n",
    "    for i in range(len(filters)):\n",
    "        for j in [\"A\", \"B\",r\"$\\tau$\", r\"$\\sigma$\"]:\n",
    "            labels.append(j+r'$_{'+filters[i]+r'}$')\n",
    "    labels.append(r'$\\Delta$')\n",
    "    all_labels = labels.copy()\n",
    "    del labels[ss*4+2]\n",
    "\n",
    "    # To get ONLY lags\n",
    "    shifter = 2\n",
    "\n",
    "    list_only = []\n",
    "    mm = 0\n",
    "    ndim = len(filters)\n",
    "    for i in range(ndim):\n",
    "        if i != ss:\n",
    "            list_only.append(i*4+shifter+mm)\n",
    "        if i == ss:\n",
    "            mm = -1\n",
    "    # Get the \n",
    "    lag,lag_m,lag_p = np.zeros(ndim-1),np.zeros(ndim-1),np.zeros(ndim-1)\n",
    "    for j,i in enumerate(list_only):\n",
    "        #print(i)\n",
    "        q50 = np.percentile(samples[:,i],50)\n",
    "        q84 = np.percentile(samples[:,i],84)\n",
    "        q16 = np.percentile(samples[:,i],16)\n",
    "        lag[j] = q50\n",
    "        lag_m[j] = q50-q16\n",
    "        lag_p[j] = q84-q50\n",
    "    fig, ax = plt.subplots(2, figsize=(15,10), height_ratios=[5,1],sharex=True)\n",
    "    plt.subplots_adjust(wspace=0, hspace=0)\n",
    "\n",
    "    ax[0].axhline(y=0,ls='--',alpha=0.5,color='k')\n",
    "\n",
    "    if band_colors == None: band_colors = 'k'*7\n",
    "    xerr = [584.89/2, 1262.68/2, 840/2, 1149.52/2, 1238.95/2, 994.39/2]\n",
    "    mm = 0\n",
    "    for i in range(lag.size):        \n",
    "        ax[0].errorbar(wavelengths[i]/(1+redshift),lag[i]/(1+redshift),\n",
    "                    yerr=lag_m[i], xerr=xerr[i],marker='o',\n",
    "                    color='k', zorder=100)\n",
    "\n",
    "    if redshift > 0:\n",
    "        ax[1].set_xlabel(r'Rest Wavelength ($\\mathrm{\\AA})$')\n",
    "        ax[0].set_ylabel(r'$\\tau_{\\rm rest}$ (day)')\n",
    "    else:\n",
    "        ax[1].set_xlabel(r'Observed Wavelength ($\\mathrm{\\AA})$')\n",
    "        ax[0].set_ylabel(r'$\\tau$ / day')\n",
    "        \n",
    "    delta = round(np.mean([np.median(samples[:, -1]), np.mean(samples[:, -1]), sp.stats.mode(samples[:, -1])[0]]))\n",
    "        \n",
    "    ax[0].lines[-1].set_label(fr'lags, $\\Delta$={round(delta,2)}')\n",
    "    \n",
    "    lam0 = dataframe.loc[(dataframe['Filter'] == 'gp')]['Wavelength'].values[0]/(1+redshift)\n",
    "    \n",
    "    ax[0].axvline(lam0, color='k', linestyle=\"--\", alpha=0.5, zorder=0)\n",
    "\n",
    "    fitwave = np.array(wavelengths)/(1+redshift)\n",
    "    fitlag = np.array(lag)/(1+redshift)\n",
    "    fiterr = np.array(lag_m)\n",
    "    BLRX = BLR_DF['X'].values/(1+redshift)\n",
    "    BLRY = BLR_DF['Y'].values\n",
    "    XRYX = np.linspace(1, max(BLRX),1000)\n",
    "    \n",
    "    df = pd.read_pickle(f\"{name}_model{add}.pkl\")\n",
    "    keys = df['keys'].values\n",
    "    stdplus = df['stdplus'].values\n",
    "    stdminus=df['stdminus'].values\n",
    "    \n",
    "    df = pd.read_pickle(f\"{name}_uncertainty{add}.pkl\")\n",
    "    med_model = df['med_model'].values\n",
    "    spread = df['spread'].values\n",
    "    \n",
    "    values = ['A', 'X', 'beta', 'y0', 'logmdotedd', 'syserr']\n",
    "    df = pd.read_pickle(f\"{name}_flat_samples{add}.pkl\")\n",
    "    flat_samples = np.array([df[f'{v}'] for v in values]).T\n",
    "    \n",
    "    df = pd.read_pickle(f\"{name}_samples{add}.pkl\")\n",
    "    samples = []\n",
    "    for i in range(len(values)-1):\n",
    "        v=values[i]\n",
    "        smpl = np.array(df[f'{v}_step'], df[f'{v}'])\n",
    "        samples.append(smpl)\n",
    "    samples.append(np.array(df['step'], df['syserr']))\n",
    "    \n",
    "    df = pd.read_pickle(f\"{name}_components{add}.pkl\")\n",
    "    best_fit_model = df['best_fit'].values\n",
    "    power_model = df['power_model'].values\n",
    "    BLR_model = df['BLR_model'].values\n",
    "    power = df['power'].values\n",
    "    BLR = df['BLR'].values\n",
    "    \n",
    "    df = pd.read_pickle(f\"{name}_residuals{add}.pkl\")\n",
    "    res = df['res'].values\n",
    "    reserr = df['reserr'].values   \n",
    "    \n",
    "    upper = []\n",
    "    lower = []\n",
    "    for i in range(len(med_model)):\n",
    "        upper.append(med_model[i]+spread[i])\n",
    "        lower.append(med_model[i]-spread[i])\n",
    "    \n",
    "    ax[0].plot(XRYX, best_fit_model, label=\"disc+BLR\", color=\"orange\", zorder=5)\n",
    "    ax[0].plot(XRYX, power_model,linestyle=\"--\", color=\"deeppink\", alpha=0.5, zorder=0, label=\"disc component\")\n",
    "    ax[0].plot(XRYX, BLR_model,linestyle=\"--\", color=\"dodgerblue\", alpha=0.5, zorder=0, label=\"BLR component\")\n",
    "    # ax[0].axhline(reference, linestyle=\"dashdot\", color=\"indigo\", alpha=0.5, zorder=0, label=fr\"$\\lambda_0$ offset\")\n",
    "    \n",
    "    ax[0].fill_between(XRYX, upper[0], lower[0], color=\"orange\", alpha=0.2, zorder=0, label=fr\"$1\\sigma$ spread\")\n",
    "    ax[0].fill_between(XRYX, upper[1], lower[1], color=\"deeppink\", alpha=0.2, zorder=0, label=fr\"$1\\sigma$ spread\")\n",
    "    ax[0].fill_between(XRYX, upper[2], lower[2], color=\"dodgerblue\", alpha=0.2, zorder=0, label=fr\"$1\\sigma$ spread\")\n",
    "    # ax[0].fill_between(XRYX, upper[3], lower[3], color=\"indigo\", alpha=0.2, zorder=0, label=fr\"$1\\sigma$ spread\")\n",
    "    \n",
    "    ax[1].set_ylabel(fr'$\\chi$')\n",
    "    ax[1].errorbar(fitwave, res,yerr=reserr, marker='o', color=\"k\", ls='none')\n",
    "    ax[1].axhline(0, color='k', linestyle=\"--\", alpha=0.5, zorder=0)\n",
    "    \n",
    "    lines, labels = ax[0].get_legend_handles_labels()\n",
    "    \n",
    "    ax[0].legend([lines[0], lines[1], lines[2], lines[3], (lines[4], lines[5], lines[6])], \n",
    "                 labels, handler_map={tuple: HandlerTuple(ndivide=None)}, ncol=5, bbox_to_anchor=(0,0))\n",
    "    ax[0].yaxis.grid(True, which='minor')\n",
    "    plt.minorticks_on()\n",
    "    \n",
    "    bestmodel = [keys, stdminus, stdplus, med_model, spread, samples, flat_samples]\n",
    "    allcomp = [best_fit_model, power_model, BLR_model, power, BLR]\n",
    "    \n",
    "    plt.savefig(\"BLR_fig.png\", dpi=300)\n",
    "    \n",
    "    labels=['A', 'x', r'$\\beta$', r'$y_0$', r'$\\log{\\dot{M}_{\\rm Edd}}$', 'syserr']\n",
    "    \n",
    "    return bestmodel, allcomp, [res, reserr]\n",
    "\n",
    "outputdir = \"/home/jovyan/PyROA2/\"\n",
    "blr, comp, residuals = BLRLagPlot(filters,delay_ref,outputdir=outputdir,\n",
    "                                burnin=burnin,\n",
    "                                band_colors=band_colors,\n",
    "                                wavelengths=waves,redshift=redshift,\n",
    "                                dataframe = DF_FCCF, name=\"BLR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mathdisplay(keys, stdminus, stdplus, labels):\n",
    "    for i in range(len(keys)):\n",
    "        txt = \"\\mathrm{{{3}}} = {0:.3f}_{{-{1:.3f}}}^{{{2:.3f}}}\"\n",
    "        txt = txt.format(keys[i], stdminus[i], stdplus[i], labels[i])\n",
    "        display(Math(txt))\n",
    "        \n",
    "    print(keys[4]/(10**8 * 1.989e30))\n",
    "    print(stdminus[4]/(10**8 * 1.989e30))\n",
    "    print(stdplus[4]/(10**8 * 1.989e30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cornerplot(name, labels, add=\"\", burnin=6540000):\n",
    "    values = ['A', 'X', 'beta', 'y0', 'logmdotedd']\n",
    "    df = pd.read_pickle(f\"{name}_flat_samples{add}.pkl\")\n",
    "    flat_samples = np.array([df[f'{v}'] for v in values]).T[burnin:]\n",
    "    emcee_corner = corner.corner(flat_samples, labels=labels, show_titles=True, quantiles=(0.16, 0.5, 0.84), title_kwargs={'fontsize':15}, levels=(0.5, 1, 2, 3))\n",
    "    plt.savefig(f\"{name}_corner{add}.png\", dpi=300)\n",
    "\n",
    "labels=['A', 'X', r'$\\beta$', r'$y_0$', r'$\\log{\\dot{M}_{\\rm Edd}}$']\n",
    "cornerplot(\"BLR\", labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BLRtau0(filters,delay_ref,wavelengths,\n",
    "                burnin=0,samples_file='samples_flat.obj',\n",
    "                outputdir = './',\n",
    "                band_colors = None,\n",
    "                redshift=0.0,\n",
    "                savefig=True,figname=None,\n",
    "                dataframe=None, flats=None, FluxWeight=True, labels=None):\n",
    "    \n",
    "    if outputdir[-1] != '/': outputdir += '/'\n",
    "    file = open(outputdir+samples_file,'rb')\n",
    "    samples = pickle.load(file)[burnin:]\n",
    "\n",
    "    ss = np.where(np.array(filters) == delay_ref)[0][0]\n",
    "    \n",
    "    # To get ONLY lags\n",
    "    shifter = 2\n",
    "\n",
    "    list_only = []\n",
    "    mm = 0\n",
    "    ndim = len(filters)\n",
    "    for i in range(ndim):\n",
    "        if i != ss:\n",
    "            list_only.append(i*4+shifter+mm)\n",
    "        if i == ss:\n",
    "            mm = -1\n",
    "    # Get the \n",
    "    lag,lag_m,lag_p = np.zeros(ndim-1),np.zeros(ndim-1),np.zeros(ndim-1)\n",
    "    for j,i in enumerate(list_only):\n",
    "        #print(i)\n",
    "        q50 = np.percentile(samples[:,i],50)\n",
    "        q84 = np.percentile(samples[:,i],84)\n",
    "        q16 = np.percentile(samples[:,i],16)\n",
    "        lag[j] = q50\n",
    "        lag_m[j] = q50-q16\n",
    "        lag_p[j] = q84-q50\n",
    "        \n",
    "    xerr = [584.89/2, 1262.68/2, 840/2, 1149.52/2, 1238.95/2, 994.39/2]\n",
    "    \n",
    "    delta = np.mean([np.median(samples[:, -1]), np.mean(samples[:, -1]), sp.stats.mode(samples[:, -1])[0]])\n",
    "    \n",
    "    lam0 = dataframe.loc[(dataframe['Filter'] == 'gp')]['Wavelength'].values[0]/(1+redshift)\n",
    "\n",
    "    fitwave = np.array(wavelengths)/(1+redshift)\n",
    "    fitlag = np.array(lag)/(1+redshift)\n",
    "    fiterr = np.array(lag_m)\n",
    "    BLRX = BLR_DF['X'].values/(1+redshift)\n",
    "    BLRY = BLR_DF['Y'].values\n",
    "    XRYX = np.linspace(1, max(BLRX),1000)\n",
    "\n",
    "    def alpha(theta, FluxWeight=True, lam0=1):\n",
    "        A, x, beta, y0, logmdotedd = theta\n",
    "        \n",
    "        c = 299792458 #m/s\n",
    "        k = 1.3806452e-23 #J/K\n",
    "        lam0 = lam0*1e-10#4770*1e-10 #m\n",
    "        h = 6.62607015e-34 #J/Hz\n",
    "        G = 6.6743e-11 #Nm^2/kg^2\n",
    "        M = (10**8.00)*1.989e30 #kg\n",
    "        sigma = 5.670374419e-8 #W/m^2K^4\n",
    "        eta = 0.1\n",
    "        kappa = 1\n",
    "        mdotedd = 10**logmdotedd\n",
    "        Ledd = lumin(M)/1e7 #W\n",
    "        \n",
    "        if FluxWeight == True:\n",
    "            X = 2.49\n",
    "        else:\n",
    "            X=4.97\n",
    "            \n",
    "        alpha = (1/c)*( X*k*lam0/(h*c) )**(4/3) * ( (G*M/(8*np.pi*sigma)) * (Ledd/(eta*c**2)) * (3+kappa)*mdotedd )**(1/3)\n",
    "        return alpha/(60*60*24)\n",
    "    \n",
    "    \n",
    "    def timelag(theta, lam=fitwave, lam0=1, FluxWeight=True):\n",
    "        A, x, beta, y0, logmdotedd = theta\n",
    "        \n",
    "        alph = alpha(theta, FluxWeight=FluxWeight)\n",
    "        return alph*((lam/lam0)**(4/3) - 1), alph\n",
    "    \n",
    "    def newmodel(theta, FluxWeight=True, lam=fitwave, BLRY=BLRY, callam1= 0, callam2=lam0):\n",
    "        A, x, beta, y0, logmdotedd = theta\n",
    "        \n",
    "        pwl, alph = timelag(theta, FluxWeight=FluxWeight)\n",
    "        \n",
    "        tck = sp.interpolate.splrep(lam, pwl)\n",
    "        pwl1 = sp.interpolate.splev(callam1, tck)\n",
    "        pwl2 = sp.interpolate.splev(callam2, tck)\n",
    "        # print(pwl1, pwl2)\n",
    "        return pwl2-pwl1\n",
    "    \n",
    "    def sample_walkers(model, nsamples=10000, flattened_chain=None, FluxWeight=True):\n",
    "        models=[]\n",
    "        draw = np.floor(np.random.uniform(0, len(flat_samples), size=nsamples)).astype(int)\n",
    "        thetas = flattened_chain[draw]\n",
    "        for i in thetas:\n",
    "            mod = model(i)\n",
    "            models.append(mod)\n",
    "        spread = np.std(models, axis=0)\n",
    "        med_model = np.median(models, axis=0)\n",
    "        return med_model, spread\n",
    "    \n",
    "    flat_samples = np.array([flats[f'{l}'] for l in labels]).T\n",
    "    \n",
    "    med_model, spread = sample_walkers(newmodel, flattened_chain=flat_samples, FluxWeight=FluxWeight)\n",
    "    \n",
    "    print('%7.3f +- %7.3f days'%(med, spread))\n",
    "    \n",
    "    return med_model, spread"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flats = pd.read_pickle(\"BLR_flat_samples.pkl\")\n",
    "labels = ['A', 'X', 'beta', 'y0', 'logmdotedd']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "med, spread = BLRtau0(filters,delay_ref,outputdir=outputdir,\n",
    "                                burnin=burnin,\n",
    "                                band_colors=band_colors,\n",
    "                                wavelengths=waves,redshift=redshift,\n",
    "                                dataframe = DF_FCCF, \n",
    "                      flats=flats, labels=labels, FluxWeight=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Frequency-resolved lag spectrum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputdir = [\"/home/jovyan/PyROAF/\", \"/home/jovyan/PyROA2/\",\"/home/jovyan/PyROA5/\", \"/home/jovyan/PyROA10/\",  \"/home/jovyan/PyROAD20/\", \"/home/jovyan/PyROA40\"]\n",
    "def FreqSpectrum(filters,delay_ref,wavelengths,\n",
    "                burnin=0,samples_file='samples_flat.obj',\n",
    "                outputdirs = './',\n",
    "                band_colors = None,\n",
    "                redshift=0.0,\n",
    "                savefig=True,figname=None,\n",
    "                dataframe=None, database=None):\n",
    "    fig, ax = plt.subplots(figsize=(10,7))\n",
    "    for s in range(len(outputdirs)):\n",
    "        outputdir=outputdirs[s]\n",
    "        if outputdir[-1] != '/': outputdir += '/'\n",
    "        file = open(outputdir+samples_file,'rb')\n",
    "        samples = pickle.load(file)[burnin:]\n",
    "        deltas = np.mean([np.median(samples[:, -1]), np.mean(samples[:, -1]), sp.stats.mode(samples[:, -1])[0]])\n",
    "        delta = np.array([fr'{round(deltas,2)}', 2, 5, 10, 20, 40])[s]\n",
    "        color=np.array(['k', 'purple','r','b', 'orange', 'green'])[s]\n",
    "        \n",
    "        ss = np.where(np.array(filters) == delay_ref)[0][0]\n",
    "        #print(ss)\n",
    "        labels = []\n",
    "        for i in range(len(filters)):\n",
    "            for j in [\"A\", \"B\",r\"$\\tau$\", r\"$\\sigma$\"]:\n",
    "                labels.append(j+r'$_{'+filters[i]+r'}$')\n",
    "        labels.append(r'$\\Delta$')\n",
    "        all_labels = labels.copy()\n",
    "        del labels[ss*4+2]\n",
    "\n",
    "        # To get ONLY lags\n",
    "        shifter = 2\n",
    "\n",
    "        list_only = []\n",
    "        mm = 0\n",
    "        ndim = len(filters)\n",
    "        for i in range(ndim):\n",
    "            if i != ss:\n",
    "                list_only.append(i*4+shifter+mm)\n",
    "            if i == ss:\n",
    "                mm = -1\n",
    "        # Get the \n",
    "        lag,lag_m,lag_p = np.zeros(ndim-1),np.zeros(ndim-1),np.zeros(ndim-1)\n",
    "        for j,i in enumerate(list_only):\n",
    "            #print(i)\n",
    "            q50 = np.percentile(samples[:,i],50)\n",
    "            q84 = np.percentile(samples[:,i],84)\n",
    "            q16 = np.percentile(samples[:,i],16)\n",
    "            lag[j] = q50\n",
    "            lag_m[j] = q50-q16\n",
    "            lag_p[j] = q84-q50\n",
    "        \n",
    "        # ax = fig.add_subplot(111)\n",
    "\n",
    "        plt.axhline(y=0,ls='--',alpha=0.5,color='k')\n",
    "\n",
    "        if band_colors == None: band_colors = 'k'*7\n",
    "\n",
    "        mm = 0\n",
    "        wavelengths = np.array(wavelengths)\n",
    "        # for i in range(lag.size):        \n",
    "        plt.plot(wavelengths/(1+redshift),lag/(1+redshift),color=color)\n",
    "        plt.fill_between(wavelengths/(1+redshift), lag/(1+redshift)-lag_m, lag/(1+redshift)+lag_m,color=color, alpha=0.3) \n",
    "            # print(wavelengths[i])\n",
    "\n",
    "        if redshift > 0:\n",
    "            plt.xlabel(r'Rest Wavelength ($\\mathrm{\\AA}$)')\n",
    "            plt.ylabel(r'$\\tau_{\\rm rest}$ (day)')\n",
    "        else:\n",
    "            plt.xlabel(r'Observed Wavelength / $\\mathrm{\\AA}$')\n",
    "            plt.ylabel(r'$\\tau$ / day')\n",
    "        if outputdir == \"/home/jovyan/PyROAF/\":\n",
    "            ax.lines[-1].set_label(fr'Free parameter $\\Delta \\simeq$ {delta}')\n",
    "        else:\n",
    "            ax.lines[-1].set_label(fr'$\\Delta$ = {delta}')\n",
    "        # else:\n",
    "            # ax.lines[-1].set_label(fr'PyROA')\n",
    "        \n",
    "    lam0 = dataframe.loc[(dataframe['Filter'] == 'gp')]['Wavelength'].values[0]\n",
    "    \n",
    "    ax.axvline(lam0, color='k', linestyle=\"--\", alpha=0.5, zorder=0)\n",
    "        \n",
    "    plt.legend(fontsize=12, ncol=2)\n",
    "    plt.savefig(\"freqspec.png\", dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FreqSpectrum(filters,delay_ref,outputdirs=outputdir,\n",
    "                burnin=burnin,\n",
    "                band_colors=band_colors,\n",
    "                wavelengths=waves,redshift=redshift,\n",
    "                dataframe = DF_FCCF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XSPEC plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_no(df, names):\n",
    "    for name in names:\n",
    "        df.loc[df[fr'{name}'] == \"NO\", fr'{name}'] = float(\"nan\")\n",
    "    df = df.astype(np.float32)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = ['X','X_err','Y','Y_err','total', 'relagn', 'relxill']\n",
    "names_delchi = ['X','X_err','Y','Y_err']\n",
    "modelname = [\"relagn\", \"relxill\"]\n",
    "df = pd.read_table('2agnxill_data.qdp',skiprows=3,names=names, delimiter=' ')\n",
    "df_delchi = pd.read_table('2agnxill02_delchi.qdp', names =names_delchi, skiprows=3, delimiter=\" \")\n",
    "\n",
    "df = df_no(df, names)\n",
    "df_delchi=df_no(df_delchi, names_delchi)\n",
    "\n",
    "names2 = ['X','X_err','Y','relagn', 'relxill']\n",
    "modelname2 = [\"relagn\", \"relxill\"]\n",
    "df2 = pd.read_table('2agnxill02_nufnu.qdp',skiprows=3,names=names2, delimiter=' ')\n",
    "df2 = df_no(df2, names2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting photon counts against energy and XSPEC model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_plot(df, modelname, delchi, X = 'Energy (keV)', Y = 'Photon (counts/s/keV)'):\n",
    "    fig, ax = plt.subplots(2, figsize=(10,8), height_ratios=[5,1],sharex=True)\n",
    "    plt.subplots_adjust(wspace=0, hspace=0)\n",
    "    # # Plot using Matplotlib:\n",
    "    ax[0].errorbar(df['X'].values, df['Y'].values, xerr=abs(df['X_err'].values),yerr=abs(df['Y_err'].values),fmt='o',label='data', zorder=0, color='k')\n",
    "    ax[0].plot(df['X'].values, df['total'].values, color='red',label=r'2016-2017 XSPEC model',ls=\"--\")\n",
    "    color=['red', 'dodgerblue']\n",
    "    linestyle=[\"dashed\", \"dotted\"]\n",
    "    for j in range(0):\n",
    "        if j == 0:\n",
    "            i = 0\n",
    "        else:\n",
    "            i = 4\n",
    "        ax[0].plot(df['X'].values[i:], df[f'{modelname[j]}'].values[i:],label=f'{modelname[j]}', color=color[j], linestyle=linestyle[j])\n",
    "    plt.xlabel(fr'{X}')\n",
    "    ax[0].set_ylabel(fr'{Y}')\n",
    "    ax[0].set_xscale(\"log\")\n",
    "    ax[0].set_yscale(\"log\")\n",
    "    ax[0].yaxis.set_ticks_position('left')\n",
    "    ax[0].set_ylim((2e-5,1e4))\n",
    "    ax[1].errorbar(delchi['X'].values, delchi['Y'].values, xerr=delchi['X_err'].values, yerr=delchi['Y_err'].values, fmt='.', color='k', alpha=0.7)\n",
    "    ax[1].set_ylabel(fr'$\\chi$')\n",
    "    ax[1].axhline(0, ls=\"--\", zorder=0, color=\"grey\")\n",
    "    ax[1].yaxis.set_ticks_position('left') \n",
    "    # ax[1].set_ylim(\n",
    "    # ax.grid()\n",
    "    ax[0].legend()\n",
    "    ax[0].xaxis.set_ticks_position('both') \n",
    "    ax1 = ax[0].twiny()\n",
    "    ax1.set_xticks(ax[0].get_xticks())\n",
    "    ax1.set_xbound(ax[0].get_xbound())\n",
    "    ax1.set_xscale('log')\n",
    "    ax1.set_xticklabels([round(12.398/x) for x in ax[0].get_xticks()])\n",
    "    \n",
    "    ax[0].minorticks_off()\n",
    "    ax[1].minorticks_off()\n",
    "    ax1.minorticks_off()\n",
    "    \n",
    "    \n",
    "    ax1.set_xlabel(r\"Wavelength $\\lambda$ ($\\mathrm{\\AA}$)\")\n",
    "    plt.show()\n",
    "    fig.savefig(\"specdatares.png\", dpi=300)\n",
    "df_plot(df, modelname, df_delchi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting XSPEC SED model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def A_energy(A):\n",
    "        return 12.398425/A\n",
    "    \n",
    "#for converting nufnu xspec plots and plotting model components\n",
    "def plotter(df2, modelname):\n",
    "    energies = A_energy(df2['X'].values)\n",
    "    fig, ax = plt.subplots(figsize=(10,6))\n",
    "    # # Plot using Matplotlib:\n",
    "    ax.plot(energies, df2['Y'].values*1e23, color='plum',label='total model') #Jy = 1023 ergs1cm2Hz1\n",
    "    color=['red', 'dodgerblue']\n",
    "    for j in range(2):\n",
    "        ax.plot(energies, df2[f'{modelname[j]}'].values*1e23,label=f'{modelname[j]}', color=color[j], linestyle=\"dotted\")\n",
    "\n",
    "    ax.set_xlabel('Energy (keV)')\n",
    "    ax.set_ylabel(r'$\\nu \\mathrm{F}_\\nu$')\n",
    "    ax.set_xscale(\"log\")\n",
    "    ax.set_yscale(\"log\")\n",
    "    ax.set_ylim((1e10,1e13))\n",
    "    ax.set_xlim((1e-3,1e3))\n",
    "    ax.grid()\n",
    "    ax.legend()\n",
    "    plt.show()\n",
    "    return energies, df2['Y'].values*1e23\n",
    "energies, Y = plotter(df2, modelname2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting XSPEC SED model against models from Mehdipour et al. 2023"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_plot(dfs, models, colors, X = 'Energy (keV)', Y = fr'$\\nu F_\\nu$ (Jy)'):\n",
    "    fig, ax = plt.subplots(figsize=(10,8))\n",
    "    # # Plot using Matplotlib:\n",
    "    for i in range(len(dfs)):\n",
    "        df = dfs[i]\n",
    "        ax.plot(df['X'].values, df['Y'].values,label=models[i], color=colors[i])\n",
    "    \n",
    "    ax.set_xlabel(fr'{X}')\n",
    "    ax.set_ylabel(fr'{Y}')\n",
    "    ax.set_xscale(\"log\")\n",
    "    ax.set_yscale(\"log\")\n",
    "    ax.yaxis.set_ticks_position('left')\n",
    "    ax.legend()\n",
    "    ax.xaxis.set_ticks_position('both') \n",
    "    ax.set_ylim((1e11,1e13))\n",
    "    ax.set_xlim((1e-3,1e2))\n",
    "    ax1 = ax.twiny()\n",
    "    ax1.set_xticks(ax.get_xticks())\n",
    "    ax1.set_xbound(ax.get_xbound())\n",
    "    ax1.set_xscale('log')\n",
    "    ax1.set_xticklabels([round(12.398/x) for x in ax.get_xticks()])\n",
    "    \n",
    "    ax.minorticks_off()\n",
    "    ax1.minorticks_off()\n",
    "    ax1.set_xlabel(r\"Wavelength $\\lambda$ ($\\mathrm{\\AA}$)\")\n",
    "    plt.show()\n",
    "    fig.savefig(\"sedcomp.png\", dpi=300)\n",
    "    \n",
    "red = pd.read_csv(\"red.csv\", names=['X', 'Y'])\n",
    "blue = pd.read_csv(\"blue.csv\", names=['X', 'Y'])\n",
    "dfmy = pd.DataFrame({'X':energies, 'Y':Y})\n",
    "model_plot([blue, red, dfmy], ['2001', '2022', '2016 March-2017 August'], ['blue', 'red', 'green'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RELAGN components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dagn=relagn(M=1e8, dist=161, log_mdot=-1.17)\n",
    "\n",
    "#getting total AGN SED\n",
    "Lnu_rel = dagn.get_totSED(rel=True)\n",
    "\n",
    "#getting components\n",
    "Lrel_dsc = dagn.get_DiscComponent()\n",
    "Lrel_wrm = dagn.get_WarmComponent()\n",
    "Lrel_hot = dagn.get_HotComponent()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(11, 8))\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "nu = dagn.nu_obs #frequency grid\n",
    "ang=3e18/nu\n",
    "ax.loglog(ang, nu*Lrel_dsc, ls='-.', color='deeppink', label=\"accretion disk\")\n",
    "ax.loglog(ang, nu*Lrel_wrm, ls='-.', color='indigo', label=\"warm Comptonising component\")\n",
    "ax.loglog(ang, nu*Lrel_hot, ls='-.', color='midnightblue', label=\"hot Comptonising component\")\n",
    "\n",
    "ax.loglog(ang, nu*Lnu_rel, color='k', label=\"total SED\")\n",
    "\n",
    "\n",
    "ax.set_ylim(max(nu*Lnu_nr)*1e-2, max(nu*Lnu_nr)*2)\n",
    "\n",
    "ax.set_xlabel(r'Wavelength   ($\\mathrm{\\AA}$)')\n",
    "ax.set_ylabel(r'$\\nu F_{\\nu}$   (ergs/s)')\n",
    "plt.legend()\n",
    "plt.savefig(\"example.png\", dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XSPEC codes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for running XSPEC fit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "method leven 500 0.01\n",
    "abund angr\n",
    "xsect vern\n",
    "cosmo 70 0 0.73\n",
    "xset delta 0.01\n",
    "xset KYRH     1.14106727\n",
    "xset KYRIN     4.02531052\n",
    "xset KYRMS     1.45449758\n",
    "systematic 0\n",
    "model  phabs(relagn + relxill)\n",
    "      0.0310315      0.001          0          0     100000      1e+06\n",
    "          1e+08         -1          1          1      1e+10      1e+10\n",
    "          161.2         -1     0.0001     0.0001       1000       1000\n",
    "       -1.18768       0.01        -10        -10          2          2\n",
    "           0.99         -1          0          0      0.998      0.998\n",
    "            0.5         -1       0.09       0.09      0.998      0.998\n",
    "            100         -1         10         10        300        300\n",
    "       0.114629      0.001       0.01       0.01          1          1\n",
    "        1.82446       0.01        1.3        1.3          3          3\n",
    "        2.39367       0.01          2          2          5          5\n",
    "        4.33208        0.1          1          1        500        500\n",
    "              6       -0.1          6          6        500        500\n",
    "             -1         -1         -1         -1          7          7\n",
    "/\n",
    "        9.57348          1          6          6         10         10\n",
    "         0.0364         -1          0          0          1          1\n",
    "              1         -1          0          0      1e+20      1e+24\n",
    "              3       -0.1        -10        -10         10         10\n",
    "              3       -0.1        -10        -10         10         10\n",
    "/\n",
    "= p5\n",
    "= acos(p6)/3.141592*180\n",
    "             -1         -1       -100       -100         -1         -1\n",
    "        2.89228        0.1          1          1        400       1000\n",
    "= p16\n",
    "= p9\n",
    "        1.69563       0.01          0          0        4.7        4.7\n",
    "              1         -1        0.5        0.5         10         10\n",
    "            300       -0.1          5          5       1000       1000\n",
    "             -1         -1      -1000          0         10       1000\n",
    "    0.000140173       0.01          0          0      1e+20      1e+24\n",
    "= p1\n",
    "= p2\n",
    "= p3\n",
    "= p4\n",
    "= p5\n",
    "= p6\n",
    "= p7\n",
    "= p8\n",
    "= p9\n",
    "= p10\n",
    "= p11\n",
    "= p12\n",
    "= p13\n",
    "= p14\n",
    "= p15\n",
    "= p16\n",
    "= p17\n",
    "= p18\n",
    "= p19\n",
    "= p20\n",
    "= p21\n",
    "= p22\n",
    "= p23\n",
    "= p24\n",
    "= p25\n",
    "= p26\n",
    "= p27\n",
    "= p28\n",
    "= p29\n",
    "= p30\n",
    "= p31\n",
    "newpar 14 = p23\n",
    "newpar 20 = p23\n",
    "bayes off"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for plotting models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "method leven 500 0.01\n",
    "abund angr\n",
    "xsect vern\n",
    "cosmo 70 0 0.73\n",
    "xset delta 0.01\n",
    "xset KYRH     1.14106727\n",
    "xset KYRIN     4.02531052\n",
    "xset KYRMS     1.45449758\n",
    "systematic 0\n",
    "model  (relagn + relxill)\n",
    "          1e+08         -1          1          1      1e+10      1e+10\n",
    "          161.2         -1     0.0001     0.0001       1000       1000\n",
    "       -1.18768       0.01        -10        -10          2          2\n",
    "           0.99         -1          0          0      0.998      0.998\n",
    "            0.5         -1       0.09       0.09      0.998      0.998\n",
    "            100         -1         10         10        300        300\n",
    "       0.114629      0.001       0.01       0.01          1          1\n",
    "        1.82446       0.01        1.3        1.3          3          3\n",
    "        2.39367       0.01          2          2          5          5\n",
    "        4.33208        0.1          1          1        500        500\n",
    "              6       -0.1          6          6        500        500\n",
    "             -1         -1         -1         -1          7          7\n",
    "/\n",
    "        9.57348          1          6          6         10         10\n",
    "         0.0364         -1          0          0          1          1\n",
    "              1         -1          0          0      1e+20      1e+24\n",
    "              3       -0.1        -10        -10         10         10\n",
    "              3       -0.1        -10        -10         10         10\n",
    "/\n",
    "= p4\n",
    "= acos(p5)/3.141592*180\n",
    "             -1         -1       -100       -100         -1         -1\n",
    "        2.89228        0.1          1          1        400       1000\n",
    "= p15\n",
    "= p8\n",
    "        1.69563       0.01          0          0        4.7        4.7\n",
    "              1         -1        0.5        0.5         10         10\n",
    "            300       -0.1          5          5       1000       1000\n",
    "             -1         -1      -1000          0         10       1000\n",
    "    0.000140173       0.01          0          0      1e+20      1e+24\n",
    "= p1\n",
    "= p2\n",
    "= p3\n",
    "= p4\n",
    "= p5\n",
    "= p6\n",
    "= p7\n",
    "= p8\n",
    "= p9\n",
    "= p10\n",
    "= p11\n",
    "= p12\n",
    "= p13\n",
    "= p14\n",
    "= p15\n",
    "= p16\n",
    "= p17\n",
    "= p18\n",
    "= p19\n",
    "= p20\n",
    "= p21\n",
    "= p22\n",
    "= p23\n",
    "= p24\n",
    "= p25\n",
    "= p26\n",
    "= p27\n",
    "= p28\n",
    "= p29\n",
    "= p30\n",
    "newpar 13 = p22\n",
    "newpar 19 = p22\n",
    "bayes off"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "obsastro",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
